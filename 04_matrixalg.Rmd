# Matrix Algebra

```{block2 echo=FALSE}
In this book we try to minimize mathematical notation as much as possible. Furthermore, we avoid using calculus to motivate statistical concepts. However, Matrix Algebra (also referred to as Linear Algebra) and its mathematical notation greatly facilitates the exposition of the advanced data analysis techniques covered in the remainder of this book. We therefore dedicate a chapter of this book to introducing Matrix Algebra. We do this in the context of data analysis and using one of the main applications: Linear Models.
```
&emsp;&emsp;本书中，我们尝试尽可能的使用较少的数学上的符号。因此，我们避免使用微积分来推导统计学概念。然而，在这本书的其余部分，矩阵代数（也被称为线性代数）的数学符号能够极大的促进对数据分析技术的阐述。因此，我们在本章节中来介绍矩阵代数。我们在数据分析过程中这样做，并且使用其主要的应用：线性代数。
```{block2 echo=FALSE}
We will describe three examples from the life sciences: one from physics, one related to genetics, and one from a mouse experiment. They are very different, yet we end up using the same statistical technique: fitting linear models. Linear models are typically taught and described in the language of matrix algebra. 
```
&emsp;&emsp;我们将使用三组来自于生命科学的数据，一个来自于物理学、一个与遗传学相关、一个来自于小鼠的实验。这三组数据之间差距较大，但是我们仍然使用相同的统计学方法拟合线性模型来结束分析。人们通常使用数据矩阵的语言来对线性模型进行讲解和表述。
```{r echo=FALSE}
library(rafalib)
```

## Motivating Examples

#### Falling objects
```{block2 echo=FALSE}
Imagine you are Galileo in the 16th century trying to describe the velocity of a falling object. An assistant climbs the Tower of Pisa and drops a ball, while several other assistants record the position at different times. Let's simulate some data using the equations we know today and adding some measurement error:
```
&emsp;&emsp;假设你是16世纪的伽利略，正在尝试着描述高空坠物的的速度。你的一个助手爬上比萨的高塔扔下一个球，然后其他的助手记录不同时间时球的位置。让我们利用我们今天所知道的方程式来模拟一些数据，并且增加一些测量误差。
```{r}
set.seed(1)
g <- 9.8 ##meters per second
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, note: we use tt because t is a base function
d <- 56.67  - 0.5*g*tt^2 + rnorm(n,sd=1) ##meters
```
```{block2 echo=FALSE}
The assistants hand the data to Galileo and this is what he sees:
```
&emsp;&emsp;助手们拿着数据找到伽利略告诉他这是他们看到的数据:
```{r gravity, fig.cap="Simulated data for distance travelled versus time of falling object measured with error."}
mypar()
plot(tt,d,ylab="Distance in meters",xlab="Time in seconds")
```
```{block2 echo=FALSE}
He does not know the exact equation, but by looking at the plot above he deduces that the position should follow a parabola. So he models the data with:
```
&emsp;&emsp;他不知道准确的方程式，但是他通过观察纸上的标绘，推断球的位置应该遵循一个抛物线。因此，他根据这些数据建立了一个模型:
$$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n $$
```{block2 echo=FALSE}
With $Y_i$ representing location, $x_i$ representing the time, and $\varepsilon_i$ accounting for measurement error. This is a linear model because it is a linear combination of known quantities (the $x$'s) referred to as predictors or covariates and unknown parameters (the $\beta$'s).
```
&emsp;&emsp;其中，$Y_i$代表球的位置，$x_i$代表观察的时间，$\varepsilon_i$表示测量的误差。这是一个线性的模型，因为这是已知数据的线性组合，被成为预测器或者叫做参数位置的协变量($\beta$'s)。
#### Father & son heights
```{block2 echo=FALSE}
Now imagine you are Francis Galton in the 19th century and you collect paired height data from fathers and sons. You suspect that height is inherited. Your data:
```
&emsp;&emsp;现在假设你是19世纪的弗朗西斯，你收集了一些父亲和儿子之间的身高数据。你猜想，身高是可遗传的，你的数据如下所示:
```{r,message=FALSE}
data(father.son,package="UsingR")
x=father.son$fheight
y=father.son$sheight
```
```{block2 echo=FALSE}
looks like this:
```
&emsp;&emsp;看起来像这样：
```{r galton_data, fig.cap="Galton's data. Son heights versus father heights."}
plot(x,y,xlab="Father's height",ylab="Son's height")
```
```{block2 echo=FALSE}
The sons' heights do seem to increase linearly with the fathers' heights. In this case, a model that describes the data is as follows:
```
&emsp;&emsp;儿子的身高似乎随着父亲的身高而线性的升高。在这种情况下，可以用一个模型来描述这些数据，如下所示:
$$ Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i=1,\dots,N $$
```{block2 echo=FALSE}
This is also a linear model with $x_i$ and $Y_i$, the father and son heights respectively, for the $i$-th pair and $\varepsilon_i$ a term to account for the extra variability. Here we think of the fathers' heights as the predictor and being fixed (not random) so we use lower case. Measurement error alone can't explain all the variability seen in $\varepsilon_i$. This makes sense as there are other variables not in the model, for example, mothers' heights, genetic randomness, and environmental factors.
```
&emsp;&emsp;这也是一个关于父亲的身高$x_i$ 和儿子的身高$Y_i$的线性模型，对于数据对$i$-th和$\varepsilon_i$用来解释额外的变量。这里，我们想利用父亲的身高作为一个预测器并且被固定（不是随机的），因此我们使用小写字母。单独的测量误差不能解释在$\varepsilon_i$所有的变量。这就表明了这里还存在其他的误差，但是不在所推测的模型中，例如，母亲的身高、遗传的随机性和环境因素
#### Random samples from multiple populations
```{block2 echo=FALSE}
Here we read-in mouse body weight data from mice that were fed two different diets: high fat and control (chow). We have a random sample of 12 mice for each. We are interested in determining if the diet has an effect on weight. Here is the data:
```
&emsp;&emsp;在这里我们从两组不同喂养方式一个是喂养高脂肪数据另一个是喂养正常食物的小鼠中记录老鼠的体重。我们在每一组中随机选择12个小鼠。我们对两组喂养方式是否对小鼠的体重有一定的影响感兴趣。下面是具体的数据。
```{r, echo=FALSE}
library(downloader)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
filename <- "femaleMiceWeights.csv"
if (!file.exists(filename)) download(url,destfile=filename)
```

```{r mice_weights,fig.cap="Mouse weights under two diets."}
dat <- read.csv("femaleMiceWeights.csv")
mypar(1,1)
stripchart(Bodyweight~Diet,data=dat,vertical=TRUE,method="jitter",pch=1,main="Mice weights")
```
```{block2 echo=FALSE}
We want to estimate the difference in average weight between populations. We demonstrated how to do this using t-tests and confidence intervals, based on the difference in sample averages. We can obtain the same exact results using a linear model:
```
&emsp;&emsp;我们想要评估在群体之间体重的差异。我们展示如何基于样本平均值之间的差异来使用t-tests和置信区间。我们可以利用线性模型得到准确的结果。

$$ Y_i = \beta_0 + \beta_1 x_{i} + \varepsilon_i$$
```{block2 echo=FALSE}
with $\beta_0$ the chow diet average weight, $\beta_1$ the difference between averages, $x_i = 1$ when mouse $i$ gets the high fat (hf) diet, $x_i = 0$ when it gets the chow diet, and $\varepsilon_i$ explains the differences between mice of the same population. 
 ```
 &emsp;&emsp;$\beta_0$是饮食的平均重量，$\beta_1$是平均值之间的差异，当小鼠$i$吃的是高脂肪的食物时$x_i = 1$，当小鼠吃的是普通饲料时$x_i = 0$，$\varepsilon_i$解释了相同样本中不同小鼠之间的差异。
```{block2 echo=FALSE}
#### Linear models in general
 ```
 ####一般线性模型
 ```{block2 echo=FALSE}
We have seen three very different examples in which linear models can be used. A general model that encompasses all of the above examples is the following:
```
&emsp;&emsp;上面我们看了三个非常不同例子都可以使用线性模型来进行分析。一个通用的模型可以包含以上所有的例子，如下所示：
$$ Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \dots +  \beta_2 x_{i,p} + \varepsilon_i, i=1,\dots,n $$

 
$$ Y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{i,j} + \varepsilon_i, i=1,\dots,n $$
```{block2 echo=FALSE}
Note that we have a general number of predictors $p$. Matrix algebra provides a compact language and mathematical framework to compute and make derivations with any linear model that fits into the above framework.
```
&emsp;&emsp;注意，我们的预测因子有一个一般是数值$p$。矩阵代数提供了一个严式语言和数学框架来计算并且能够派生出任何能够使用以上框架的线性模型。
<a name="estimates"></a>
```{block2 echo=FALSE}
#### Estimating parameters
```
#### 参数估计
```{block2 echo=FALSE}
For the models above to be useful we have to estimate the unknown $\beta$ s. In the first example, we want to describe a physical process for which we can't have unknown parameters. In the second example, we better understand inheritance by estimating how much, on average, the father's height affects the son's height. In the final example, we want to determine if there is in fact a difference: if $\beta_1 \neq 0$. 
```
&emsp;&emsp;以上的模型对于我们必须评估不知道的$\beta$值时能够非常适用。在第一个例子中，我们想要描述一个我们不能有的未知参数的物理过程。在第二个例子中，我们通过在平均水平上评估了父亲的身高能够多少遗传给儿子的对遗传有了更好的认识。在最后的例子中，我们想要确定当$\beta_1 \neq 0$时是否有一个本质的不同。
```{block2 echo=FALSE}
The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this chapter:
```
&emsp;&emsp;科学的标准方法是需找值能够减少适用模型和真实数据之间的差异。下面的例子是叫做最小二乘方程，这个方法将会在这个章节中经常看到。
$$ \sum_{i=1}^n \left\{  Y_i - \left(\beta_0 + \sum_{j=1}^p \beta_j x_{i,j}\right)\right\}^2 $$
```{block2 echo=FALSE}
Once we find the minimum, we will call the values the least squares estimates (LSE) and denote them with $\hat{\beta}$. The quantity obtained when evaluating the least squares equation at the estimates is called the residual sum of squares (RSS). Since all these quantities depend on $Y$, *they are random variables*. The $\hat{\beta}$ s are random variables and we will eventually perform inference on them.
```
&emsp;&emsp;一旦找到最小值，我们称之为最小二乘估计（LSE）并用$\hat{\beta}$表示。当我们利用最小二乘估计评估获得的数值，也叫做残差平方和。由于所有的数值都是基于$Y$，*他们都是随机变量*。$\hat{\beta}$是一个随机变量，因此最终我们会对它们进行推理。
```{block2 echo=FALSE}
#### Falling object example revisited
```
#### 下落物体例子的重新审视
```{block2 echo=FALSE}
Thanks to my high school physics teacher, I know that the equation for the trajectory of a falling object is: 
```
&emsp;&emsp;感谢我的高中物理老师，我知道一个下落物体的轨迹的方程是：
$$d = h_0 + v_0 t -  0.5 \times 9.8 t^2$$
```{block2 echo=FALSE}
with $h_0$ and $v_0$ the starting height and velocity respectively. The data we simulated above followed this equation and added measurement error to simulate `n` observations for dropping the ball $(v_0=0)$ from the tower of Pisa $(h_0=56.67)$. This is why we used this code to simulate data:
``` 
&emsp;&emsp;$h_0$和$v_0$分别是开始的高度和速度。以上我们模拟的数据符合这个等式并且当球从比萨斜塔$(h_0=56.67)$下降$(v_0=0)$的时候，我们为观测值增加了测量误差`n`。这就是为什么我们使用以下代码来实现数据模拟。
```{r simulate_drop_data}
g <- 9.8 ##meters per second
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, t is a base function
f <- 56.67  - 0.5*g*tt^2
y <-  f + rnorm(n,sd=1)
```
```{block2 echo=FALSE}
Here is what the data looks like with the solid line representing the true trajectory:
```
&emsp;&emsp;这些看起来像实线的数据代表着球下落的真实轨迹。
```{r simulate_drop_data_with_fit, fig.cap="Fitted model for simulated data for distance travelled versus time of falling object measured with error."}
plot(tt,y,ylab="Distance in meters",xlab="Time in seconds")
lines(tt,f,col=2)
```
```{block2 echo=FALSE}
But we were pretending to be Galileo and so we don't know the parameters in the model. The data does suggest it is a parabola, so we model it as such:
```
&emsp;&emsp;但我们假装是伽利略,因此我们不知道在这个模型中的参数。这些数据表明它是一个抛物线，因此我们将其建模为这样：
$$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i, i=1,\dots,n $$
```{block2 echo=FALSE}
How do we find the LSE?
```
&emsp;&emsp;我们如何得到最小二乘估计？
```{block2 echo=FALSE}
#### The `lm` function
```
#### `lm`函数
```{block2 echo=FALSE}
In R we can fit this model by simply using the `lm` function. We will describe this function in detail later, but here is a preview:
```
&emsp;&emsp;在R语言中，我们可以通过`lm`函数来符合这个模型。我们将在接下来详细的描述这个函数，但是这里给出了一个预览：
```{r}
tt2 <-tt^2
fit <- lm(y~tt+tt2)
summary(fit)$coef
```
```{block2 echo=FALSE}
It gives us the LSE, as well as standard errors and p-values. 
 ```
 &emsp;&emsp;通过`lm`函数我们可以得到最小二乘估计以及标准误差及p-values。
 ```{block2 echo=FALSE}
Part of what we do in this section is to explain the mathematics behind this function. 
```
&emsp;&emsp;我们在这一节中所做的部分是用来解释这个函数背后的数学原理。
```{block2 echo=FALSE}
#### The least squares estimate (LSE)
```
#### 最小二乘估计 
```{block2 echo=FALSE}
Let's write a function that computes the RSS for any vector $\beta$:
```
&emsp;&emsp;让我写一个函数来计算任何向量$\beta$的和的平方根。
```{r}
rss <- function(Beta0,Beta1,Beta2){
  r <- y - (Beta0+Beta1*tt+Beta2*tt^2)
  return(sum(r^2))
}
```
```{block2 echo=FALSE}
So for any three dimensional vector we get an RSS. Here is a plot of the RSS as a function of $\beta_2$ when we keep the other two fixed:
```
&emsp;&emsp;任何的三维向量我们都可以计算其和的平方根。下面是当我们把另外两个向量固定的时候根据$\beta_2$绘制的和的平方根的图。
```{r rss_versus_estimate, fig.cap="Residual sum of squares obtained for several values of the parameters."}
Beta2s<- seq(-10,0,len=100)
plot(Beta2s,sapply(Beta2s,rss,Beta0=55,Beta1=0),
     ylab="RSS",xlab="Beta2",type="l")
##Let's add another curve fixing another pair:
Beta2s<- seq(-10,0,len=100)
lines(Beta2s,sapply(Beta2s,rss,Beta0=65,Beta1=0),col=2)
```
```{block2 echo=FALSE}
Trial and error here is not going to work. Instead, we can use calculus: take the partial derivatives, set them to 0 and solve. Of course, if we have many parameters, these equations can get rather complex. Linear algebra provides a compact and general way of solving this problem. 
```
&emsp;&emsp;这里的尝试和误差都不起作用。相反的，我们可以用微积分：取偏导数，把它们设为0并求解。当然，如果我们有许多的参数，这些方程可能变得相当复杂。线性代数提供了一种简洁而通用的方法来解决这一问题。
```{block2 echo=FALSE}
#### More on Galton (Advanced)
```
#### 更多关于高尔顿 （高级）
```{block2 echo=FALSE}
When studying the father-son data, Galton made a fascinating discovery using exploratory analysis.
```
&emsp;&emsp;当我们研究父子遗传的数据时，高尔顿通过探索性分析创造了一个有趣的发现。
![Galton's plot.](http://upload.wikimedia.org/wikipedia/commons/b/b2/Galton's_correlation_diagram_1875.jpg) 
```{block2 echo=FALSE}
He noted that if he tabulated the number of father-son height pairs and followed all the x,y values having the same totals in the table, they formed an ellipse. In the plot above, made by Galton, you see the ellipse formed by the pairs having 3 cases. This then led to modeling this data as correlated bivariate normal which we described earlier: 
```
&emsp;&emsp;他指出，如果将父亲与儿子的身高对制成表格，并且所有的x,y值在表格中都有相同的总数，它们可以形成一个椭圆形。在上面由高尔顿制作的图中，你可以看到由两对组成的椭圆由三种情况。然后可以通过将此数据建模为我们之前描述的二元正态相关。
$$ 
Pr(X<a,Y<b) =
$$


$$
\int_{-\infty}^{a} \int_{-\infty}^{b} \frac{1}{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}
\exp{ \left\{
\frac{1}{2(1-\rho^2)}
\left[\left(\frac{x-\mu_x}{\sigma_x}\right)^2 -  
2\rho\left(\frac{x-\mu_x}{\sigma_x}\right)\left(\frac{y-\mu_y}{\sigma_y}\right)+
\left(\frac{y-\mu_y}{\sigma_y}\right)^2
\right]
\right\}
}
$$
```{block2 echo=FALSE}
We described how we can use math to show that if you keep $X$ fixed (condition to be $x$) the distribution of $Y$ is normally distributed with mean: $\mu_x +\sigma_y \rho \left(\frac{x-\mu_x}{\sigma_x}\right)$ and standard deviation $\sigma_y \sqrt{1-\rho^2}$. Note that $\rho$ is the correlation between $Y$ and $X$, which implies that if we fix $X=x$, $Y$ does in fact follow a linear model. The $\beta_0$ and $\beta_1$ parameters in our simple linear model can be expressed in terms of $\mu_x,\mu_y,\sigma_x,\sigma_y$, and $\rho$.
```
&emsp;&emsp;我们描述了如何利用公式来展示如果我们保持$X$固定，那么$Y$的分布和其均值$\mu_x +\sigma_y \rho \left(\frac{x-\mu_x}{\sigma_x}\right)$以及标准偏差是正态分布的。请注意，$\rho$是$Y$和$X$系数，它意味着如果我们固定$X=x$，那么$Y$实际上是遵循线性模型的。在我们简单的线性模型中的参数$\beta_0$ and $\beta_1$，可以用$\mu_x,\mu_y,\sigma_x,\sigma_y$, and $\rho$来表示。
```{block2 echo=FALSE}
## Matrix Notation
```
## 矩阵符号
```{block2 echo=FALSE}
Here we introduce the basics of matrix notation. Initially this may seem over-complicated, but once we discuss examples, you will appreciate the power of using this notation to both explain and derive solutions, as well as implement them as R code. 
```
&emsp;&emsp;在这一章中我们将介绍矩阵符号的基本知识。起初这看起来似乎过于复杂，但是一旦我们讨论例子，你将会领会利用这个符号来解释和推导出解决方案的能力，并且将它们作为R代码来实现。
```{block2 echo=FALSE}
#### The language of linear models
```
#### 线性模型的语言
```{block2 echo=FALSE}
Linear algebra notation actually simplifies the mathematical descriptions and manipulations of linear models, as well as coding in R. We will discuss the basics of this notation and then show some examples in R.

The main point of this entire exercise is to show how we can write the models above using matrix notation, and then explain how this is useful for solving the least squares equation. We start by simply defining notation and matrix multiplication, but bear with us since we eventually get back to the practical application.
```
&emsp;&emsp;线性代数符号实际上是数学描述和操作线性模型的简化形式，就像R语言的代码。我们将讨论这个符号的基本知识并且使用R语言来展示一些例子。整个练习的要点是展示如何使用矩阵符号写出上面的几个模型，并且解释它是如何有效来解最小二乘方程的。我们从简单定义符号和矩阵乘法开始，但是请忍受我们，因为我们最终还是回到了实际应用。
```{block2 echo=FALSE}
## Solving Systems of Equations
```
## 求解系统方程
```{block2 echo=FALSE}
Linear algebra was created by mathematicians to solve systems of linear equations such as this:
```
&emsp;&emsp;线性代数是由数学家创造的用来求解线性方程方程式组，就像下面的例子：
$$
\begin{align*}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$
```{block2 echo=FALSE}
It provides very useful machinery to solve these problems generally. We will learn how we can write and solve this system using matrix algebra notation:
```
&emsp;&emsp;它为解决这些问题提供了非常有用的机制。我们将学习如何用矩阵代数符号来书写和求解这个方程组。
$$ 
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$
```{block2 echo=FALSE}
This section explains the notation used above. It turns out that we can borrow this notation for linear models in statistics as well.
```
&emsp;&emsp;本节解释了上面所使用的符号。这证明了我们也可以用这个符号来表示统计中的线性模型。
```{block2 echo=FALSE}
## Vectors, Matrices, and Scalars
```
## 向量、矩阵和标量
```{block2 echo=FALSE}
In the falling object, father-son heights, and mouse weight examples, the random variables associated with the data were represented by $Y_1,\dots,Y_n$. We can think of this as a vector. In fact, in R we are already doing this:
```
&emsp;&emsp;在下落物体、父亲和儿子升高以及老鼠体重的例子中，与这些数据相关的随机变量我们用$Y_1,\dots,Y_n$来表示。我们可以认为这些数据为一个向量。实际上，在R语言中我们已经做了相关的工作。
```{r,message=FALSE}
data(father.son,package="UsingR")
y=father.son$fheight
head(y)
```
```{block2 echo=FALSE}
In math we can also use just one symbol. We usually use bold to distinguish it from the individual entries:
```
&emsp;&emsp;在数学中，我们也可以只用一个符号来表示.我们通常使用粗体来区分它与单个条目。
$$ \mathbf{Y} = \begin{pmatrix}
Y_1\\\
Y_2\\\
\vdots\\\
Y_N
\end{pmatrix}
$$

```{block2 echo=FALSE}
For reasons that will soon become clear, default representation of data vectors have dimension $N\times 1$ as opposed to $1 \times N$ .

Here we don't always use bold because normally one can tell what is a matrix from the context.

Similarly, we can use math notation to represent the covariates or predictors. In a case with two predictors we can represent them like this:
```
&emsp;&emsp;原因很快就会清晰，数据向量的默认表示方法是具有维度的$N\times 1$而不是$1 \times N$。这里我们并不总是使用粗体，因为通常我们可以从上下文中看出什么是矩阵。类似的，我们可以用数学符号来表示协变量或预测因子。在一个有两个预测因子的情况下，我们可以这样表示它们：
$$ 
\mathbf{X}_1 = \begin{pmatrix}
x_{1,1}\\
\vdots\\
x_{N,1}
\end{pmatrix} \mbox{ and }
\mathbf{X}_2 = \begin{pmatrix}
x_{1,2}\\
\vdots\\
x_{N,2}
\end{pmatrix}
$$
```{block2 echo=FALSE}
Note that for the falling object example $x_{1,1}= t_i$ and $x_{i,1}=t_i^2$ with $t_i$ the time of the i-th observation. Also, keep in mind that vectors can be thought of as $N\times 1$ matrices.

For reasons that will soon become apparent, it is convenient to represent these in matrices:
```
&emsp;&emsp;请注意在下降物体的例子中$x_{1,1}= t_i$ and $x_{i,1}=t_i^2$和观测i-th的时间$t_i$。同样，请记住向量可以被认为是$N\times 1$的矩阵。在矩阵中表示这些是很方便的，因此原因很快就会显现出来，。
$$ 
\mathbf{X} = [ \mathbf{X}_1 \mathbf{X}_2 ] = \begin{pmatrix}
x_{1,1}&x_{1,2}\\
\vdots\\
x_{N,1}&x_{N,2}
\end{pmatrix}
$$
```{block2 echo=FALSE}
This matrix has dimension $N \times 2$. We can create this matrix in R this way:
```
&emsp;&emsp;这个矩阵的维度是$N \times 2$。我们可以用R语言的方式来创建这个矩阵：
```{r}
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, t is a base function
X <- cbind(X1=tt,X2=tt^2)
head(X)
dim(X)
```
```{block2 echo=FALSE}
We can also use this notation to denote an arbitrary number of covariates with the following $N\times p$ matrix:
```
&emsp;&emsp;我们也通过利用$N\times p$这个矩阵符号来表示任意数量的协变量：
$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix}
$$
```{block2 echo=FALSE}
Just as an example, we show you how to make one in R now using `matrix` instead of `cbind`:
```
&emsp;&emsp;举个例子，我们将展示在R语言中如何使用`matrix` 而不是 `cbind`来创建矩阵：
```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p)
head(X)
dim(X)
```
```{block2 echo=FALSE}
By default, the matrices are filled column by column. The `byrow=TRUE` argument lets us change that to row by row:
```
&emsp;&emsp;默认的，在R语言中矩阵是按列填充的。参数`byrow=TRUE`能够让我们以按行填充的方式来创建矩阵。
```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p,byrow=TRUE)
head(X)
```
```{block2 echo=FALSE}
Finally, we define a scalar. A scalar is just a number, which we call a scalar because we want to distinguish it from vectors and matrices. We usually use lower case and don't bold. In the next section, we will understand why we make this distinction.
```
&emsp;&emsp;最后，我们定义一个标量。标量就是一个数字，之所以称之为标量是因为我们想要把它和向量以及矩阵区分开。我们通常用小写，不加粗的方式来表示。在下一节中，我们将明白我们为什么要做出这样的一个区分。
```{block2 echo=FALSE}
## Matrix Operations
```
## 矩阵运算
```{block2 echo=FALSE}
In a previous section, we motivated the use of matrix algebra with this system of equations:
```
&emsp;&emsp;在前一章节中，我们用下面的这个系统方程来激发了对矩阵代数的使用：

$$
\begin{align*}
a + b + c &= 6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$
```{block2 echo=FALSE}
We described how this system can be rewritten and solved using matrix algebra:
```
我们描述了如何使用矩阵代数来重写和求解这个系统：
$$
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix} =
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
\implies
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}=
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}^{-1}
\begin{pmatrix}
6\\
2\\
1
\end{pmatrix}
$$
```{block2 echo=FALSE}
Having described matrix notation, we will explain the operation we perform with them. For example, above we have matrix multiplication and we also have a symbol representing the inverse of a matrix. The importance of these operations and others will become clear once we present specific examples related to data analysis.
 ```
 &emsp;&emsp;之前已经对矩阵有了相关的描述，接下来，我们将解释如何来操纵它们。例如：上面我们已经构建的矩阵乘法和我们也有了一个表示逆矩阵的符号。一旦我们提供了与数据分析相关的具体例子，这些以及其他的重要操作将会变得清晰起来。
```{block2 echo=FALSE}
#### Multiplying by a scalar
```
#### 标量的乘法
```{block2 echo=FALSE}
We start with one of the simplest operations: scalar multiplication. If $a$ is scalar and $\mathbf{X}$ is a matrix, then:
```
&emsp;&emsp;我们从最简单的一个操作开始：标量乘法。如果$a$是一个标量， $\mathbf{X}$ 是一个矩阵，那么：

$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix} \implies
a \mathbf{X} = 
\begin{pmatrix}
  a x_{1,1} & \dots & a x_{1,p}\\
  a x_{2,1}&\dots & a x_{2,p} \\
  & \vdots & \\
  a x_{N,1} & \dots & a  x_{N,p}
\end{pmatrix}
$$
```{block2 echo=FALSE}
R automatically follows this rule when we multiply a number by a matrix using `*`:
```
当我们用`*`来表示一个数字乘以一个矩阵时，R会自动遵循这个规则：
```{r}
X <- matrix(1:12,4,3)
print(X)
a <- 2
print(a*X)
```

```{block2 echo=FALSE}
#### The transpose
```
#### 转置
```{block2 echo=FALSE}
The transpose is an operation that simply changes columns to rows. We use a $\top$ to denote a transpose. The technical definition is as follows: if X is as we defined it above, here is the transpose which will be $p\times N$:
```
&emsp;&emsp;转置是一个简单地将列更改为行的操作。我们利用$\top$来声明一个转置。技术上的定义如下所示：如果X是和我们上面定义的矩阵一样，在这里我们用$p\times N$来表示其转置矩阵：
$$
\mathbf{X} = \begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix} \implies
\mathbf{X}^\top = \begin{pmatrix}
  x_{1,1}&\dots & x_{p,1} \\
  x_{1,2}&\dots & x_{p,2} \\
   & \vdots & \\
  x_{1,N}&\dots & x_{p,N} 
  \end{pmatrix}
$$
```{block2 echo=FALSE}
In R we simply use `t`:
```
在R语言中我们只需简单是使用`t`就可以完成：
```{r}
X <- matrix(1:12,4,3)
X
t(X)
```
```{block2 echo=FALSE}
#### Matrix multiplication
```
#### 矩阵乘法
```{block2 echo=FALSE}
We start by describing the matrix multiplication shown in the original system of equations example:
```
&emsp;&emsp;我们首先从描述原始系统方程实例中所示的矩阵乘法开始：
$$
\begin{align*}
a + b + c &=6\\
3a - 2b + c &= 2\\
2a + b  - c &= 1
\end{align*}
$$
```{block2 echo=FALSE}
What we are doing is multiplying the rows of the first matrix by the columns of the second. Since the second matrix only has one column, we perform this multiplication by doing the following:
```
我们要做的是将第一个矩阵的行乘以第二个矩阵的列，由于第二个矩阵只有一列，因此我们要通过以下步骤来执行这个乘法运算：
$$
\,
\begin{pmatrix}
1&1&1\\
3&-2&1\\
2&1&-1
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}=
\begin{pmatrix}
a + b + c \\
3a - 2b + c \\
2a + b  - c 
\end{pmatrix}
$$
```{block2 echo=FALSE}
Here is a simple example. We can check to see if `abc=c(3,2,1)` is a solution:
```
这里有一个简单的例子。我们可以检查`abc = c（3,2,1）`是否是一个解决方案：
```{r}
X  <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
abc <- c(3,2,1) #use as an example
rbind( sum(X[1,]*abc), sum(X[2,]*abc), sum(X[3,]*abc))
```
```{block2 echo=FALSE}
We can use the `%*%` to perform the matrix multiplication and make this much more compact:
```
我们可以使用`%*%`来实现矩阵乘法并且让其变得更紧凑： 

```{r}
X%*%abc
```

```{block2 echo=FALSE}
We can see that `c(3,2,1)` is not a solution as the answer here is not the required `c(6,2,1)`.

To get the solution, we will need to invert the matrix on the left, a concept we learn about below.

Here is the general definition of matrix multiplication of matrices $A$ and $X$:
```
我们发现`c(3,2,1)`不是一个解决办法，因为通过以上运算得到的答案不是我们想要得到的`c(6,2,1)`。为了能够解决这个问题，我们需要把矩阵向左翻转，这是我们接下来要学的一个概念。下面是对矩阵$A$和$X$的矩阵乘法的一般定义$：
$$
\mathbf{AX} = \begin{pmatrix}
  a_{1,1} & a_{1,2} & \dots & a_{1,N}\\
  a_{2,1} & a_{2,2} & \dots & a_{2,N}\\
  & & \vdots & \\
  a_{M,1} & a_{M,2} & \dots & a_{M,N}
\end{pmatrix}
\begin{pmatrix}
  x_{1,1}&\dots & x_{1,p} \\
  x_{2,1}&\dots & x_{2,p} \\
   & \vdots & \\
  x_{N,1}&\dots & x_{N,p} 
  \end{pmatrix}
$$
  
$$  = \begin{pmatrix}
  \sum_{i=1}^N a_{1,i} x_{i,1} & \dots & \sum_{i=1}^N a_{1,i} x_{i,p}\\
  & \vdots & \\
  \sum_{i=1}^N a_{M,i} x_{i,1} & \dots & \sum_{i=1}^N a_{M,i} x_{i,p}
\end{pmatrix}
$$
```{block2 echo=FALSE}
You can only take the product if the number of columns of the first matrix $A$ equals the number of rows of the second one $X$. Also, the final matrix has the same row numbers as the first $A$ and the same column numbers as the second $X$. 
After you study the example below, you may want to come back and re-read the sections above.
```
如果第一个矩阵$A$的列数等于第二个矩阵$X$的行数，则只能得到这个乘积。同样的，你最终得到的矩阵和矩阵$A$的行数是一样的，和第二个矩阵$X$的列数是一样的。在你学习下面的例子之后，你可能想回来重读上面的章节。
```{block2 echo=FALSE}
#### The identity matrix
```
#### 单位矩阵
```{block2 echo=FALSE}
The identity matrix is analogous to the number 1: if you multiply the identity matrix by another matrix, you get the same matrix. For this to happen, we need it to be like this:
```
&emsp;&emsp;单位矩阵类似于1:如果你将单位矩阵乘以另一个矩阵，你将得到同样的一个矩阵。为了得到这个结果，我们需要矩阵像这样：
$$
\mathbf{I} = \begin{pmatrix}
1&0&0&\dots&0&0\\
0&1&0&\dots&0&0\\
0&0&1&\dots&0&0\\
\vdots &\vdots & \vdots&\ddots&\vdots&\vdots\\
0&0&0&\dots&1&0\\
0&0&0&\dots&0&1
\end{pmatrix}
$$
```{block2 echo=FALSE}
By this definition, the identity always has to have the same number of rows as columns or be what we call a square matrix.

If you follow the matrix multiplication rule above, you notice this works out:
```
按照这个定义，单位矩阵必须含有相同的列数和行数，或者我们称之为是一个平方矩阵。如果你遵循上面的矩阵乘法法则，你注意它产生结果：
$$
\mathbf{XI} = 
\begin{pmatrix}
   x_{1,1} & \dots &  x_{1,p}\\
  & \vdots & \\
   x_{N,1} & \dots &   x_{N,p}
\end{pmatrix}
\begin{pmatrix}
1&0&0&\dots&0&0\\
0&1&0&\dots&0&0\\
0&0&1&\dots&0&0\\
 & & &\vdots& &\\
0&0&0&\dots&1&0\\
0&0&0&\dots&0&1
\end{pmatrix} = 
\begin{pmatrix}
   x_{1,1} & \dots &  x_{1,p}\\
  & \vdots & \\
   x_{N,1} & \dots & x_{N,p}
\end{pmatrix}
$$

```{block2 echo=FALSE}
In R you can form an identity matrix this way:
```
在R语言中你可以这样来创建一个单位矩阵：
```{r}
n <- 5 #pick dimensions
diag(n)
```
```{block2 echo=FALSE}
#### The inverse
```
#### 逆矩阵
```{block2 echo=FALSE}
The inverse of matrix $X$, denoted with $X^{-1}$, has the property that, when multiplied, gives you the identity $X^{-1}X=I$. Of course, not all matrices have inverses. For example, a $2\times 2$ matrix with 1s in all its entries does not have an inverse. 

As we will see when we get to the section on applications to linear models, being able to compute the inverse of a matrix is quite useful. A very convenient aspect of R is that it includes a predefined function `solve` to do this. Here is how we would use it to solve the linear of equations:
```
&emsp;&emsp;矩阵$X$的逆矩阵用$X^{-1}$来表示，当相乘的时候，它有$X^{-1}X=I$相同的属性。当然，不是所有的矩阵都有逆矩阵。例如，当一个$2\times 2$ 的矩阵与一个数相乘时得到的所有的条目都没有逆矩阵。正如我们将看到的，我们到了关于线性模型的应用部分，能够计算矩阵的逆矩阵是非常有用的。R语言的一个非常便利的方面是它包含了一个预定义的函数`solve`来解决这个问题。下面我们就将演示如何用它来解线性方程。
```{r}
X <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
y <- matrix(c(6,2,1),3,1)
solve(X)%*%y #equivalent to solve(X,y)
```
```{block2 echo=FALSE}
Please note that `solve` is a function that should be used with caution as it is not generally numerically stable. We explain this in much more detail in the QR factorization section. 
```
请注意，`solve` 是一个应该谨慎使用的函数，因为它一般不是数值稳定的。我们在将在QR因子分解部分更详细地解释了这一点。

## Examples 

Now we are ready to see how matrix algebra can be useful when analyzing data. We start with some simple examples and eventually arrive at the main one: how to write linear models with matrix algebra notation and solve the least squares problem.


#### The average

To compute the sample average and variance of our data, we use these formulas $\bar{Y}=\frac{1}{N} Y_i$ and $\mbox{var}(Y)=\frac{1}{N} \sum_{i=1}^N (Y_i - \bar{Y})^2$. We can represent these with matrix multiplication. First, define this $N \times 1$ matrix made just of 1s:

$$
A=\begin{pmatrix}
1\\
1\\
\vdots\\
1
\end{pmatrix}
$$

This implies that:

$$
\frac{1}{N}
\mathbf{A}^\top Y = \frac{1}{N}
\begin{pmatrix}1&1&\dots&1\end{pmatrix}
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}=
\frac{1}{N} \sum_{i=1}^N Y_i
= \bar{Y}
$$

Note that we are multiplying by the scalar $1/N$. In R, we multiply matrix using `%*%`:

```{r,message=FALSE}
data(father.son,package="UsingR")
y <- father.son$sheight
print(mean(y))

N <- length(y)
Y<- matrix(y,N,1)
A <- matrix(1,N,1)
barY=t(A)%*%Y / N

print(barY)
```

#### The variance

As we will see later, multiplying the transpose of a matrix with another is very common in statistics. In fact, it is so common that there is a function in R:

```{r}
barY=crossprod(A,Y) / N
print(barY)
```

For the variance, we note that if:

$$
\mathbf{r}\equiv \begin{pmatrix}
Y_1 - \bar{Y}\\
\vdots\\
Y_N - \bar{Y}
\end{pmatrix}, \,\,
\frac{1}{N} \mathbf{r}^\top\mathbf{r} = 
\frac{1}{N}\sum_{i=1}^N (Y_i - \bar{Y})^2
$$

In R, if you only send one matrix into `crossprod`, it computes: $r^\top r$ so we can simply type:

```{r}
r <- y - barY
crossprod(r)/N
```

Which is almost equivalent to:
```{r}
library(rafalib)
popvar(y) 
```

#### Linear models

Now we are ready to put all this to use. Let's start with Galton's example. If we define these matrices:
 
$$
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
,
\mathbf{X} = \begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
,
\mathbf{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} \mbox{ and }
\mathbf{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$



Then we can write the model:

$$ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i=1,\dots,N 
$$

as: 


$$
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix} = 
\begin{pmatrix}
1&x_1\\
1&x_2\\
\vdots\\
1&x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
$$

or simply: 

$$
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
$$

which is a much simpler way to write it. 


The least squares equation becomes simpler as well since it is the following cross-product:

$$
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})
$$

So now we are ready to determine which values of $\beta$ minimize the above, which we  can do  using calculus to find the minimum. 

#### Advanced: Finding the minimum using calculus

There are a series of rules that permit us to compute partial derivative equations in matrix notation. By equating the derivative to 0 and solving for the $\beta$, we will have our solution. The only one we need here tells us that the derivative of the above equation is:

$$
2 \mathbf{X}^\top (\mathbf{Y} - \mathbf{X} \boldsymbol{\hat{\beta}})=0
$$

$$
\mathbf{X}^\top \mathbf{X} \boldsymbol{\hat{\beta}} = \mathbf{X}^\top \mathbf{Y}   
$$


$$
\boldsymbol{\hat{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}   
$$

and we have our solution. We usually put a hat on the $\beta$ that solves this, $\hat{\beta}$ , as it is an estimate of the "real" $\beta$ that generated the data.

Remember that the least squares are like a square (multiply something by itself) and that this formula is similar to the derivative of $f(x)^2$ being $2f(x)f\prime (x)$. 


#### Finding LSE in R

Let's see how it works in R:

```{r}
data(father.son,package="UsingR")
x=father.son$fheight
y=father.son$sheight
X <- cbind(1,x)
betahat <- solve( t(X) %*% X ) %*% t(X) %*% y
###or
betahat <- solve( crossprod(X) ) %*% crossprod( X, y )
```


Now we can see the results of this by computing the estimated $\hat{\beta}_0+\hat{\beta}_1 x$ for any value of $x$:

```{r galton_regression_line, fig.cap="Galton's data with fitted regression line."}
newx <- seq(min(x),max(x),len=100)
X <- cbind(1,newx)
fitted <- X%*%betahat
plot(x,y,xlab="Father's height",ylab="Son's height")
lines(newx,fitted,col=2)
```

This $\hat{\boldsymbol{\beta}}=(\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}$ is one of the most widely used results in data analysis. One of the advantages of this approach is that we can use it in many different situations.  For example, in our falling object problem: 
 
```{r}
set.seed(1)
g <- 9.8 #meters per second
n <- 25
tt <- seq(0,3.4,len=n) #time in secs, t is a base function
d <- 56.67  - 0.5*g*tt^2 + rnorm(n,sd=1)
```

Notice that we are using almost the same exact code:


```{r gravity_with_fitted_parabola, fig.cap="Fitted parabola to simulated data for distance travelled versus time of falling object measured with error."}
X <- cbind(1,tt,tt^2)
y <- d
betahat <- solve(crossprod(X))%*%crossprod(X,y)
newtt <- seq(min(tt),max(tt),len=100)
X <- cbind(1,newtt,newtt^2)
fitted <- X%*%betahat
plot(tt,y,xlab="Time",ylab="Height")
lines(newtt,fitted,col=2)
```

And the resulting estimates are what we expect:

```{r}
betahat
```

The Tower of Pisa is about 56 meters high. Since we are just dropping the object there is no initial velocity, and half the constant of gravity is 9.8/2=4.9 meters per second squared.

#### The `lm` Function
R has a very convenient function that fits these models. We will learn more about this function later, but here is a preview:

```{r}
X <- cbind(tt,tt^2)
fit=lm(y~X)
summary(fit)
```

Note that we obtain the same values as above.

#### Summary

We have shown how to write linear models using linear algebra. We are going to do this for several examples, many of which are related to designed experiments. We also demonstrated how to obtain least squares estimates. Nevertheless, it is important to remember that because $y$ is a random variable, these estimates are random as well. In a later section, we will learn how to compute standard error for these estimates and use this to perform inference.




