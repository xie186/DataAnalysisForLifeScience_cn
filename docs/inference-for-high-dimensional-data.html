<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>生物信息R数据分析</title>
  <meta name="description" content="生物信息R数据分析">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="生物信息R数据分析" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.jpg" />
  <meta property="og:description" content="生物信息R数据分析" />
  <meta name="github-repo" content="xie186/HarvardDataScienceForLifeScience_cn" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="生物信息R数据分析" />
  
  <meta name="twitter:description" content="生物信息R数据分析" />
  <meta name="twitter:image" content="images/cover.jpg" />

<meta name="author" content="作者：Rafael A. Irizarry; Mike I. Love 翻译：张三 李四 麻子">


<meta name="date" content="2017-11-17">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linear-models-1.html">
<link rel="next" href="statistical-models.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">生物信息R数据分析</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover picture</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a><ul>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who Will Find This Book Useful?</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#what-does-this-book-cover"><i class="fa fa-check"></i>What Does This Book Cover?</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#how-is-this-book-different"><i class="fa fa-check"></i>How Is This Book Different?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>1.1</b> Installing R</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>1.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#learn-r-basics"><i class="fa fa-check"></i><b>1.3</b> Learn R Basics</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#installing-packages"><i class="fa fa-check"></i><b>1.4</b> Installing Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#importing-data-into-r"><i class="fa fa-check"></i><b>1.5</b> Importing Data into R</a></li>
<li class="chapter" data-level="1.6" data-path="getting-started.html"><a href="getting-started.html#brief-introduction-to-dplyr"><i class="fa fa-check"></i><b>1.6</b> Brief Introduction to <code>dplyr</code></a></li>
<li class="chapter" data-level="1.7" data-path="getting-started.html"><a href="getting-started.html#mathematical-notation"><i class="fa fa-check"></i><b>1.7</b> Mathematical Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="inference.html"><a href="inference.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="inference.html"><a href="inference.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="inference.html"><a href="inference.html#the-null-hypothesis"><i class="fa fa-check"></i><b>2.3</b> The Null Hypothesis</a></li>
<li class="chapter" data-level="2.4" data-path="inference.html"><a href="inference.html#distributions"><i class="fa fa-check"></i><b>2.4</b> Distributions</a></li>
<li class="chapter" data-level="2.5" data-path="inference.html"><a href="inference.html#probability-distribution"><i class="fa fa-check"></i><b>2.5</b> Probability Distribution</a></li>
<li class="chapter" data-level="2.6" data-path="inference.html"><a href="inference.html#normal-distribution"><i class="fa fa-check"></i><b>2.6</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.7" data-path="inference.html"><a href="inference.html#populations-samples-and-estimates"><i class="fa fa-check"></i><b>2.7</b> Populations, Samples and Estimates</a></li>
<li class="chapter" data-level="2.8" data-path="inference.html"><a href="inference.html#central-limit-theorem-and-t-distribution"><i class="fa fa-check"></i><b>2.8</b> Central Limit Theorem and t-distribution</a></li>
<li class="chapter" data-level="2.9" data-path="inference.html"><a href="inference.html#central-limit-theorem-in-practice"><i class="fa fa-check"></i><b>2.9</b> Central Limit Theorem in Practice</a></li>
<li class="chapter" data-level="2.10" data-path="inference.html"><a href="inference.html#t-tests-in-practice"><i class="fa fa-check"></i><b>2.10</b> t-tests in Practice</a></li>
<li class="chapter" data-level="2.11" data-path="inference.html"><a href="inference.html#the-t-distribution-in-practice"><i class="fa fa-check"></i><b>2.11</b> The t-distribution in Practice</a></li>
<li class="chapter" data-level="2.12" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>2.12</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.13" data-path="inference.html"><a href="inference.html#power-calculations"><i class="fa fa-check"></i><b>2.13</b> Power Calculations</a></li>
<li class="chapter" data-level="2.14" data-path="inference.html"><a href="inference.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>2.14</b> Monte Carlo Simulation</a></li>
<li class="chapter" data-level="2.15" data-path="inference.html"><a href="inference.html#parametric-simulations-for-the-observations"><i class="fa fa-check"></i><b>2.15</b> Parametric Simulations for the Observations</a></li>
<li class="chapter" data-level="2.16" data-path="inference.html"><a href="inference.html#permutation-tests"><i class="fa fa-check"></i><b>2.16</b> Permutation Tests</a></li>
<li class="chapter" data-level="2.17" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>2.17</b> Association Tests</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>3.1</b> Quantile Quantile Plots</a></li>
<li class="chapter" data-level="3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots"><i class="fa fa-check"></i><b>3.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatterplots-and-correlation"><i class="fa fa-check"></i><b>3.3</b> Scatterplots and Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>3.4</b> Stratification</a></li>
<li class="chapter" data-level="3.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>3.5</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="3.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plots-to-avoid"><i class="fa fa-check"></i><b>3.6</b> Plots to Avoid</a></li>
<li class="chapter" data-level="3.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#misunderstanding-correlation-advanced"><i class="fa fa-check"></i><b>3.7</b> Misunderstanding Correlation (Advanced)</a></li>
<li class="chapter" data-level="3.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#robust-summaries"><i class="fa fa-check"></i><b>3.8</b> Robust Summaries</a></li>
<li class="chapter" data-level="3.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>3.9</b> Wilcoxon Rank Sum Test</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>4</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#motivating-examples"><i class="fa fa-check"></i><b>4.1</b> Motivating Examples</a></li>
<li class="chapter" data-level="4.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrix-notation"><i class="fa fa-check"></i><b>4.2</b> Matrix Notation</a></li>
<li class="chapter" data-level="4.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#solving-systems-of-equations"><i class="fa fa-check"></i><b>4.3</b> Solving Systems of Equations</a></li>
<li class="chapter" data-level="4.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#vectors-matrices-and-scalars"><i class="fa fa-check"></i><b>4.4</b> Vectors, Matrices, and Scalars</a></li>
<li class="chapter" data-level="4.5" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrix-operations"><i class="fa fa-check"></i><b>4.5</b> Matrix Operations</a></li>
<li class="chapter" data-level="4.6" data-path="matrix-algebra.html"><a href="matrix-algebra.html#examples"><i class="fa fa-check"></i><b>4.6</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-models-1.html"><a href="linear-models-1.html"><i class="fa fa-check"></i><b>5</b> Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-models-1.html"><a href="linear-models-1.html#the-design-matrix"><i class="fa fa-check"></i><b>5.1</b> The Design Matrix</a></li>
<li class="chapter" data-level="5.2" data-path="linear-models-1.html"><a href="linear-models-1.html#the-mathematics-behind-lm"><i class="fa fa-check"></i><b>5.2</b> The Mathematics Behind lm()</a></li>
<li class="chapter" data-level="5.3" data-path="linear-models-1.html"><a href="linear-models-1.html#standard-errors"><i class="fa fa-check"></i><b>5.3</b> Standard Errors</a></li>
<li class="chapter" data-level="5.4" data-path="linear-models-1.html"><a href="linear-models-1.html#interactions-and-contrasts"><i class="fa fa-check"></i><b>5.4</b> Interactions and Contrasts</a></li>
<li class="chapter" data-level="5.5" data-path="linear-models-1.html"><a href="linear-models-1.html#linear-model-with-interactions"><i class="fa fa-check"></i><b>5.5</b> Linear Model with Interactions</a></li>
<li class="chapter" data-level="5.6" data-path="linear-models-1.html"><a href="linear-models-1.html#analysis-of-variance"><i class="fa fa-check"></i><b>5.6</b> Analysis of Variance</a></li>
<li class="chapter" data-level="5.7" data-path="linear-models-1.html"><a href="linear-models-1.html#collinearity"><i class="fa fa-check"></i><b>5.7</b> Collinearity</a></li>
<li class="chapter" data-level="5.8" data-path="linear-models-1.html"><a href="linear-models-1.html#rank"><i class="fa fa-check"></i><b>5.8</b> Rank</a></li>
<li class="chapter" data-level="5.9" data-path="linear-models-1.html"><a href="linear-models-1.html#removing-confounding"><i class="fa fa-check"></i><b>5.9</b> Removing Confounding</a></li>
<li class="chapter" data-level="5.10" data-path="linear-models-1.html"><a href="linear-models-1.html#the-qr-factorization-advanced"><i class="fa fa-check"></i><b>5.10</b> The QR Factorization (Advanced)</a></li>
<li class="chapter" data-level="5.11" data-path="linear-models-1.html"><a href="linear-models-1.html#going-further"><i class="fa fa-check"></i><b>5.11</b> Going Further</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html"><i class="fa fa-check"></i><b>6</b> Inference for High Dimensional Data</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#inference-in-practice"><i class="fa fa-check"></i><b>6.2</b> Inference in Practice</a></li>
<li class="chapter" data-level="6.3" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#procedures"><i class="fa fa-check"></i><b>6.3</b> Procedures</a></li>
<li class="chapter" data-level="6.4" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#error-rates"><i class="fa fa-check"></i><b>6.4</b> Error Rates</a></li>
<li class="chapter" data-level="6.5" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#the-bonferroni-correction"><i class="fa fa-check"></i><b>6.5</b> The Bonferroni Correction</a></li>
<li class="chapter" data-level="6.6" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#false-discovery-rate"><i class="fa fa-check"></i><b>6.6</b> False Discovery Rate</a></li>
<li class="chapter" data-level="6.7" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#direct-approach-to-fdr-and-q-values-advanced"><i class="fa fa-check"></i><b>6.7</b> Direct Approach to FDR and q-values (Advanced)</a></li>
<li class="chapter" data-level="6.8" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#basic-exploratory-data-analysis"><i class="fa fa-check"></i><b>6.8</b> Basic Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistical-models.html"><a href="statistical-models.html"><i class="fa fa-check"></i><b>7</b> Statistical Models</a><ul>
<li class="chapter" data-level="7.1" data-path="statistical-models.html"><a href="statistical-models.html#the-binomial-distribution"><i class="fa fa-check"></i><b>7.1</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="7.2" data-path="statistical-models.html"><a href="statistical-models.html#the-poisson-distribution"><i class="fa fa-check"></i><b>7.2</b> The Poisson Distribution</a></li>
<li class="chapter" data-level="7.3" data-path="statistical-models.html"><a href="statistical-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>7.3</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="7.4" data-path="statistical-models.html"><a href="statistical-models.html#distributions-for-positive-continuous-values"><i class="fa fa-check"></i><b>7.4</b> Distributions for Positive Continuous Values</a></li>
<li class="chapter" data-level="7.5" data-path="statistical-models.html"><a href="statistical-models.html#bayesian-statistics"><i class="fa fa-check"></i><b>7.5</b> Bayesian Statistics</a></li>
<li class="chapter" data-level="7.6" data-path="statistical-models.html"><a href="statistical-models.html#hierarchical-models"><i class="fa fa-check"></i><b>7.6</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">本书由 bookdown 强力驱动</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">生物信息R数据分析</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inference-for-high-dimensional-data" class="section level1">
<h1><span class="header-section-number">第 6 章</span> Inference for High Dimensional Data</h1>
<div id="introduction-4" class="section level2">
<h2><span class="header-section-number">6.1</span> Introduction</h2>
<p>High-throughput technologies have changed basic biology and the biomedical sciences from data poor disciplines to data intensive ones. A specific example comes from research fields interested in understanding gene expression. Gene expression is the process in which DNA, the blueprint for life, is copied into RNA, the templates for the synthesis of proteins, the building blocks for life. In the 1990s, the analysis of gene expression data amounted to spotting black dots on a piece of paper or extracting a few numbers from standard curves. With high-throughput technologies, such as microarrays, this suddenly changed to sifting through tens of thousands of numbers. More recently, RNA sequencing has further increased data complexity. Biologists went from using their eyes or simple summaries to categorize results, to having thousands (and now millions) of measurements per sample to analyze. In this chapter, we will focus on statistical inference in the context of high-throughput measurements. Specifically, we focus on the problem of detecting differences in groups using statistical tests and quantifying uncertainty in a meaningful way. We also introduce exploratory data analysis techniques that should be used in conjunction with inference when analyzing high-throughput data. In later chapters, we will study the statistics behind clustering, machine learning, factor analysis and multi-level modeling.</p>
<p>Since there is a vast number of available public datasets, we use several gene expression examples. Nonetheless, the statistical techniques you will learn have also proven useful in other fields that make use of high-throughput technologies. Technologies such as microarrays, next generation sequencing, fMRI, and mass spectrometry all produce data to answer questions for which what we learn here will be indispensable.</p>
<p><a name="threetables"></a></p>
<div id="data-packages" class="section level4">
<h4><span class="header-section-number">6.1.0.1</span> Data packages</h4>
<p>Several of the examples we are going to use in the following sections are best obtained through R packages. These are available from GitHub and can be installed using the <code>install_github</code> function from the <code>devtools</code> package. Microsoft Windows users might need to follow <a href="https://github.com/genomicsclass/windows">these instructions</a> to properly install <code>devtools</code>.</p>
<p>Once <code>devtools</code> is installed, you can then install the data packages like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(devtools)
<span class="kw">install_github</span>(<span class="st">&quot;genomicsclass/GSE5859Subset&quot;</span>)</code></pre></div>
</div>
<div id="the-three-tables" class="section level4">
<h4><span class="header-section-number">6.1.0.2</span> The three tables</h4>
<p>Most of the data we use as examples in this book are created with high-throughput technologies. These technologies measure thousands of <em>features</em>. Examples of features are genes, single base locations of the genome, genomic regions, or image pixel intensities. Each specific measurement product is defined by a specific set of features. For example, a specific gene expression microarray product is defined by the set of genes that it measures.</p>
<p>A specific study will typically use one product to make measurements on several experimental units, such as individuals. The most common experimental unit will be the individual, but they can also be defined by other entities, for example different parts of a tumor. We often call the experimental units <em>samples</em> following experimental jargon. It is important that these are not confused with samples as referred to in previous chapters, for example “random sample”.</p>
<p>So a high-throughput experiment is usually defined by three tables: one with the high-throughput measurements and two tables with information about the columns and rows of this first table respectively.</p>
<p>Because a dataset is typically defined by a set of experimental units and a product defines a fixed set of features, the high-throughput measurements can be stored in an <span class="math inline">\(n \times m\)</span> matrix, with <span class="math inline">\(n\)</span> the number of units and <span class="math inline">\(m\)</span> the number of features. In R, the convention has been to store the transpose of these matrices.</p>
<p>Here is an example from a gene expression dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSE5859Subset)
<span class="kw">data</span>(GSE5859Subset) ##this loads the three tables
<span class="kw">dim</span>(geneExpression)</code></pre></div>
<pre><code>## [1] 8793   24</code></pre>
<p>We have RNA expression measurements for 8793 genes from blood taken from 24 individuals (the experimental units). For most statistical analyses, we will also need information about the individuals. For example, in this case the data was originally collected to compare gene expression across ethnic groups. However, we have created a subset of this dataset for illustration and separated the data into two groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(sampleInfo)</code></pre></div>
<pre><code>## [1] 24  4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(sampleInfo)</code></pre></div>
<pre><code>##     ethnicity       date         filename group
## 107       ASN 2005-06-23 GSM136508.CEL.gz     1
## 122       ASN 2005-06-27 GSM136530.CEL.gz     1
## 113       ASN 2005-06-27 GSM136517.CEL.gz     1
## 163       ASN 2005-10-28 GSM136576.CEL.gz     1
## 153       ASN 2005-10-07 GSM136566.CEL.gz     1
## 161       ASN 2005-10-07 GSM136574.CEL.gz     1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sampleInfo<span class="op">$</span>group</code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0</code></pre>
<p>One of the columns, filenames, permits us to connect the rows of this table to the columns of the measurement table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">match</span>(sampleInfo<span class="op">$</span>filename,<span class="kw">colnames</span>(geneExpression))</code></pre></div>
<pre><code>##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17
## [18] 18 19 20 21 22 23 24</code></pre>
<p>Finally, we have a table describing the features:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(geneAnnotation)</code></pre></div>
<pre><code>## [1] 8793    4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(geneAnnotation)</code></pre></div>
<pre><code>##      PROBEID  CHR     CHRLOC SYMBOL
## 1  1007_s_at chr6   30852327   DDR1
## 30   1053_at chr7  -73645832   RFC2
## 31    117_at chr1  161494036  HSPA6
## 32    121_at chr2 -113973574   PAX8
## 33 1255_g_at chr6   42123144 GUCA1A
## 34   1294_at chr3  -49842638   UBA7</code></pre>
<p>The table includes an ID that permits us to connect the rows of this table with the rows of the measurement table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">match</span>(geneAnnotation<span class="op">$</span>PROBEID,<span class="kw">rownames</span>(geneExpression)))</code></pre></div>
<pre><code>## [1] 1 2 3 4 5 6</code></pre>
<p>The table also includes biological information about the features, namely chromosome location and the gene “name” used by biologists.</p>
</div>
</div>
<div id="inference-in-practice" class="section level2">
<h2><span class="header-section-number">6.2</span> Inference in Practice</h2>
<p>Suppose we were given high-throughput gene expression data that was measured for several individuals in two populations. We are asked to report which genes have different average expression levels in the two populations. If instead of thousands of genes, we were handed data from just one gene, we could simply apply the inference techniques that we have learned before. We could, for example, use a t-test or some other test. Here we review what changes when we consider high-throughput data.</p>
<div id="p-values-are-random-variables" class="section level4">
<h4><span class="header-section-number">6.2.0.1</span> p-values are random variables</h4>
<p>An important concept to remember in order to understand the concepts presented in this chapter is that p-values are random variables. To see this, consider the example in which we define a p-value from a t-test with a large enough sample size to use the CLT approximation. Then our p-value is defined as the probability that a normally distributed random variable is larger, in absolute value, than the observed t-test, call it <span class="math inline">\(Z\)</span>. So for a two sided test the p-value is:</p>
<p><span class="math display">\[
p = 2 \{ 1 - \Phi(\mid Z \mid)\}
\]</span></p>
<p>In R, we write:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span>( <span class="dv">1</span><span class="op">-</span><span class="kw">pnorm</span>( <span class="kw">abs</span>(Z) ) )</code></pre></div>
<p>Now because <span class="math inline">\(Z\)</span> is a random variable and <span class="math inline">\(\Phi\)</span> is a deterministic function, <span class="math inline">\(p\)</span> is also a random variable. We will create a Monte Carlo simulation showing how the values of <span class="math inline">\(p\)</span> change. We use <code>femaleControlsPopulation.csv</code> from earlier chapters.</p>
<p>We read in the data, and use <code>replicate</code> to repeatedly create p-values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
population =<span class="st"> </span><span class="kw">unlist</span>( <span class="kw">read.csv</span>(filename) )
N &lt;-<span class="st"> </span><span class="dv">12</span>
B &lt;-<span class="st"> </span><span class="dv">10000</span>
pvals &lt;-<span class="st"> </span><span class="kw">replicate</span>(B,{
  control =<span class="st"> </span><span class="kw">sample</span>(population,N)
  treatment =<span class="st"> </span><span class="kw">sample</span>(population,N)
  <span class="kw">t.test</span>(treatment,control)<span class="op">$</span>p.val 
  })
<span class="kw">hist</span>(pvals)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/pvalue_hist-1.png" alt="P-value histogram for 10,000 tests in which null hypothesis is true." width="672" />
<p class="caption">
(#fig:pvalue_hist)P-value histogram for 10,000 tests in which null hypothesis is true.
</p>
</div>
<p>As implied by the histogram, in this case the distribution of the p-value is uniformly distributed. In fact, we can show theoretically that when the null hypothesis is true, this is always the case. For the case in which we use the CLT, we have that the null hypothesis <span class="math inline">\(H_0\)</span> implies that our test statistic <span class="math inline">\(Z\)</span> follows a normal distribution with mean 0 and SD 1 thus:</p>
<p><span class="math display">\[
p_a = \mbox{Pr}(Z &lt; a \mid H_0) = \Phi(a)
\]</span></p>
<p>This implies that:</p>
<p><span class="math display">\[
\begin{align*}
\mbox{Pr}(p &lt; p_a) &amp;= \mbox{Pr}[ \Phi^{-1}(p) &lt; \Phi^{-1}(p_a) ] \\
  &amp; = \mbox{Pr}(Z &lt; a) = p_a
\end{align*}
\]</span></p>
<p>which is the definition of a uniform distribution.</p>
</div>
<div id="thousands-of-tests" class="section level4">
<h4><span class="header-section-number">6.2.0.2</span> Thousands of tests</h4>
<p>In this data we have two groups denoted with 0 and 1:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSE5859Subset)
<span class="kw">data</span>(GSE5859Subset)
g &lt;-<span class="st"> </span>sampleInfo<span class="op">$</span>group
g</code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0</code></pre>
<p>If we were interested in a particular gene, let’s arbitrarily pick the one on the 25th row, we would simply compute a t-test. To compute a p-value, we will use the t-distribution approximation and therefore we need the population data to be approximately normal. We check this assumption with a qq-plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">e &lt;-<span class="st"> </span>geneExpression[<span class="dv">25</span>,]

<span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)

<span class="kw">qqnorm</span>(e[g<span class="op">==</span><span class="dv">1</span>])
<span class="kw">qqline</span>(e[g<span class="op">==</span><span class="dv">1</span>])

<span class="kw">qqnorm</span>(e[g<span class="op">==</span><span class="dv">0</span>])
<span class="kw">qqline</span>(e[g<span class="op">==</span><span class="dv">0</span>])</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/qqplots_for_one_gene-1.png" alt="Normal qq-plots for one gene. Left plot shows first group and right plot shows second group." width="1008" />
<p class="caption">
(#fig:qqplots_for_one_gene)Normal qq-plots for one gene. Left plot shows first group and right plot shows second group.
</p>
</div>
<p>The qq-plots show that the data is well approximated by the normal approximation. The t-test does not find this gene to be statistically significant:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(e[g<span class="op">==</span><span class="dv">1</span>],e[g<span class="op">==</span><span class="dv">0</span>])<span class="op">$</span>p.value</code></pre></div>
<pre><code>## [1] 0.7793</code></pre>
<p>To answer the question for each gene, we simply repeat the above for each gene. Here we will define our own function and use <code>apply</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">myttest &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">t.test</span>(x[g<span class="op">==</span><span class="dv">1</span>],x[g<span class="op">==</span><span class="dv">0</span>],<span class="dt">var.equal=</span><span class="ot">TRUE</span>)<span class="op">$</span>p.value
pvals &lt;-<span class="st"> </span><span class="kw">apply</span>(geneExpression,<span class="dv">1</span>,myttest)</code></pre></div>
<p>We can now see which genes have p-values less than, say, 0.05. For example, right away we see that…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(pvals<span class="op">&lt;</span><span class="fl">0.05</span>)</code></pre></div>
<pre><code>## [1] 1383</code></pre>
<p>… genes had p-values less than 0.05.</p>
<p>However, as we will describe in more detail below, we have to be careful in interpreting this result because we have performed over 8,000 tests. If we performed the same procedure on random data, for which the null hypothesis is true for all features, we obtain the following results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
m &lt;-<span class="st"> </span><span class="kw">nrow</span>(geneExpression)
n &lt;-<span class="st"> </span><span class="kw">ncol</span>(geneExpression)
randomData &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>m),m,n)
nullpvals &lt;-<span class="st"> </span><span class="kw">apply</span>(randomData,<span class="dv">1</span>,myttest)
<span class="kw">sum</span>(nullpvals<span class="op">&lt;</span><span class="fl">0.05</span>)</code></pre></div>
<pre><code>## [1] 419</code></pre>
<p>As we will explain later in the chapter, this is to be expected: 419 is roughly 0.05*8192 and we will describe the theory that tells us why this prediction works.</p>
</div>
<div id="faster-t-test-implementation" class="section level4">
<h4><span class="header-section-number">6.2.0.3</span> Faster t-test implementation</h4>
<p>Before we continue, we should point out that the above implementation is very inefficient. There are several faster implementations that perform t-test for high-throughput data. We make use of a function that is not available from CRAN, but rather from the Bioconductor project.</p>
<p>To download and install packages from Bioconductor, we can use the <code>install_bioc</code> function in <code>rafalib</code> to install the package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install_bioc</span>(<span class="st">&quot;genefilter&quot;</span>)</code></pre></div>
<p>Now we can show that this function is much faster than our code above and produce practically the same answer:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(genefilter)
results &lt;-<span class="st"> </span><span class="kw">rowttests</span>(geneExpression,<span class="kw">factor</span>(g))
<span class="kw">max</span>(<span class="kw">abs</span>(pvals<span class="op">-</span>results<span class="op">$</span>p))</code></pre></div>
<pre><code>## [1] 6.528e-14</code></pre>
</div>
</div>
<div id="procedures" class="section level2">
<h2><span class="header-section-number">6.3</span> Procedures</h2>
<p>In the previous section we learned how p-values are no longer a useful quantity to interpret when dealing with high-dimensional data. This is because we are testing many <em>features</em> at the same time. We refer to this as the <em>multiple comparison</em> or <em>multiple testing</em> or <em>multiplicity</em> problem. The definition of a p-value does not provide a useful quantification here. Again, because when we test many hypotheses simultaneously, a list based simply on a small p-value cut-off of, say 0.01, can result in many false positives with high probability. Here we define terms that are more appropriate in the context of high-throughput data.</p>
<p>The most widely used approach to the multiplicity problem is to define a <em>procedure</em> and then estimate or <em>control</em> an informative <em>error rate</em> for this procedure. What we mean by <em>control</em> here is that we adapt the procedure to guarantee an <em>error rate</em> below a predefined value. The procedures are typically flexible through parameters or cutoffs that let us control specificity and sensitivity. An example of a procedure is:</p>
<ul>
<li>Compute a p-value for each gene.</li>
<li>Call significant all genes with p-values smaller than <span class="math inline">\(\alpha\)</span>.</li>
</ul>
<p>Note that changing the <span class="math inline">\(\alpha\)</span> permits us to adjust specificity and sensitivity.</p>
<p>Next we define the <em>error rates</em> that we will try to estimate and control.</p>
</div>
<div id="error-rates" class="section level2">
<h2><span class="header-section-number">6.4</span> Error Rates</h2>
<p>Throughout this section we will be using the type I error and type II error terminology. We will also refer to them as false positives and false negatives respectively. We also use the more general terms specificity, which relates to type I error, and sensitivity, which relates to type II errors.</p>
<p>In the context of high-throughput data we can make several type I errors and several type II errors in one experiment, as opposed to one or the other as seen in Chapter 1. In this table, we summarize the possibilities using the notation from the seminal paper by Benjamini-Hochberg:</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Called significant</th>
<th>Not called significant</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Null True</td>
<td><span class="math inline">\(V\)</span></td>
<td><span class="math inline">\(m_0-V\)</span></td>
<td><span class="math inline">\(m_0\)</span></td>
</tr>
<tr class="even">
<td>Alternative True</td>
<td><span class="math inline">\(S\)</span></td>
<td><span class="math inline">\(m_1-S\)</span></td>
<td><span class="math inline">\(m_1\)</span></td>
</tr>
<tr class="odd">
<td>True</td>
<td><span class="math inline">\(R\)</span></td>
<td><span class="math inline">\(m-R\)</span></td>
<td><span class="math inline">\(m\)</span></td>
</tr>
</tbody>
</table>
<p>To describe the entries in the table, let’s use as an example a dataset representing measurements from 10,000 genes, which means that the total number of tests that we are conducting is: <span class="math inline">\(m=10,000\)</span>. The number of genes for which the null hypothesis is true, which in most cases represent the “non-interesting” genes, is <span class="math inline">\(m_0\)</span>, while the number of genes for which the null hypothesis is false is <span class="math inline">\(m_1\)</span>. For this we can also say that the <em>alternative hypothesis</em> is true. In general, we are interested in <em>detecting</em> as many as possible of the cases for which the alternative hypothesis is true (true positives), without incorrectly detecting cases for which the null hypothesis is true (false positives). For most high-throughput experiments, we assume that <span class="math inline">\(m_0\)</span> is much greater than <span class="math inline">\(m_1\)</span>. For example, we test 10,000 expecting 100 genes or less to be <em>interesting</em>. This would imply that <span class="math inline">\(m_1 \leq 100\)</span> and <span class="math inline">\(m_0 \geq 19,900\)</span>.</p>
<p>Throughout this chapter we refer to <em>features</em> as the units being tested. In genomics, examples of features are genes, transcripts, binding sites, CpG sites, and SNPs. In the table, <span class="math inline">\(R\)</span> represents the total number of features that we call significant after applying our procedure, while <span class="math inline">\(m-R\)</span> is the total number of genes we don’t call significant. The rest of the table contains important quantities that are unknown in practice.</p>
<ul>
<li><span class="math inline">\(V\)</span> represents the number of type I errors or false positives. Specifically, <span class="math inline">\(V\)</span> is the number of features for which the null hypothesis is true, that we call significant.</li>
<li><span class="math inline">\(S\)</span> represents the number of true positives. Specifically, <span class="math inline">\(S\)</span> is the number of features for which the alternative is true, that we call significant.</li>
</ul>
<p>This implies that there are <span class="math inline">\(m_1-S\)</span> type II errors or <em>false negatives</em> and <span class="math inline">\(m_0-V\)</span> true negatives. Keep in mind that if we only ran one test, a p-value is simply the probability that <span class="math inline">\(V=1\)</span> when <span class="math inline">\(m=m_0=1\)</span>. Power is the probability of <span class="math inline">\(S=1\)</span> when <span class="math inline">\(m=m_1=1\)</span>. In this very simple case, we wouldn’t bother making the table above, but now we show how defining the terms in the table helps for the high-dimensional setting.</p>
<div id="data-example" class="section level4">
<h4><span class="header-section-number">6.4.0.1</span> Data example</h4>
<p>Let’s compute these quantities with a data example. We will use a Monte Carlo simulation using our mice data to imitate a situation in which we perform tests for 10,000 different fad diets, none of them having an effect on weight. This implies that the null hypothesis is true for diets and thus <span class="math inline">\(m=m_0=10,000\)</span> and <span class="math inline">\(m_1=0\)</span>. Let’s run the tests with a sample size of <span class="math inline">\(N=12\)</span> and compute <span class="math inline">\(R\)</span>. Our procedure will declare any diet achieving a p-value smaller than <span class="math inline">\(\alpha=0.05\)</span> as significant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
population =<span class="st"> </span><span class="kw">unlist</span>( <span class="kw">read.csv</span>(<span class="st">&quot;femaleControlsPopulation.csv&quot;</span>) )
alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
N &lt;-<span class="st"> </span><span class="dv">12</span>
m &lt;-<span class="st"> </span><span class="dv">10000</span>
pvals &lt;-<span class="st"> </span><span class="kw">replicate</span>(m,{
  control =<span class="st"> </span><span class="kw">sample</span>(population,N)
  treatment =<span class="st"> </span><span class="kw">sample</span>(population,N)
  <span class="kw">t.test</span>(treatment,control)<span class="op">$</span>p.value
})</code></pre></div>
<p>Although in practice we do not know the fact that no diet works, in this simulation we do, and therefore we can actually compute <span class="math inline">\(V\)</span> and <span class="math inline">\(S\)</span>. Because all null hypotheses are true, we know, in this specific simulation, that <span class="math inline">\(V=R\)</span>. Of course, in practice we can compute <span class="math inline">\(R\)</span> but not <span class="math inline">\(V\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(pvals <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span>) ##This is R</code></pre></div>
<pre><code>## [1] 462</code></pre>
<p>These many false positives are not acceptable in most contexts.</p>
<p>Here is more complicated code showing results where 10% of the diets are effective with an average effect size of <span class="math inline">\(\Delta= 3\)</span> ounces. Studying this code carefully will help us understand the meaning of the table above. First let’s define <em>the truth</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
N &lt;-<span class="st"> </span><span class="dv">12</span>
m &lt;-<span class="st"> </span><span class="dv">10000</span>
p0 &lt;-<span class="st"> </span><span class="fl">0.90</span> ##10% of diets work, 90% don&#39;t
m0 &lt;-<span class="st"> </span>m<span class="op">*</span>p0
m1 &lt;-<span class="st"> </span>m<span class="op">-</span>m0
nullHypothesis &lt;-<span class="st"> </span><span class="kw">c</span>( <span class="kw">rep</span>(<span class="ot">TRUE</span>,m0), <span class="kw">rep</span>(<span class="ot">FALSE</span>,m1))
delta &lt;-<span class="st"> </span><span class="dv">3</span></code></pre></div>
<p>Now we are ready to simulate 10,000 tests, perform a t-test on each, and record if we rejected the null hypothesis or not:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
calls &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>m, <span class="cf">function</span>(i){
  control &lt;-<span class="st"> </span><span class="kw">sample</span>(population,N)
  treatment &lt;-<span class="st"> </span><span class="kw">sample</span>(population,N)
  <span class="cf">if</span>(<span class="op">!</span>nullHypothesis[i]) treatment &lt;-<span class="st"> </span>treatment <span class="op">+</span><span class="st"> </span>delta
  <span class="kw">ifelse</span>( <span class="kw">t.test</span>(treatment,control)<span class="op">$</span>p.value <span class="op">&lt;</span><span class="st"> </span>alpha, 
          <span class="st">&quot;Called Significant&quot;</span>,
          <span class="st">&quot;Not Called Significant&quot;</span>)
})</code></pre></div>
<p>Because in this simulation we know the truth (saved in <code>nullHypothesis</code>), we can compute the entries of the table:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">null_hypothesis &lt;-<span class="st"> </span><span class="kw">factor</span>( nullHypothesis, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;TRUE&quot;</span>,<span class="st">&quot;FALSE&quot;</span>))
<span class="kw">table</span>(null_hypothesis,calls)</code></pre></div>
<pre><code>##                calls
## null_hypothesis Called Significant
##           TRUE                 421
##           FALSE                520
##                calls
## null_hypothesis Not Called Significant
##           TRUE                    8579
##           FALSE                    480</code></pre>
<p>The first column of the table above shows us <span class="math inline">\(V\)</span> and <span class="math inline">\(S\)</span>. Note that <span class="math inline">\(V\)</span> and <span class="math inline">\(S\)</span> are random variables. If we run the simulation repeatedly, these values change. Here is a quick example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">10</span> ##number of simulations
VandS &lt;-<span class="st"> </span><span class="kw">replicate</span>(B,{
  calls &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>m, <span class="cf">function</span>(i){
    control &lt;-<span class="st"> </span><span class="kw">sample</span>(population,N)
    treatment &lt;-<span class="st"> </span><span class="kw">sample</span>(population,N)
    <span class="cf">if</span>(<span class="op">!</span>nullHypothesis[i]) treatment &lt;-<span class="st"> </span>treatment <span class="op">+</span><span class="st"> </span>delta
    <span class="kw">t.test</span>(treatment,control)<span class="op">$</span>p.val <span class="op">&lt;</span><span class="st"> </span>alpha
  })
  <span class="kw">cat</span>(<span class="st">&quot;V =&quot;</span>,<span class="kw">sum</span>(nullHypothesis <span class="op">&amp;</span><span class="st"> </span>calls), <span class="st">&quot;S =&quot;</span>,<span class="kw">sum</span>(<span class="op">!</span>nullHypothesis <span class="op">&amp;</span><span class="st"> </span>calls),<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)
  <span class="kw">c</span>(<span class="kw">sum</span>(nullHypothesis <span class="op">&amp;</span><span class="st"> </span>calls),<span class="kw">sum</span>(<span class="op">!</span>nullHypothesis <span class="op">&amp;</span><span class="st"> </span>calls))
  })</code></pre></div>
<pre><code>## V = 410 S = 564 
## V = 400 S = 552 
## V = 366 S = 546 
## V = 382 S = 553 
## V = 372 S = 505 
## V = 382 S = 530 
## V = 381 S = 539 
## V = 396 S = 554 
## V = 380 S = 550 
## V = 405 S = 569</code></pre>
<p>This motivates the definition of error rates. We can, for example, estimate probability that <span class="math inline">\(V\)</span> is larger than 0. This is interpreted as the probability of making at least one type I error among the 10,000 tests. In the simulation above, <span class="math inline">\(V\)</span> was much larger than 1 in every single simulation, so we suspect this probability is very practically 1. When <span class="math inline">\(m=1\)</span>, this probability is equivalent to the p-value. When we have a multiple tests situation, we call it the Family Wise Error Rate (FWER) and it relates to a technique that is widely used: The Bonferroni Correction.</p>
</div>
</div>
<div id="the-bonferroni-correction" class="section level2">
<h2><span class="header-section-number">6.5</span> The Bonferroni Correction</h2>
<p>Now that we have learned about the Family Wise Error Rate (FWER), we describe what we can actually do to control it. In practice, we want to choose a <em>procedure</em> that guarantees the FWER is smaller than a predetermined value such as 0.05. We can keep it general and instead of 0.05, use <span class="math inline">\(\alpha\)</span> in our derivations.</p>
<p>Since we are now describing what we do in practice, we no longer have the advantage of knowing <em>the truth</em>. Instead, we pose a procedure and try to estimate the FWER. Let’s consider the naive procedure: “reject all the hypotheses with p-value &lt;0.01”. For illustrative purposes we will assume all the tests are independent (in the case of testing diets this is a safe assumption; in the case of genes it is not so safe since some groups of genes act together). Let <span class="math inline">\(p_1,\dots,p_{10000}\)</span> be the the p-values we get from each test. These are independent random variables so:</p>
<p><span class="math display">\[
\begin{align*}
\mbox{Pr}(\mbox{at least one rejection}) &amp;= 1 -\mbox{Pr}(\mbox{no rejections}) \\
&amp;= 1 - \prod_{i=1}^{1000} \mbox{Pr}(p_i&gt;0.01) \\
&amp;= 1-0.99^{1000} \approx 1
\end{align*}
\]</span></p>
<p>Or if you want to use simulations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B&lt;-<span class="dv">10000</span>
minpval &lt;-<span class="st"> </span><span class="kw">replicate</span>(B, <span class="kw">min</span>(<span class="kw">runif</span>(<span class="dv">10000</span>,<span class="dv">0</span>,<span class="dv">1</span>))<span class="op">&lt;</span><span class="fl">0.01</span>)
<span class="kw">mean</span>(minpval<span class="op">&gt;=</span><span class="dv">1</span>)</code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>So our FWER is 1! This is not what we were hoping for. If we wanted it to be lower than <span class="math inline">\(\alpha=0.05\)</span>, we failed miserably.</p>
<p>So what do we do to make the probability of a mistake lower than <span class="math inline">\(\alpha\)</span> ? Using the derivation above we can change the procedure by selecting a more stringent cutoff, previously 0.01, to lower our probability of at least one mistake to be 5%. Namely, by noting that:</p>
<p><span class="math display">\[\mbox{Pr}(\mbox{at least one rejection}) =  1-(1-k)^{10000}\]</span></p>
<p>and solving for <span class="math inline">\(k\)</span>, we get <span class="math inline">\(1-(1-k)^{10000}=0.01 \implies k = 1-0.99^{1/10000} \approx 1e-6\)</span></p>
<p>This now gives a specific example of a <em>procedure</em>. This one is actually called Sidak’s procedure. Specifically, we define a set of instructions, such as “reject all the null hypothesis for which p-values &lt; 1e-6”. Then, knowing the p-values are random variables, we use statistical theory to compute how many mistakes, on average, we are expected to make if we follow this procedure. More precisely, we compute bounds on these rates; that is, we show that they are smaller than some predetermined value. There is a preference in the life sciences to err on the side of being conservative.</p>
<p>A problem with Sidak’s procedure is that it assumes the tests are independent. It therefore only controls FWER when this assumption holds. The Bonferroni correction is more general in that it controls FWER even if the tests are not independent. As with Sidak’s procedure we start by noting that:</p>
<p><span class="math display">\[FWER = \mbox{Pr}(V&gt;0) \leq \mbox{Pr}(V&gt;0 \mid \mbox{all nulls are true})\]</span></p>
<p>or using the notation from the table above:</p>
<p><span class="math display">\[\mbox{Pr}(V&gt;0) \leq \mbox{Pr}(V&gt;0 \mid m_1=0)\]</span></p>
<p>The Bonferroni procedure sets <span class="math inline">\(k=\alpha/m\)</span> since we can show that:</p>
<p><span class="math display">\[
\begin{align*}
\mbox{Pr}(V&gt;0 \,\mid \, m_1=0) &amp;= \mbox{Pr}\left( \min_i \{p_i\} \leq \frac{\alpha}{m} \mid m_1=0 \right)\\
 &amp;\leq \sum_{i=1}^m \mbox{Pr}\left(p_i \leq \frac{\alpha}{m} \right)\\
 &amp;= m \frac{\alpha}{m}=\alpha
\end{align*}
\]</span></p>
<p>Controlling the FWER at 0.05 is a very conservative approach. Using the p-values computed in the previous section…</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
pvals &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>m, <span class="cf">function</span>(i){
  control &lt;-<span class="st"> </span><span class="kw">sample</span>(population,N)
  treatment &lt;-<span class="st"> </span><span class="kw">sample</span>(population,N)
  <span class="cf">if</span>(<span class="op">!</span>nullHypothesis[i]) treatment &lt;-<span class="st"> </span>treatment <span class="op">+</span><span class="st"> </span>delta
  <span class="kw">t.test</span>(treatment,control)<span class="op">$</span>p.value
})</code></pre></div>
<p>…we note that only:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(pvals <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span><span class="op">/</span><span class="dv">10000</span>)</code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>are called significant after applying the Bonferroni procedure, despite having 1,000 diets that work.</p>
</div>
<div id="false-discovery-rate" class="section level2">
<h2><span class="header-section-number">6.6</span> False Discovery Rate</h2>
<p>There are many situations for which requiring an FWER of 0.05 does not make sense as it is much too strict. For example, consider the very common exercise of running a preliminary small study to determine a handful of candidate genes. This is referred to as a <em>discovery</em> driven project or experiment. We may be in search of an unknown causative gene and more than willing to perform follow-up studies with many more samples on just the candidates. If we develop a procedure that produces, for example, a list of 10 genes of which 1 or 2 pan out as important, the experiment is a resounding success. With a small sample size, the only way to achieve a FWER <span class="math inline">\(\leq\)</span> 0.05 is with an empty list of genes. We already saw in the previous section that despite 1,000 diets being effective, we ended up with a list with just 2. Change the sample size to 6 and you very likely get 0:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
pvals &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span>m, <span class="cf">function</span>(i){
  control &lt;-<span class="st"> </span><span class="kw">sample</span>(population,<span class="dv">6</span>)
  treatment &lt;-<span class="st"> </span><span class="kw">sample</span>(population,<span class="dv">6</span>)
  <span class="cf">if</span>(<span class="op">!</span>nullHypothesis[i]) treatment &lt;-<span class="st"> </span>treatment <span class="op">+</span><span class="st"> </span>delta
  <span class="kw">t.test</span>(treatment,control)<span class="op">$</span>p.value
  })
<span class="kw">sum</span>(pvals <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.05</span><span class="op">/</span><span class="dv">10000</span>)</code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>By requiring a FWER <span class="math inline">\(\leq\)</span> 0.05, we are practically assuring 0 power (sensitivity). In many applications, this specificity requirement is over-kill. A widely used alternative to the FWER is the false discovery rate (FDR). The idea behind FDR is to focus on the random variable <span class="math inline">\(Q \equiv V/R\)</span> with <span class="math inline">\(Q=0\)</span> when <span class="math inline">\(R=0\)</span> and <span class="math inline">\(V=0\)</span>. Note that <span class="math inline">\(R=0\)</span> (nothing called significant) implies <span class="math inline">\(V=0\)</span> (no false positives). So <span class="math inline">\(Q\)</span> is a random variable that can take values between 0 and 1 and we can define a rate by considering the average of <span class="math inline">\(Q\)</span>. To better understand this concept here, we compute <span class="math inline">\(Q\)</span> for the procedure: call everything p-value &lt; 0.05 significant.</p>
<div id="vectorizing-code" class="section level4">
<h4><span class="header-section-number">6.6.0.1</span> Vectorizing code</h4>
<p>Before running the simulation, we are going to <em>vectorize</em> the code. This means that instead of using <code>sapply</code> to run <code>m</code> tests, we will create a matrix with all data in one call to sample. This code runs several times faster than the code above, which is necessary here due to the fact that we will be generating several simulations. Understanding this chunk of code and how it is equivalent to the code above using <code>sapply</code> will take a you long way in helping you code efficiently in R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(genefilter) ##rowttests is here
<span class="kw">set.seed</span>(<span class="dv">1</span>)
##Define groups to be used with rowttests
g &lt;-<span class="st"> </span><span class="kw">factor</span>( <span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">0</span>,N),<span class="kw">rep</span>(<span class="dv">1</span>,N)) )
B &lt;-<span class="st"> </span><span class="dv">1000</span> ##number of simulations
Qs &lt;-<span class="st"> </span><span class="kw">replicate</span>(B,{
  ##matrix with control data (rows are tests, columns are mice)
  controls &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(population, N<span class="op">*</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>),<span class="dt">nrow=</span>m)
  
  ##matrix with control data (rows are tests, columns are mice)
  treatments &lt;-<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">sample</span>(population, N<span class="op">*</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>),<span class="dt">nrow=</span>m)
  
  ##add effect to 10% of them
  treatments[<span class="kw">which</span>(<span class="op">!</span>nullHypothesis),]&lt;-treatments[<span class="kw">which</span>(<span class="op">!</span>nullHypothesis),]<span class="op">+</span>delta
  
  ##combine to form one matrix
  dat &lt;-<span class="st"> </span><span class="kw">cbind</span>(controls,treatments)
  
 calls &lt;-<span class="st"> </span><span class="kw">rowttests</span>(dat,g)<span class="op">$</span>p.value <span class="op">&lt;</span><span class="st"> </span>alpha
 R=<span class="kw">sum</span>(calls)
 Q=<span class="kw">ifelse</span>(R<span class="op">&gt;</span><span class="dv">0</span>,<span class="kw">sum</span>(nullHypothesis <span class="op">&amp;</span><span class="st"> </span>calls)<span class="op">/</span>R,<span class="dv">0</span>)
 <span class="kw">return</span>(Q)
})</code></pre></div>
</div>
<div id="controlling-fdr" class="section level4">
<h4><span class="header-section-number">6.6.0.2</span> Controlling FDR</h4>
<p>The code above is a Monte Carlo simulation that generates 10,000 experiments 1,000 times, each time saving the observed <span class="math inline">\(Q\)</span>. Here is a histogram of these values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">hist</span>(Qs) ##Q is a random variable, this is its distribution</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/Q_distribution-1.png" alt="Q (false positives divided by number of features called significant) is a random variable. Here we generated a distribution with a Monte Carlo simulation." width="672" />
<p class="caption">
(#fig:Q_distribution)Q (false positives divided by number of features called significant) is a random variable. Here we generated a distribution with a Monte Carlo simulation.
</p>
</div>
<p>The FDR is the average value of <span class="math inline">\(Q\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">FDR=<span class="kw">mean</span>(Qs)
<span class="kw">print</span>(FDR)</code></pre></div>
<pre><code>## [1] 0.4463</code></pre>
<p>The FDR is relatively high here. This is because for 90% of the tests, the null hypotheses is true. This implies that with a 0.05 p-value cut-off, out of the 100 tests we incorrectly call between 4 and 5 significant on average. This combined with the fact that we don’t “catch” all the cases where the alternative is true, gives us a relatively high FDR. So how can we control this? What if we want lower FDR, say 5%?</p>
<p>To visually see why the FDR is high, we can make a histogram of the p-values. We use a higher value of <code>m</code> to have more data from the histogram. We draw a horizontal line representing the uniform distribution one gets for the <code>m0</code> cases for which the null is true.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
controls &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(population, N<span class="op">*</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>),<span class="dt">nrow=</span>m)
treatments &lt;-<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">sample</span>(population, N<span class="op">*</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>),<span class="dt">nrow=</span>m)
treatments[<span class="kw">which</span>(<span class="op">!</span>nullHypothesis),]&lt;-treatments[<span class="kw">which</span>(<span class="op">!</span>nullHypothesis),]<span class="op">+</span>delta
dat &lt;-<span class="st"> </span><span class="kw">cbind</span>(controls,treatments)
pvals &lt;-<span class="st"> </span><span class="kw">rowttests</span>(dat,g)<span class="op">$</span>p.value 

h &lt;-<span class="st"> </span><span class="kw">hist</span>(pvals,<span class="dt">breaks=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>))
<span class="kw">polygon</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.05</span>,<span class="fl">0.05</span>,<span class="dv">0</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,h<span class="op">$</span>counts[<span class="dv">1</span>],h<span class="op">$</span>counts[<span class="dv">1</span>]),<span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span>m0<span class="op">/</span><span class="dv">20</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/pval_hist-1.png" alt="Histogram of p-values. Monte Carlo simulation was used to generate data with m_1 genes having differences between groups." width="672" />
<p class="caption">
(#fig:pval_hist)Histogram of p-values. Monte Carlo simulation was used to generate data with m_1 genes having differences between groups.
</p>
</div>
<p>The first bar (grey) on the left represents cases with p-values smaller than 0.05. From the horizontal line we can infer that about 1/2 are false positives. This is in agreement with an FDR of 0.50. If we look at the bar for 0.01, we can see a lower FDR, as expected, but would call fewer features significant.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">h &lt;-<span class="st"> </span><span class="kw">hist</span>(pvals,<span class="dt">breaks=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.01</span>))
<span class="kw">polygon</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.01</span>,<span class="fl">0.01</span>,<span class="dv">0</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,h<span class="op">$</span>counts[<span class="dv">1</span>],h<span class="op">$</span>counts[<span class="dv">1</span>]),<span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span>m0<span class="op">/</span><span class="dv">100</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/pval_hist2-1.png" alt="Histogram of p-values with breaks at every 0.01. Monte Carlo simulation was used to generate data with m_1 genes having differences between groups." width="672" />
<p class="caption">
(#fig:pval_hist2)Histogram of p-values with breaks at every 0.01. Monte Carlo simulation was used to generate data with m_1 genes having differences between groups.
</p>
</div>
<p>As we consider a lower and lower p-value cut-off, the number of features detected decreases (loss of sensitivity), but our FDR also decreases (gain of specificity). So how do we decide on this cut-off? One approach is to set a desired FDR level <span class="math inline">\(\alpha\)</span>, and then develop procedures that control the error rate: FDR <span class="math inline">\(\leq \alpha\)</span>.</p>
</div>
<div id="benjamini-hochberg-advanced" class="section level4">
<h4><span class="header-section-number">6.6.0.3</span> Benjamini-Hochberg (Advanced)</h4>
<p>We want to construct a procedure that guarantees the FDR to be below a certain level <span class="math inline">\(\alpha\)</span>. For any given <span class="math inline">\(\alpha\)</span>, the Benjamini-Hochberg (1995) procedure is very practical because it simply requires that we are able to compute p-values for each of the individual tests and this permits a procedure to be defined.</p>
<p>For this procedure, order the p-values in increasing order: <span class="math inline">\(p_{(1)},\dots,p_{(m)}\)</span>. Then define <span class="math inline">\(k\)</span> to be the largest <span class="math inline">\(i\)</span> for which</p>
<p><span class="math display">\[p_{(i)} \leq \frac{i}{m}\alpha\]</span></p>
<p>The procedure is to reject tests with p-values smaller or equal to <span class="math inline">\(p_{(k)}\)</span>. Here is an example of how we would select the <span class="math inline">\(k\)</span> with code using the p-values computed above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
i =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">along=</span>pvals)

<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="kw">plot</span>(i,<span class="kw">sort</span>(pvals))
<span class="kw">abline</span>(<span class="dv">0</span>,i<span class="op">/</span>m<span class="op">*</span>alpha)
##close-up
<span class="kw">plot</span>(i[<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>],<span class="kw">sort</span>(pvals)[<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>],<span class="dt">main=</span><span class="st">&quot;Close-up&quot;</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,i<span class="op">/</span>m<span class="op">*</span>alpha)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/pvalue_vs_rank_plot-1.png" alt="Plotting p-values plotted against their rank illustrates the Benjamini-Hochberg procedure. The plot on the right is a close-up of the plot on the left." width="1008" />
<p class="caption">
(#fig:pvalue_vs_rank_plot)Plotting p-values plotted against their rank illustrates the Benjamini-Hochberg procedure. The plot on the right is a close-up of the plot on the left.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="kw">max</span>( <span class="kw">which</span>( <span class="kw">sort</span>(pvals) <span class="op">&lt;</span><span class="st"> </span>i<span class="op">/</span>m<span class="op">*</span>alpha) )
cutoff &lt;-<span class="st"> </span><span class="kw">sort</span>(pvals)[k]
<span class="kw">cat</span>(<span class="st">&quot;k =&quot;</span>,k,<span class="st">&quot;p-value cutoff=&quot;</span>,cutoff)</code></pre></div>
<pre><code>## k = 11 p-value cutoff= 3.763e-05</code></pre>
<p>We can show mathematically that this procedure has FDR lower than 5%. Please see Benjamini-Hochberg (1995) for details. An important outcome is that we now have selected 11 tests instead of just 2. If we are willing to set an FDR of 50% (this means we expect at least 1/2 our genes to be hits), then this list grows to 1063. The FWER does not provide this flexibility since any list of substantial size will result in an FWER of 1.</p>
<p>Keep in mind that we don’t have to run the complicated code above as we have functions to do this. For example, using the p-values <code>pvals</code> computed above, we simply type the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fdr &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(pvals, <span class="dt">method=</span><span class="st">&quot;fdr&quot;</span>)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(pvals,fdr,<span class="dt">log=</span><span class="st">&quot;xy&quot;</span>)
<span class="kw">abline</span>(<span class="dt">h=</span>alpha,<span class="dt">v=</span>cutoff) ##cutoff was computed above</code></pre></div>
<div class="figure"><span id="fig:fdr-versus-pval"></span>
<img src="bookdown_files/figure-html/fdr-versus-pval-1.png" alt="FDR estimates plotted against p-value." width="672" />
<p class="caption">
图 6.1: FDR estimates plotted against p-value.
</p>
</div>
<p>We can run a Monte-Carlo simulation to confirm that the FDR is in fact lower than .05. We compute all p-values first, and then use these to decide which get called.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">alpha &lt;-<span class="st"> </span><span class="fl">0.05</span>
B &lt;-<span class="st"> </span><span class="dv">1000</span> ##number of simulations. We should increase for more precision
res &lt;-<span class="st"> </span><span class="kw">replicate</span>(B,{
  controls &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(population, N<span class="op">*</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>),<span class="dt">nrow=</span>m)
  treatments &lt;-<span class="st">  </span><span class="kw">matrix</span>(<span class="kw">sample</span>(population, N<span class="op">*</span>m, <span class="dt">replace=</span><span class="ot">TRUE</span>),<span class="dt">nrow=</span>m)
  treatments[<span class="kw">which</span>(<span class="op">!</span>nullHypothesis),]&lt;-treatments[<span class="kw">which</span>(<span class="op">!</span>nullHypothesis),]<span class="op">+</span>delta
  dat &lt;-<span class="st"> </span><span class="kw">cbind</span>(controls,treatments)
  pvals &lt;-<span class="st"> </span><span class="kw">rowttests</span>(dat,g)<span class="op">$</span>p.value 
  ##then the FDR
  calls &lt;-<span class="st"> </span><span class="kw">p.adjust</span>(pvals,<span class="dt">method=</span><span class="st">&quot;fdr&quot;</span>) <span class="op">&lt;</span><span class="st"> </span>alpha
  R=<span class="kw">sum</span>(calls)
  Q=<span class="kw">ifelse</span>(R<span class="op">&gt;</span><span class="dv">0</span>,<span class="kw">sum</span>(nullHypothesis <span class="op">&amp;</span><span class="st"> </span>calls)<span class="op">/</span>R,<span class="dv">0</span>)
  <span class="kw">return</span>(<span class="kw">c</span>(R,Q))
})
Qs &lt;-<span class="st"> </span>res[<span class="dv">2</span>,]
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">hist</span>(Qs) ##Q is a random variable, this is its distribution</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/Q_distribution2-1.png" alt="Histogram of Q (false positives divided by number of features called significant) when the alternative hypothesis is true for some features." width="672" />
<p class="caption">
(#fig:Q_distribution2)Histogram of Q (false positives divided by number of features called significant) when the alternative hypothesis is true for some features.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">FDR=<span class="kw">mean</span>(Qs)
<span class="kw">print</span>(FDR)</code></pre></div>
<pre><code>## [1] 0.03814</code></pre>
<p>The FDR is lower than 0.05. This is to be expected because we need to be conservative to ensure the FDR <span class="math inline">\(\leq\)</span> 0.05 for any value of <span class="math inline">\(m_0\)</span>, such as for the extreme case where every hypothesis tested is null: <span class="math inline">\(m=m_0\)</span>. If you re-do the simulation above for this case, you will find that the FDR increases.</p>
<p>We should also note that in …</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Rs &lt;-<span class="st"> </span>res[<span class="dv">1</span>,]
<span class="kw">mean</span>(Rs<span class="op">==</span><span class="dv">0</span>)<span class="op">*</span><span class="dv">100</span></code></pre></div>
<pre><code>## [1] 0.7</code></pre>
<p>… percent of the simulations, we did not call any genes significant.</p>
<p>Finally, note that the <code>p.adjust</code> function has several options for error rate controlling procedures:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p.adjust.methods</code></pre></div>
<pre><code>## [1] &quot;holm&quot;       &quot;hochberg&quot;   &quot;hommel&quot;     &quot;bonferroni&quot;
## [5] &quot;BH&quot;         &quot;BY&quot;         &quot;fdr&quot;        &quot;none&quot;</code></pre>
<p>It is important to remember that these options offer not just different approaches to estimating error rates, but also that different error rates are estimated: namely FWER and FDR. This is an important distinction. More information is available from:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">?p.adjust</code></pre></div>
<p>In summary, requiring that FDR <span class="math inline">\(\leq\)</span> 0.05 is a much more lenient requirement FWER <span class="math inline">\(\leq\)</span> 0.05. Although we will end up with more false positives, FDR gives us much more power. This makes it particularly appropriate for discovery phase experiments where we may accept FDR levels much higher than 0.05.</p>
</div>
</div>
<div id="direct-approach-to-fdr-and-q-values-advanced" class="section level2">
<h2><span class="header-section-number">6.7</span> Direct Approach to FDR and q-values (Advanced)</h2>
<p>Here we review the results described by John D. Storey in J. R. Statist. Soc. B (2002). One major distinction between Storey’s approach and Benjamini and Hochberg’s is that we are no longer going to set a <span class="math inline">\(\alpha\)</span> level a priori. Because in many high-throughput experiments we are interested in obtaining some list for validation, we can instead decide beforehand that we will consider all tests with p-values smaller than 0.01. We then want to attach an estimate of an error rate. Using this approach, we are guaranteed to have <span class="math inline">\(R&gt;0\)</span>. Note that in the FDR definition above we assigned <span class="math inline">\(Q=0\)</span> in the case that <span class="math inline">\(R=V=0\)</span>. We were therefore computing:</p>
<p><span class="math display">\[
\mbox{FDR} = E\left( \frac{V}{R} \mid R&gt;0\right) \mbox{Pr}(R&gt;0)
\]</span></p>
<p>In the approach proposed by Storey, we condition on having a non-empty list, which implies <span class="math inline">\(R&gt;0\)</span>, and we instead compute the <em>positive FDR</em></p>
<p><span class="math display">\[
\mbox{pFDR} = E\left( \frac{V}{R} \mid R&gt;0\right) 
\]</span></p>
<p>A second distinction is that while Benjamini and Hochberg’s procedure controls under the worst case scenario, in which all null hypotheses are true ( <span class="math inline">\(m=m_0\)</span> ), Storey proposes that we actually try to estimate <span class="math inline">\(m_0\)</span> from the data. Because in high-throughput experiments we have so much data, this is certainly possible. The general idea is to pick a relatively high value p-value cut-off, call it <span class="math inline">\(\lambda\)</span>, and assume that tests obtaining p-values &gt; <span class="math inline">\(\lambda\)</span> are mostly from cases in which the null hypothesis holds. We can then estimate <span class="math inline">\(\pi_0 = m_0/m\)</span> as:</p>
<p><span class="math display">\[
\hat{\pi}_0 = \frac{\#\left\{p_i &gt; \lambda \right\} }{ (1-\lambda) m }
\]</span></p>
<p>There are more sophisticated procedures than this, but they follow the same general idea. Here is an example setting <span class="math inline">\(\lambda=0.1\)</span>. Using the p-values computed above we have:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(pvals,<span class="dt">breaks=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>),<span class="dt">freq=</span><span class="ot">FALSE</span>)
lambda =<span class="st"> </span><span class="fl">0.1</span>
pi0=<span class="kw">sum</span>(pvals<span class="op">&gt;</span><span class="st"> </span>lambda) <span class="op">/</span>((<span class="dv">1</span><span class="op">-</span>lambda)<span class="op">*</span>m)
<span class="kw">abline</span>(<span class="dt">h=</span> pi0)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/pi0_estimate-1.png" alt="p-value histogram with pi0 estimate." width="672" />
<p class="caption">
(#fig:pi0_estimate)p-value histogram with pi0 estimate.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(pi0) ##this is close to the trye pi0=0.9</code></pre></div>
<pre><code>## [1] 0.9311</code></pre>
<p>With this estimate in place we can, for example, alter the Benjamini and Hochberg procedures to select the <span class="math inline">\(k\)</span> to be the largest value so that:</p>
<p><span class="math display">\[\hat{\pi}_0 p_{(i)} \leq \frac{i}{m}\alpha\]</span></p>
<p>However, instead of doing this, we compute a <em>q-value</em> for each test. If a feature resulted in a p-value of <span class="math inline">\(p\)</span>, the q-value is the estimated pFDR for a list of all the features with a p-value at least as small as <span class="math inline">\(p\)</span>.</p>
<p>In R, this can be computed with the <code>qvalue</code> function in the <code>qvalue</code> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(qvalue)
res &lt;-<span class="st"> </span><span class="kw">qvalue</span>(pvals)
qvals &lt;-<span class="st"> </span>res<span class="op">$</span>qvalues
<span class="kw">plot</span>(pvals,qvals)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/qval_vs_pval-1.png" alt="q-values versus p-values." width="672" />
<p class="caption">
(#fig:qval_vs_pval)q-values versus p-values.
</p>
</div>
<p>we also obtain the estimate of <span class="math inline">\(\hat{\pi}_0\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res<span class="op">$</span>pi0</code></pre></div>
<pre><code>## [1] 0.8814</code></pre>
<p>This function uses a more sophisticated approach at estimating <span class="math inline">\(\pi_0\)</span> than what is described above.</p>
<div id="note-on-estimating-pi_0" class="section level4">
<h4><span class="header-section-number">6.7.0.1</span> Note on estimating <span class="math inline">\(\pi_0\)</span></h4>
<p>In our experience the estimation of <span class="math inline">\(\pi_0\)</span> can be unstable and adds a step of uncertainty to the data analysis pipeline. Although more conservative, the Benjamini-Hochberg procedure is computationally more stable.</p>
</div>
</div>
<div id="basic-exploratory-data-analysis" class="section level2">
<h2><span class="header-section-number">6.8</span> Basic Exploratory Data Analysis</h2>
<p>An under-appreciated advantage of working with high-throughput data is that problems with the data are sometimes more easily exposed than with low-throughput data. The fact that we have thousands of measurements permits us to see problems that are not apparent when only a few measurements are available. A powerful way to detect these problems is with exploratory data analysis (EDA). Here we review some of the plots that allow us to detect quality problems.</p>
<div id="volcano-plots" class="section level4">
<h4><span class="header-section-number">6.8.0.1</span> Volcano plots</h4>
<p>Here we will use the results obtained from applying t-test to data from a gene expression dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(genefilter)
<span class="kw">library</span>(GSE5859Subset)
<span class="kw">data</span>(GSE5859Subset)
g &lt;-<span class="st"> </span><span class="kw">factor</span>(sampleInfo<span class="op">$</span>group)
results &lt;-<span class="st"> </span><span class="kw">rowttests</span>(geneExpression,g)
pvals &lt;-<span class="st"> </span>results<span class="op">$</span>p.value</code></pre></div>
<p>And we also generate p-values from a dataset for which we know the null is true:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw">nrow</span>(geneExpression)
n &lt;-<span class="st"> </span><span class="kw">ncol</span>(geneExpression)
randomData &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n<span class="op">*</span>m),m,n)
nullpvals &lt;-<span class="st"> </span><span class="kw">rowttests</span>(randomData,g)<span class="op">$</span>p.value</code></pre></div>
<p>As we described earlier, reporting only p-values is a mistake when we can also report effect sizes. With high-throughput data, we can visualize the results by making a <em>volcano plot</em>. The idea behind a volcano plot is to show these for all features. In the y-axis we plot -log (base 10) p-values and on the x-axis we plot the effect size. By using -log (base 10), the “highly significant” features appear at the top of the plot. Using log also permits us to better distinguish between small and very small p-values, for example 0.01 and <span class="math inline">\(10^6\)</span>. Here is the volcano plot for our results above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(results<span class="op">$</span>dm,<span class="op">-</span><span class="kw">log10</span>(results<span class="op">$</span>p.value),
     <span class="dt">xlab=</span><span class="st">&quot;Effect size&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;- log (base 10) p-values&quot;</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/volcano_plot-1.png" width="672" /></p>
<p>Many features with very small p-values, but small effect sizes as we see here, are sometimes indicative of problematic data.</p>
</div>
<div id="p-value-histograms" class="section level4">
<h4><span class="header-section-number">6.8.0.2</span> p-value Histograms</h4>
<p>Another plot we can create to get an overall idea of the results is to make histograms of p-values. When we generate completely null data the histogram follows a uniform distribution. With our original dataset we see a higher frequency of smaller p-values.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="kw">hist</span>(nullpvals,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1400</span>))
<span class="kw">hist</span>(pvals,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1400</span>))</code></pre></div>
<div class="figure"><span id="fig:pval-hist"></span>
<img src="bookdown_files/figure-html/pval-hist-1.png" alt="P-value histogram. We show a simulated case in which all null hypotheses are true (left) and p-values from the gene expression described above." width="1008" />
<p class="caption">
图 6.2: P-value histogram. We show a simulated case in which all null hypotheses are true (left) and p-values from the gene expression described above.
</p>
</div>
<p>When we expect most hypotheses to be null and don’t see a uniform p-value distribution, it might be indicative of unexpected properties, such as correlated samples.</p>
<p>If we permute the outcomes and calculate p-values then, if the samples are independent, we should see a uniform distribution. With these data we do not:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">permg &lt;-<span class="st"> </span><span class="kw">sample</span>(g)
permresults &lt;-<span class="st"> </span><span class="kw">rowttests</span>(geneExpression,permg)
<span class="kw">hist</span>(permresults<span class="op">$</span>p.value)</code></pre></div>
<div class="figure"><span id="fig:pval-hist2"></span>
<img src="bookdown_files/figure-html/pval-hist2-1.png" alt="Histogram obtained after permuting labels." width="672" />
<p class="caption">
图 6.3: Histogram obtained after permuting labels.
</p>
</div>
<p>In a later chapter we will see that the columns in this dataset are not independent and thus the assumptions used to compute the p-values here are incorrect.</p>
</div>
<div id="data-boxplots-and-histograms" class="section level4">
<h4><span class="header-section-number">6.8.0.3</span> Data boxplots and histograms</h4>
<p>With high-throughput data, we have thousands of measurements for each experimental unit. As mentioned earlier, this can help us detect quality issues. For example, if one sample has a completely different distribution than the rest, we might suspect there are problems. Although a complete change in distribution could be due to real biological differences, more often than not it is due to a technical problem. Here we load a large gene expression experiment available from Bioconductor. We “accidentally” use log instead of log2 on one of the samples.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(Biobase)
<span class="kw">library</span>(GSE5859) 
<span class="kw">data</span>(GSE5859) 
ge &lt;-<span class="st"> </span><span class="kw">exprs</span>(e) ##ge for gene expression
ge[,<span class="dv">49</span>] &lt;-<span class="st"> </span>ge[,<span class="dv">49</span>]<span class="op">/</span><span class="kw">log2</span>(<span class="kw">exp</span>(<span class="dv">1</span>)) ##imitate error</code></pre></div>
<p>A quick look at a summary of the distribution using boxplots immediately highlights the mistake:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">boxplot</span>(ge,<span class="dt">range=</span><span class="dv">0</span>,<span class="dt">names=</span><span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(e),<span class="dt">col=</span><span class="kw">ifelse</span>(<span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(ge)<span class="op">==</span><span class="dv">49</span>,<span class="dv">1</span>,<span class="dv">2</span>))</code></pre></div>
<div class="figure"><span id="fig:boxplots"></span>
<img src="bookdown_files/figure-html/boxplots-1.png" alt="Boxplot for log-scale expression for all samples." width="1008" />
<p class="caption">
图 6.4: Boxplot for log-scale expression for all samples.
</p>
</div>
<p>Note that the number of samples is a bit too large here, making it hard to see the boxes. One can instead simply show the boxplot summaries without the boxes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">qs &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(ge,<span class="dv">2</span>,quantile,<span class="dt">prob=</span><span class="kw">c</span>(<span class="fl">0.05</span>,<span class="fl">0.25</span>,<span class="fl">0.5</span>,<span class="fl">0.75</span>,<span class="fl">0.95</span>)))
<span class="kw">matplot</span>(qs,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">lty=</span><span class="dv">1</span>)</code></pre></div>
<div class="figure"><span id="fig:kaboxplot"></span>
<img src="bookdown_files/figure-html/kaboxplot-1.png" alt="The 0.05, 0.25, 0.5, 0.75, and 0.95 quantiles are plotted for each sample." width="1008" />
<p class="caption">
图 6.5: The 0.05, 0.25, 0.5, 0.75, and 0.95 quantiles are plotted for each sample.
</p>
</div>
<p>We refer to this figure as a <em>kaboxplot</em> because Karl Broman was the first we saw use it as an alternative to boxplots.</p>
<p>We can also plot all the histograms. Because we have so much data, we create histograms using small bins, then smooth the heights of the bars and then plot <em>smooth histograms</em>. We re-calibrate the height of these smooth curves so that if a bar is made with base of size “unit” and height given by the curve at <span class="math inline">\(x_0\)</span>, the area approximates the number of points in region of size “unit” centered at <span class="math inline">\(x_0\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">shist</span>(ge,<span class="dt">unit=</span><span class="fl">0.5</span>)</code></pre></div>
<div class="figure"><span id="fig:shist"></span>
<img src="bookdown_files/figure-html/shist-1.png" alt="Smooth histograms for each sample." width="672" />
<p class="caption">
图 6.6: Smooth histograms for each sample.
</p>
</div>
</div>
<div id="ma-plot" class="section level4">
<h4><span class="header-section-number">6.8.0.4</span> MA plot</h4>
<p>Scatterplots and correlation are not the best tools to detect replication problems. A better measure of replication can be obtained from examining the differences between the values that should be the same. Therefore, a better plot is a rotation of the scatterplot containing the differences on the y-axis and the averages on the x-axis. This plot was originally named a Bland-Altman plot, but in genomics it is commonly referred to as an MA-plot. The name MA comes from plots of red log intensity minus (M) green intensities versus average (A) log intensities used with microarrays (MA) data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>ge[,<span class="dv">1</span>]
y &lt;-<span class="st"> </span>ge[,<span class="dv">2</span>]
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="kw">plot</span>(x,y)
<span class="kw">plot</span>((x<span class="op">+</span>y)<span class="op">/</span><span class="dv">2</span>,x<span class="op">-</span>y)</code></pre></div>
<div class="figure"><span id="fig:maplot"></span>
<img src="bookdown_files/figure-html/maplot-1.png" alt="Scatter plot (left) and M versus A plot (right) for the same data." width="1008" />
<p class="caption">
图 6.7: Scatter plot (left) and M versus A plot (right) for the same data.
</p>
</div>
<p>Note that once we rotate the plot, the fact that these data have differences of about:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(y<span class="op">-</span>x)</code></pre></div>
<pre><code>## [1] 0.2025</code></pre>
<p>becomes immediate. The scatterplot shows very strong correlation, which is not necessarily informative here.</p>
<p>We will later introduce dendograms, heatmaps, and multi-dimensional scaling plots.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-models-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="statistical-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/xie186/DataAnalysisForLifeScience_cn/edit/master/06_advinference.Rmd",
"text": "编辑"
},
"download": ["bookdown.pdf", "bookdown.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
