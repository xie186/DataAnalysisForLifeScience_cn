<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>生物信息R数据分析</title>
  <meta name="description" content="生物信息R数据分析">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="生物信息R数据分析" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="生物信息R数据分析" />
  <meta name="github-repo" content="xie186/HarvardDataScienceForLifeScience_cn" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="生物信息R数据分析" />
  
  <meta name="twitter:description" content="生物信息R数据分析" />
  

<meta name="author" content="作者：Rafael A. Irizarry; Mike I. Love 翻译：张三 李四 麻子">


<meta name="date" content="2017-11-16">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="matrix-algebra.html">
<link rel="next" href="inference-for-high-dimensional-data.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">生物信息R数据分析</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a><ul>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who Will Find This Book Useful?</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#what-does-this-book-cover"><i class="fa fa-check"></i>What Does This Book Cover?</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#how-is-this-book-different"><i class="fa fa-check"></i>How Is This Book Different?</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>1.1</b> Installing R</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>1.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#learn-r-basics"><i class="fa fa-check"></i><b>1.3</b> Learn R Basics</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#installing-packages"><i class="fa fa-check"></i><b>1.4</b> Installing Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#importing-data-into-r"><i class="fa fa-check"></i><b>1.5</b> Importing Data into R</a></li>
<li class="chapter" data-level="1.6" data-path="getting-started.html"><a href="getting-started.html#brief-introduction-to-dplyr"><i class="fa fa-check"></i><b>1.6</b> Brief Introduction to <code>dplyr</code></a></li>
<li class="chapter" data-level="1.7" data-path="getting-started.html"><a href="getting-started.html#mathematical-notation"><i class="fa fa-check"></i><b>1.7</b> Mathematical Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="inference.html"><a href="inference.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="inference.html"><a href="inference.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="inference.html"><a href="inference.html#the-null-hypothesis"><i class="fa fa-check"></i><b>2.3</b> The Null Hypothesis</a></li>
<li class="chapter" data-level="2.4" data-path="inference.html"><a href="inference.html#distributions"><i class="fa fa-check"></i><b>2.4</b> Distributions</a></li>
<li class="chapter" data-level="2.5" data-path="inference.html"><a href="inference.html#probability-distribution"><i class="fa fa-check"></i><b>2.5</b> Probability Distribution</a></li>
<li class="chapter" data-level="2.6" data-path="inference.html"><a href="inference.html#normal-distribution"><i class="fa fa-check"></i><b>2.6</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.7" data-path="inference.html"><a href="inference.html#populations-samples-and-estimates"><i class="fa fa-check"></i><b>2.7</b> Populations, Samples and Estimates</a></li>
<li class="chapter" data-level="2.8" data-path="inference.html"><a href="inference.html#central-limit-theorem-and-t-distribution"><i class="fa fa-check"></i><b>2.8</b> Central Limit Theorem and t-distribution</a></li>
<li class="chapter" data-level="2.9" data-path="inference.html"><a href="inference.html#central-limit-theorem-in-practice"><i class="fa fa-check"></i><b>2.9</b> Central Limit Theorem in Practice</a></li>
<li class="chapter" data-level="2.10" data-path="inference.html"><a href="inference.html#t-tests-in-practice"><i class="fa fa-check"></i><b>2.10</b> t-tests in Practice</a></li>
<li class="chapter" data-level="2.11" data-path="inference.html"><a href="inference.html#the-t-distribution-in-practice"><i class="fa fa-check"></i><b>2.11</b> The t-distribution in Practice</a></li>
<li class="chapter" data-level="2.12" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>2.12</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.13" data-path="inference.html"><a href="inference.html#power-calculations"><i class="fa fa-check"></i><b>2.13</b> Power Calculations</a></li>
<li class="chapter" data-level="2.14" data-path="inference.html"><a href="inference.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>2.14</b> Monte Carlo Simulation</a></li>
<li class="chapter" data-level="2.15" data-path="inference.html"><a href="inference.html#parametric-simulations-for-the-observations"><i class="fa fa-check"></i><b>2.15</b> Parametric Simulations for the Observations</a></li>
<li class="chapter" data-level="2.16" data-path="inference.html"><a href="inference.html#permutation-tests"><i class="fa fa-check"></i><b>2.16</b> Permutation Tests</a></li>
<li class="chapter" data-level="2.17" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>2.17</b> Association Tests</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>3.1</b> Quantile Quantile Plots</a></li>
<li class="chapter" data-level="3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots"><i class="fa fa-check"></i><b>3.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatterplots-and-correlation"><i class="fa fa-check"></i><b>3.3</b> Scatterplots and Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>3.4</b> Stratification</a></li>
<li class="chapter" data-level="3.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>3.5</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="3.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plots-to-avoid"><i class="fa fa-check"></i><b>3.6</b> Plots to Avoid</a></li>
<li class="chapter" data-level="3.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#misunderstanding-correlation-advanced"><i class="fa fa-check"></i><b>3.7</b> Misunderstanding Correlation (Advanced)</a></li>
<li class="chapter" data-level="3.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#robust-summaries"><i class="fa fa-check"></i><b>3.8</b> Robust Summaries</a></li>
<li class="chapter" data-level="3.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>3.9</b> Wilcoxon Rank Sum Test</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>4</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#motivating-examples"><i class="fa fa-check"></i><b>4.1</b> Motivating Examples</a></li>
<li class="chapter" data-level="4.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrix-notation"><i class="fa fa-check"></i><b>4.2</b> Matrix Notation</a></li>
<li class="chapter" data-level="4.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#solving-systems-of-equations"><i class="fa fa-check"></i><b>4.3</b> Solving Systems of Equations</a></li>
<li class="chapter" data-level="4.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#vectors-matrices-and-scalars"><i class="fa fa-check"></i><b>4.4</b> Vectors, Matrices, and Scalars</a></li>
<li class="chapter" data-level="4.5" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrix-operations"><i class="fa fa-check"></i><b>4.5</b> Matrix Operations</a></li>
<li class="chapter" data-level="4.6" data-path="matrix-algebra.html"><a href="matrix-algebra.html#examples"><i class="fa fa-check"></i><b>4.6</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-models-1.html"><a href="linear-models-1.html"><i class="fa fa-check"></i><b>5</b> Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-models-1.html"><a href="linear-models-1.html#the-design-matrix"><i class="fa fa-check"></i><b>5.1</b> The Design Matrix</a></li>
<li class="chapter" data-level="5.2" data-path="linear-models-1.html"><a href="linear-models-1.html#the-mathematics-behind-lm"><i class="fa fa-check"></i><b>5.2</b> The Mathematics Behind lm()</a></li>
<li class="chapter" data-level="5.3" data-path="linear-models-1.html"><a href="linear-models-1.html#standard-errors"><i class="fa fa-check"></i><b>5.3</b> Standard Errors</a></li>
<li class="chapter" data-level="5.4" data-path="linear-models-1.html"><a href="linear-models-1.html#interactions-and-contrasts"><i class="fa fa-check"></i><b>5.4</b> Interactions and Contrasts</a></li>
<li class="chapter" data-level="5.5" data-path="linear-models-1.html"><a href="linear-models-1.html#linear-model-with-interactions"><i class="fa fa-check"></i><b>5.5</b> Linear Model with Interactions</a></li>
<li class="chapter" data-level="5.6" data-path="linear-models-1.html"><a href="linear-models-1.html#analysis-of-variance"><i class="fa fa-check"></i><b>5.6</b> Analysis of Variance</a></li>
<li class="chapter" data-level="5.7" data-path="linear-models-1.html"><a href="linear-models-1.html#collinearity"><i class="fa fa-check"></i><b>5.7</b> Collinearity</a></li>
<li class="chapter" data-level="5.8" data-path="linear-models-1.html"><a href="linear-models-1.html#rank"><i class="fa fa-check"></i><b>5.8</b> Rank</a></li>
<li class="chapter" data-level="5.9" data-path="linear-models-1.html"><a href="linear-models-1.html#removing-confounding"><i class="fa fa-check"></i><b>5.9</b> Removing Confounding</a></li>
<li class="chapter" data-level="5.10" data-path="linear-models-1.html"><a href="linear-models-1.html#the-qr-factorization-advanced"><i class="fa fa-check"></i><b>5.10</b> The QR Factorization (Advanced)</a></li>
<li class="chapter" data-level="5.11" data-path="linear-models-1.html"><a href="linear-models-1.html#going-further"><i class="fa fa-check"></i><b>5.11</b> Going Further</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html"><i class="fa fa-check"></i><b>6</b> Inference for High Dimensional Data</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#inference-in-practice"><i class="fa fa-check"></i><b>6.2</b> Inference in Practice</a></li>
<li class="chapter" data-level="6.3" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#procedures"><i class="fa fa-check"></i><b>6.3</b> Procedures</a></li>
<li class="chapter" data-level="6.4" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#error-rates"><i class="fa fa-check"></i><b>6.4</b> Error Rates</a></li>
<li class="chapter" data-level="6.5" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#the-bonferroni-correction"><i class="fa fa-check"></i><b>6.5</b> The Bonferroni Correction</a></li>
<li class="chapter" data-level="6.6" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#false-discovery-rate"><i class="fa fa-check"></i><b>6.6</b> False Discovery Rate</a></li>
<li class="chapter" data-level="6.7" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#direct-approach-to-fdr-and-q-values-advanced"><i class="fa fa-check"></i><b>6.7</b> Direct Approach to FDR and q-values (Advanced)</a></li>
<li class="chapter" data-level="6.8" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#basic-exploratory-data-analysis"><i class="fa fa-check"></i><b>6.8</b> Basic Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="statistical-models.html"><a href="statistical-models.html"><i class="fa fa-check"></i><b>7</b> Statistical Models</a><ul>
<li class="chapter" data-level="7.1" data-path="statistical-models.html"><a href="statistical-models.html#the-binomial-distribution"><i class="fa fa-check"></i><b>7.1</b> The Binomial Distribution</a></li>
<li class="chapter" data-level="7.2" data-path="statistical-models.html"><a href="statistical-models.html#the-poisson-distribution"><i class="fa fa-check"></i><b>7.2</b> The Poisson Distribution</a></li>
<li class="chapter" data-level="7.3" data-path="statistical-models.html"><a href="statistical-models.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>7.3</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="7.4" data-path="statistical-models.html"><a href="statistical-models.html#distributions-for-positive-continuous-values"><i class="fa fa-check"></i><b>7.4</b> Distributions for Positive Continuous Values</a></li>
<li class="chapter" data-level="7.5" data-path="statistical-models.html"><a href="statistical-models.html#bayesian-statistics"><i class="fa fa-check"></i><b>7.5</b> Bayesian Statistics</a></li>
<li class="chapter" data-level="7.6" data-path="statistical-models.html"><a href="statistical-models.html#hierarchical-models"><i class="fa fa-check"></i><b>7.6</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">本书由 bookdown 强力驱动</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">生物信息R数据分析</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-models-1" class="section level1">
<h1><span class="header-section-number">第 5 章</span> Linear Models</h1>
<p>Many of the models we use in data analysis can be presented using matrix algebra. We refer to these types of models as <em>linear models</em>. “Linear” here does not refer to lines, but rather to linear combinations. The representations we describe are convenient because we can write models more succinctly and we have the matrix algebra mathematical machinery to facilitate computation. In this chapter, we will describe in some detail how we use matrix algebra to represent and fit.</p>
<p>In this book, we focus on linear models that represent dichotomous groups: treatment versus control, for example. The effect of diet on mice weights is an example of this type of linear model. Here we describe slightly more complicated models, but continue to focus on dichotomous variables.</p>
<p>As we learn about linear models, we need to remember that we are still working with random variables. This means that the estimates we obtain using linear models are also random variables. Although the mathematics is more complex, the concepts we learned in previous chapters apply here. We begin with some exercises to review the concept of random variables in the context of linear models.</p>
<div id="the-design-matrix" class="section level2">
<h2><span class="header-section-number">5.1</span> The Design Matrix</h2>
<p>Here we will show how to use the two R functions, <code>formula</code> and <code>model.matrix</code>, in order to produce <em>design matrices</em> (also known as <em>model matrices</em>) for a variety of linear models. For example, in the mouse diet examples we wrote the model as</p>
<p><span class="math display">\[ 
Y_i = \beta_0 + \beta_1 x_i + \varepsilon_i, i=1,\dots,N 
\]</span></p>
<p>with <span class="math inline">\(Y_i\)</span> the weights and <span class="math inline">\(x_i\)</span> equal to 1 only when mouse <span class="math inline">\(i\)</span> receives the high fat diet. We use the term <em>experimental unit</em> to <span class="math inline">\(N\)</span> different entities from which we obtain a measurement. In this case, the mice are the experimental units.</p>
<p>This is the type of variable we will focus on in this chapter. We call them <em>indicator variables</em> since they simply indicate if the experimental unit had a certain characteristic or not. As we described earlier, we can use linear algebra to represent this model:</p>
<p><span class="math display">\[
\mathbf{Y} = \begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix}
,
\mathbf{X} = \begin{pmatrix}
1&amp;x_1\\
1&amp;x_2\\
\vdots\\
1&amp;x_N
\end{pmatrix}
,
\boldsymbol{\beta} = \begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} \mbox{ and }
\boldsymbol{\varepsilon} = \begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
\]</span></p>
<p>as:</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
Y_1\\
Y_2\\
\vdots\\
Y_N
\end{pmatrix} = 
\begin{pmatrix}
1&amp;x_1\\
1&amp;x_2\\
\vdots\\
1&amp;x_N
\end{pmatrix}
\begin{pmatrix}
\beta_0\\
\beta_1
\end{pmatrix} +
\begin{pmatrix}
\varepsilon_1\\
\varepsilon_2\\
\vdots\\
\varepsilon_N
\end{pmatrix}
\]</span></p>
<p>or simply:</p>
<p><span class="math display">\[
\mathbf{Y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}
\]</span></p>
<p>The design matrix is the matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Once we define a design matrix, we are ready to find the least squares estimates. We refer to this as <em>fitting the model</em>. For fitting linear models in R, we will directly provide a <em>formula</em> to the <code>lm</code> function. In this script, we will use the <code>model.matrix</code> function, which is used internally by the <code>lm</code> function. This will help us to connect the R <code>formula</code> with the matrix <span class="math inline">\(\mathbf{X}\)</span>. It will therefore help us interpret the results from <code>lm</code>.</p>
<div id="choice-of-design" class="section level4">
<h4><span class="header-section-number">5.1.0.1</span> Choice of design</h4>
<p>The choice of design matrix is a critical step in linear modeling since it encodes which coefficients will be fit in the model, as well as the inter-relationship between the samples. A common misunderstanding is that the choice of design follows straightforward from a description of which samples were included in the experiment. This is not the case. The basic information about each sample (whether control or treatment group, experimental batch, etc.) does not imply a single ‘correct’ design matrix. The design matrix additionally encodes various assumptions about how the variables in <span class="math inline">\(\mathbf{X}\)</span> explain the observed values in <span class="math inline">\(\mathbf{Y}\)</span>, on which the investigator must decide.</p>
<p>For the examples we cover here, we use linear models to make comparisons between different groups. Hence, the design matrices that we ultimately work with will have at least two columns: an <em>intercept</em> column, which consists of a column of 1’s, and a second column, which specifies which samples are in a second group. In this case, two coefficients are fit in the linear model: the intercept, which represents the population average of the first group, and a second coefficient, which represents the difference between the population averages of the second group and the first group. The latter is typically the coefficient we are interested in when we are performing statistical tests: we want to know if there is a difference between the two groups.</p>
<p>We encode this experimental design in R with two pieces. We start with a formula with the tilde symbol <code>~</code>. This means that we want to model the observations using the variables to the right of the tilde. Then we put the name of a variable, which tells us which samples are in which group.</p>
<p>Let’s try an example. Suppose we have two groups, control and high fat diet, with two samples each. For illustrative purposes, we will code these with 1 and 2 respectively. We should first tell R that these values should not be interpreted numerically, but as different levels of a <em>factor</em>. We can then use the paradigm <code>~ group</code> to, say, model on the variable <code>group</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">factor</span>( <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>) )
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group)</code></pre></div>
<pre><code>##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      1
## 4           1      1
## attr(,&quot;assign&quot;)
## [1] 0 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
<p>(Don’t worry about the <code>attr</code> lines printed beneath the matrix. We won’t be using this information.)</p>
<p>What about the <code>formula</code> function? We don’t have to include this. By starting an expression with <code>~</code>, it is equivalent to telling R that the expression is a formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(<span class="kw">formula</span>(<span class="op">~</span><span class="st"> </span>group))</code></pre></div>
<pre><code>##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      1
## 4           1      1
## attr(,&quot;assign&quot;)
## [1] 0 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
<p>What happens if we don’t tell R that <code>group</code> should be interpreted as a factor?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>)
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group)</code></pre></div>
<pre><code>##   (Intercept) group
## 1           1     1
## 2           1     1
## 3           1     2
## 4           1     2
## attr(,&quot;assign&quot;)
## [1] 0 1</code></pre>
<p>This is <strong>not</strong> the design matrix we wanted, and the reason is that we provided a numeric variable as opposed to an <em>indicator</em> to the <code>formula</code> and <code>model.matrix</code> functions, without saying that these numbers actually referred to different groups. We want the second column to have only 0 and 1, indicating group membership.</p>
<p>A note about factors: the names of the levels are irrelevant to <code>model.matrix</code> and <code>lm</code>. All that matters is the order. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="st">&quot;control&quot;</span>,<span class="st">&quot;control&quot;</span>,<span class="st">&quot;highfat&quot;</span>,<span class="st">&quot;highfat&quot;</span>))
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group)</code></pre></div>
<pre><code>##   (Intercept) grouphighfat
## 1           1            0
## 2           1            0
## 3           1            1
## 4           1            1
## attr(,&quot;assign&quot;)
## [1] 0 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
<p>produces the same design matrix as our first code chunk.</p>
</div>
<div id="more-groups" class="section level4">
<h4><span class="header-section-number">5.1.0.2</span> More groups</h4>
<p>Using the same formula, we can accommodate modeling more groups. Suppose we have a third diet:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>))
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group)</code></pre></div>
<pre><code>##   (Intercept) group2 group3
## 1           1      0      0
## 2           1      0      0
## 3           1      1      0
## 4           1      1      0
## 5           1      0      1
## 6           1      0      1
## attr(,&quot;assign&quot;)
## [1] 0 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
<p>Now we have a third column which specifies which samples belong to the third group.</p>
<p>An alternate formulation of design matrix is possible by specifying <code>+ 0</code> in the formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>))
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group <span class="op">+</span><span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<pre><code>##   group1 group2 group3
## 1      1      0      0
## 2      1      0      0
## 3      0      1      0
## 4      0      1      0
## 5      0      0      1
## 6      0      0      1
## attr(,&quot;assign&quot;)
## [1] 1 1 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
<p>This group now fits a separate coefficient for each group. We will explore this design in more depth later on.</p>
</div>
<div id="more-variables" class="section level4">
<h4><span class="header-section-number">5.1.0.3</span> More variables</h4>
<p>We have been using a simple case with just one variable (diet) as an example. In the life sciences, it is quite common to perform experiments with more than one variable. For example, we may be interested in the effect of diet and the difference in sexes. In this case, we have four possible groups:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">diet &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))
sex &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="st">&quot;f&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;m&quot;</span>))
<span class="kw">table</span>(diet,sex)</code></pre></div>
<pre><code>##     sex
## diet f m
##    1 2 2
##    2 2 2</code></pre>
<p>If we assume that the diet effect is the same for males and females (this is an assumption), then our linear model is:</p>
<p><span class="math display">\[
Y_{i}= \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \varepsilon_i 
\]</span></p>
<p>To fit this model in R, we can simply add the additional variable with a <code>+</code> sign in order to build a design matrix which fits based on the information in additional variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">diet &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>))
sex &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="st">&quot;f&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;f&quot;</span>,<span class="st">&quot;m&quot;</span>,<span class="st">&quot;m&quot;</span>))
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>diet <span class="op">+</span><span class="st"> </span>sex)</code></pre></div>
<pre><code>##   (Intercept) diet2 sexm
## 1           1     0    0
## 2           1     0    0
## 3           1     0    1
## 4           1     0    1
## 5           1     1    0
## 6           1     1    0
## 7           1     1    1
## 8           1     1    1
## attr(,&quot;assign&quot;)
## [1] 0 1 2
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$diet
## [1] &quot;contr.treatment&quot;
## 
## attr(,&quot;contrasts&quot;)$sex
## [1] &quot;contr.treatment&quot;</code></pre>
<p>The design matrix includes an intercept, a term for <code>diet</code> and a term for <code>sex</code>. We would say that this linear model accounts for differences in both the group and condition variables. However, as mentioned above, the model assumes that the diet effect is the same for both males and females. We say these are an <em>additive</em> effect. For each variable, we add an effect regardless of what the other is. Another model is possible here, which fits an additional term and which encodes the potential interaction of group and condition variables. We will cover interaction terms in depth in a later script.</p>
<p>The interaction model can be written in either of the following two formulas:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>diet <span class="op">+</span><span class="st"> </span>sex <span class="op">+</span><span class="st"> </span>diet<span class="op">:</span>sex)</code></pre></div>
<p>or</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>diet<span class="op">*</span>sex)</code></pre></div>
<pre><code>##   (Intercept) diet2 sexm diet2:sexm
## 1           1     0    0          0
## 2           1     0    0          0
## 3           1     0    1          0
## 4           1     0    1          0
## 5           1     1    0          0
## 6           1     1    0          0
## 7           1     1    1          1
## 8           1     1    1          1
## attr(,&quot;assign&quot;)
## [1] 0 1 2 3
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$diet
## [1] &quot;contr.treatment&quot;
## 
## attr(,&quot;contrasts&quot;)$sex
## [1] &quot;contr.treatment&quot;</code></pre>
</div>
<div id="releveling" class="section level4">
<h4><span class="header-section-number">5.1.0.4</span> Releveling</h4>
<p>The level which is chosen for the <em>reference level</em> is the level which is contrasted against. By default, this is simply the first level alphabetically. We can specify that we want group 2 to be the reference level by either using the <code>relevel</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))
group &lt;-<span class="st"> </span><span class="kw">relevel</span>(group, <span class="st">&quot;2&quot;</span>)
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group)</code></pre></div>
<pre><code>##   (Intercept) group1
## 1           1      1
## 2           1      1
## 3           1      0
## 4           1      0
## attr(,&quot;assign&quot;)
## [1] 0 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
<p>or by providing the levels explicitly in the <code>factor</code> call:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="kw">factor</span>(group, <span class="dt">levels=</span><span class="kw">c</span>(<span class="st">&quot;1&quot;</span>,<span class="st">&quot;2&quot;</span>))
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group)</code></pre></div>
<pre><code>##   (Intercept) group2
## 1           1      0
## 2           1      0
## 3           1      1
## 4           1      1
## attr(,&quot;assign&quot;)
## [1] 0 1
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$group
## [1] &quot;contr.treatment&quot;</code></pre>
</div>
<div id="where-does-model.matrix-look-for-the-data" class="section level4">
<h4><span class="header-section-number">5.1.0.5</span> Where does model.matrix look for the data?</h4>
<p>The <code>model.matrix</code> function will grab the variable from the R global environment, unless the data is explicitly provided as a data frame to the <code>data</code> argument:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">group &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">4</span>
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>group, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">group=</span><span class="dv">5</span><span class="op">:</span><span class="dv">8</span>))</code></pre></div>
<pre><code>##   (Intercept) group
## 1           1     5
## 2           1     6
## 3           1     7
## 4           1     8
## attr(,&quot;assign&quot;)
## [1] 0 1</code></pre>
<p>Note how the R global environment variable <code>group</code> is ignored.</p>
</div>
<div id="continuous-variables" class="section level4">
<h4><span class="header-section-number">5.1.0.6</span> Continuous variables</h4>
<p>In this chapter, we focus on models based on indicator values. In certain designs, however, we will be interested in using numeric variables in the design formula, as opposed to converting them to factors first. For example, in the falling object example, time was a continuous variable in the model and time squared was also included:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tt &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="fl">3.4</span>,<span class="dt">len=</span><span class="dv">4</span>) 
<span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>tt <span class="op">+</span><span class="st"> </span><span class="kw">I</span>(tt<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>##   (Intercept)    tt I(tt^2)
## 1           1 0.000   0.000
## 2           1 1.133   1.284
## 3           1 2.267   5.138
## 4           1 3.400  11.560
## attr(,&quot;assign&quot;)
## [1] 0 1 2</code></pre>
<p>The <code>I</code> function above is necessary to specify a mathematical transformation of a variable. For more details, see the manual page for the <code>I</code> function by typing <code>?I</code>.</p>
<p>In the life sciences, we could be interested in testing various dosages of a treatment, where we expect a specific relationship between a measured quantity and the dosage, e.g. 0 mg, 10 mg, 20 mg.</p>
<p>The assumptions imposed by including continuous data as variables are typically hard to defend and motivate than the indicator function variables. Whereas the indicator variables simply assume a different mean between two groups, continuous variables assume a very specific relationship between the outcome and predictor variables.</p>
<p>In cases like the falling object, we have the theory of gravitation supporting the model. In the father-son height example, because the data is bivariate normal, it follows that there is a linear relationship if we condition. However, we find that continuous variables are included in linear models without justification to “adjust” for variables such as age. We highly discourage this practice unless the data support the model being used.</p>
</div>
<div id="the-mouse-diet-example" class="section level4">
<h4><span class="header-section-number">5.1.0.7</span> The mouse diet example</h4>
<p>We will demonstrate how to analyze the high fat diet data using linear models instead of directly applying a t-test. We will demonstrate how ultimately these two approaches are equivalent.</p>
<p>We start by reading in the data and creating a quick stripchart:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dat &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;femaleMiceWeights.csv&quot;</span>) ##previously downloaded
<span class="kw">stripchart</span>(dat<span class="op">$</span>Bodyweight <span class="op">~</span><span class="st"> </span>dat<span class="op">$</span>Diet, <span class="dt">vertical=</span><span class="ot">TRUE</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>,
           <span class="dt">main=</span><span class="st">&quot;Bodyweight over Diet&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/bodyweight_by_diet_stripchart-1.png" alt="Mice bodyweights stratified by diet." width="672" />
<p class="caption">
(#fig:bodyweight_by_diet_stripchart)Mice bodyweights stratified by diet.
</p>
</div>
<p>We can see that the high fat diet group appears to have higher weights on average, although there is overlap between the two samples.</p>
<p>For demonstration purposes, we will build the design matrix <span class="math inline">\(\mathbf{X}\)</span> using the formula <code>~ Diet</code>. The group with the 1’s in the second column is determined by the level of <code>Diet</code> which comes second; that is, the non-reference level.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">levels</span>(dat<span class="op">$</span>Diet)</code></pre></div>
<pre><code>## [1] &quot;chow&quot; &quot;hf&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>Diet, <span class="dt">data=</span>dat)
<span class="kw">head</span>(X)</code></pre></div>
<pre><code>##   (Intercept) Diethf
## 1           1      0
## 2           1      0
## 3           1      0
## 4           1      0
## 5           1      0
## 6           1      0</code></pre>
</div>
</div>
<div id="the-mathematics-behind-lm" class="section level2">
<h2><span class="header-section-number">5.2</span> The Mathematics Behind lm()</h2>
<p>Before we use our shortcut for running linear models, <code>lm</code>, we want to review what will happen internally. Inside of <code>lm</code>, we will form the design matrix <span class="math inline">\(\mathbf{X}\)</span> and calculate the <span class="math inline">\(\boldsymbol{\beta}\)</span>, which minimizes the sum of squares using the previously described formula. The formula for this solution is:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \]</span></p>
<p>We can calculate this in R using our matrix multiplication operator <code>%*%</code>, the inverse function <code>solve</code>, and the transpose function <code>t</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y &lt;-<span class="st"> </span>dat<span class="op">$</span>Bodyweight
X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>Diet, <span class="dt">data=</span>dat)
<span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>Y</code></pre></div>
<pre><code>##               [,1]
## (Intercept) 23.813
## Diethf       3.021</code></pre>
<p>These coefficients are the average of the control group and the difference of the averages:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">split</span>(dat<span class="op">$</span>Bodyweight, dat<span class="op">$</span>Diet)
<span class="kw">mean</span>(s[[<span class="st">&quot;chow&quot;</span>]])</code></pre></div>
<pre><code>## [1] 23.81</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(s[[<span class="st">&quot;hf&quot;</span>]]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(s[[<span class="st">&quot;chow&quot;</span>]])</code></pre></div>
<pre><code>## [1] 3.021</code></pre>
<p>Finally, we use our shortcut, <code>lm</code>, to run the linear model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">lm</span>(Bodyweight <span class="op">~</span><span class="st"> </span>Diet, <span class="dt">data=</span>dat)
<span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Bodyweight ~ Diet, data = dat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -6.104 -2.436 -0.414  2.834  7.186 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    23.81       1.04   22.91   &lt;2e-16 ***
## Diethf          3.02       1.47    2.06    0.052 .  
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.6 on 22 degrees of freedom
## Multiple R-squared:  0.161,  Adjusted R-squared:  0.123 
## F-statistic: 4.22 on 1 and 22 DF,  p-value: 0.0519</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(coefs &lt;-<span class="st"> </span><span class="kw">coef</span>(fit))</code></pre></div>
<pre><code>## (Intercept)      Diethf 
##      23.813       3.021</code></pre>
<div id="examining-the-coefficients" class="section level4">
<h4><span class="header-section-number">5.2.0.1</span> Examining the coefficients</h4>
<p>The following plot provides a visualization of the meaning of the coefficients with colored arrows (code not shown):</p>
<div class="figure">
<img src="bookdown_files/figure-html/parameter_estimate_illustration-1.png" alt="Estimated linear model coefficients for bodyweight data illustrated with arrows." width="672" />
<p class="caption">
(#fig:parameter_estimate_illustration)Estimated linear model coefficients for bodyweight data illustrated with arrows.
</p>
</div>
<p>To make a connection with material presented earlier, this simple linear model is actually giving us the same result (the t-statistic and p-value) for the difference as a specific kind of t-test. This is the t-test between two groups with the assumption that the population standard deviation is the same for both groups. This was encoded into our linear model when we assumed that the errors <span class="math inline">\(\boldsymbol{\varepsilon}\)</span> were all equally distributed.</p>
<p>Although in this case the linear model is equivalent to a t-test, we will soon explore more complicated designs, where the linear model is a useful extension. Below we demonstrate that one does in fact get the exact same results:</p>
<p>Our <code>lm</code> estimates were:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)<span class="op">$</span>coefficients</code></pre></div>
<pre><code>##             Estimate Std. Error t value  Pr(&gt;|t|)
## (Intercept)   23.813      1.039  22.912 7.642e-17
## Diethf         3.021      1.470   2.055 5.192e-02</code></pre>
<p>And the t-statistic is the same:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ttest &lt;-<span class="st"> </span><span class="kw">t.test</span>(s[[<span class="st">&quot;hf&quot;</span>]], s[[<span class="st">&quot;chow&quot;</span>]], <span class="dt">var.equal=</span><span class="ot">TRUE</span>)
<span class="kw">summary</span>(fit)<span class="op">$</span>coefficients[<span class="dv">2</span>,<span class="dv">3</span>]</code></pre></div>
<pre><code>## [1] 2.055</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ttest<span class="op">$</span>statistic</code></pre></div>
<pre><code>##     t 
## 2.055</code></pre>
</div>
</div>
<div id="standard-errors" class="section level2">
<h2><span class="header-section-number">5.3</span> Standard Errors</h2>
<p>We have shown how to find the least squares estimates with matrix algebra. These estimates are random variables since they are linear combinations of the data. For these estimates to be useful, we also need to compute their standard errors. Linear algebra provides a powerful approach for this task. We provide several examples.</p>
<div id="falling-object" class="section level4">
<h4><span class="header-section-number">5.3.0.1</span> Falling object</h4>
<p>It is useful to think about where randomness comes from. In our falling object example, randomness was introduced through measurement errors. Each time we rerun the experiment, a new set of measurement errors will be made. This implies that our data will change randomly, which in turn suggests that our estimates will change randomly. For instance, our estimate of the gravitational constant will change every time we perform the experiment. The constant is fixed, but our estimates are not. To see this we can run a Monte Carlo simulation. Specifically, we will generate the data repeatedly and each time compute the estimate for the quadratic term.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">1</span>)
B &lt;-<span class="st"> </span><span class="dv">10000</span>
h0 &lt;-<span class="st"> </span><span class="fl">56.67</span>
v0 &lt;-<span class="st"> </span><span class="dv">0</span>
g &lt;-<span class="st"> </span><span class="fl">9.8</span> ##meters per second

n &lt;-<span class="st"> </span><span class="dv">25</span>
tt &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="fl">3.4</span>,<span class="dt">len=</span>n) ##time in secs, t is a base function
X &lt;-<span class="kw">cbind</span>(<span class="dv">1</span>,tt,tt<span class="op">^</span><span class="dv">2</span>)
##create X&#39;X^-1 X&#39;
A &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(X)) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X)
betahat&lt;-<span class="kw">replicate</span>(B,{
  y &lt;-<span class="st"> </span>h0 <span class="op">+</span><span class="st"> </span>v0<span class="op">*</span>tt  <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span><span class="op">*</span>g<span class="op">*</span>tt<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="dv">1</span>)
  betahats &lt;-<span class="st"> </span>A<span class="op">%*%</span>y
  <span class="kw">return</span>(betahats[<span class="dv">3</span>])
})
<span class="kw">head</span>(betahat)</code></pre></div>
<pre><code>## [1] -5.039 -4.894 -5.144 -5.221 -5.063 -4.778</code></pre>
<p>As expected, the estimate is different every time. This is because <span class="math inline">\(\hat{\beta}\)</span> is a random variable. It therefore has a distribution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="kw">hist</span>(betahat)
<span class="kw">qqnorm</span>(betahat)
<span class="kw">qqline</span>(betahat)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/regression_estimates_normally_distributed-1.png" alt="Distribution of estimated regression coefficients obtained from Monte Carlo simulated falling object data. The left is a histogram and on the right we have a qq-plot against normal theoretical quantiles." width="1008" />
<p class="caption">
(#fig:regression_estimates_normally_distributed)Distribution of estimated regression coefficients obtained from Monte Carlo simulated falling object data. The left is a histogram and on the right we have a qq-plot against normal theoretical quantiles.
</p>
</div>
<p>Since <span class="math inline">\(\hat{\beta}\)</span> is a linear combination of the data which we made normal in our simulation, it is also normal as seen in the qq-plot above. Also, the mean of the distribution is the true parameter <span class="math inline">\(-0.5g\)</span>, as confirmed by the Monte Carlo simulation performed above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">mean</span>(betahat),<span class="dv">1</span>)</code></pre></div>
<pre><code>## [1] -4.9</code></pre>
<p>But we will not observe this exact value when we estimate because the standard error of our estimate is approximately:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(betahat) </code></pre></div>
<pre><code>## [1] 0.213</code></pre>
<p>Here we will show how we can compute the standard error without a Monte Carlo simulation. Since in practice we do not know exactly how the errors are generated, we can’t use the Monte Carlo approach.</p>
</div>
<div id="father-and-son-heights" class="section level4">
<h4><span class="header-section-number">5.3.0.2</span> Father and son heights</h4>
<p>In the father and son height examples, we have randomness because we have a random sample of father and son pairs. For the sake of illustration, let’s assume that this is the entire population:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(father.son,<span class="dt">package=</span><span class="st">&quot;UsingR&quot;</span>)
x &lt;-<span class="st"> </span>father.son<span class="op">$</span>fheight
y &lt;-<span class="st"> </span>father.son<span class="op">$</span>sheight
n &lt;-<span class="st"> </span><span class="kw">length</span>(y)</code></pre></div>
<p>Now let’s run a Monte Carlo simulation in which we take a sample size of 50 over and over again.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="dv">50</span>
B &lt;-<span class="dv">1000</span>
betahat &lt;-<span class="st"> </span><span class="kw">replicate</span>(B,{
  index &lt;-<span class="st"> </span><span class="kw">sample</span>(n,N)
  sampledat &lt;-<span class="st"> </span>father.son[index,]
  x &lt;-<span class="st"> </span>sampledat<span class="op">$</span>fheight
  y &lt;-<span class="st"> </span>sampledat<span class="op">$</span>sheight
  <span class="kw">lm</span>(y<span class="op">~</span>x)<span class="op">$</span>coef
  })
betahat &lt;-<span class="st"> </span><span class="kw">t</span>(betahat) <span class="co">#have estimates in two columns</span></code></pre></div>
<p>By making qq-plots, we see that our estimates are approximately normal random variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="kw">qqnorm</span>(betahat[,<span class="dv">1</span>])
<span class="kw">qqline</span>(betahat[,<span class="dv">1</span>])
<span class="kw">qqnorm</span>(betahat[,<span class="dv">2</span>])
<span class="kw">qqline</span>(betahat[,<span class="dv">2</span>])</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/regression_estimates_normally_distributed2-1.png" alt="Distribution of estimated regression coefficients obtained from Monte Carlo simulated father-son height data. The left is a histogram and on the right we have a qq-plot against normal theoretical quantiles." width="1008" />
<p class="caption">
(#fig:regression_estimates_normally_distributed2)Distribution of estimated regression coefficients obtained from Monte Carlo simulated father-son height data. The left is a histogram and on the right we have a qq-plot against normal theoretical quantiles.
</p>
</div>
<p>We also see that the correlation of our estimates is negative:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cor</span>(betahat[,<span class="dv">1</span>],betahat[,<span class="dv">2</span>])</code></pre></div>
<pre><code>## [1] -0.9992</code></pre>
<p>When we compute linear combinations of our estimates, we will need to know this information to correctly calculate the standard error of these linear combinations.</p>
<p>In the next section, we will describe the variance-covariance matrix. The covariance of two random variables is defined as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>( (betahat[,<span class="dv">1</span>]<span class="op">-</span><span class="kw">mean</span>(betahat[,<span class="dv">1</span>] ))<span class="op">*</span><span class="st"> </span>(betahat[,<span class="dv">2</span>]<span class="op">-</span><span class="kw">mean</span>(betahat[,<span class="dv">2</span>])))</code></pre></div>
<pre><code>## [1] -1.035</code></pre>
<p>The covariance is the correlation multiplied by the standard deviations of each random variable:</p>
<p><span class="math display">\[\mbox{Corr}(X,Y) = \frac{\mbox{Cov}(X,Y)}{\sigma_X \sigma_Y}\]</span></p>
<p>Other than that, this quantity does not have a useful interpretation in practice. However, as we will see, it is a very useful quantity for mathematical derivations. In the next sections, we show useful matrix algebra calculations that can be used to estimate standard errors of linear model estimates.</p>
<p><a name="varcov"></a></p>
</div>
<div id="variance-covariance-matrix-advanced" class="section level4">
<h4><span class="header-section-number">5.3.0.3</span> Variance-covariance matrix (Advanced)</h4>
<p>As a first step we need to define the <em>variance-covariance matrix</em>, <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. For a vector of random variables, <span class="math inline">\(\mathbf{Y}\)</span>, we define <span class="math inline">\(\boldsymbol{\Sigma}\)</span> as the matrix with the <span class="math inline">\(i,j\)</span> entry:</p>
<p><span class="math display">\[ \Sigma_{i,j} \equiv \mbox{Cov}(Y_i, Y_j) \]</span></p>
<p>The covariance is equal to the variance if <span class="math inline">\(i = j\)</span> and equal to 0 if the variables are independent. In the kinds of vectors considered up to now, for example, a vector <span class="math inline">\(\mathbf{Y}\)</span> of individual observations <span class="math inline">\(Y_i\)</span> sampled from a population, we have assumed independence of each observation and assumed the <span class="math inline">\(Y_i\)</span> all have the same variance <span class="math inline">\(\sigma^2\)</span>, so the variance-covariance matrix has had only two kinds of elements:</p>
<p><span class="math display">\[ \mbox{Cov}(Y_i, Y_i) = \mbox{var}(Y_i) = \sigma^2\]</span></p>
<p><span class="math display">\[ \mbox{Cov}(Y_i, Y_j) = 0, \mbox{ for } i \neq j\]</span></p>
<p>which implies that <span class="math inline">\(\boldsymbol{\Sigma} = \sigma^2 \mathbf{I}\)</span> with <span class="math inline">\(\mathbf{I}\)</span>, the identity matrix.</p>
<p>Later, we will see a case, specifically the estimate coefficients of a linear model, <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, that has non-zero entries in the off diagonal elements of <span class="math inline">\(\boldsymbol{\Sigma}\)</span>. Furthermore, the diagonal elements will not be equal to a single value <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="variance-of-a-linear-combination" class="section level4">
<h4><span class="header-section-number">5.3.0.4</span> Variance of a linear combination</h4>
<p>A useful result provided by linear algebra is that the variance covariance-matrix of a linear combination <span class="math inline">\(\mathbf{AY}\)</span> of <span class="math inline">\(\mathbf{Y}\)</span> can be computed as follows:</p>
<p><span class="math display">\[
\mbox{var}(\mathbf{AY}) = \mathbf{A}\mbox{var}(\mathbf{Y}) \mathbf{A}^\top 
\]</span></p>
<p>For example, if <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent both with variance <span class="math inline">\(\sigma^2\)</span> then:</p>
<p><span class="math display">\[\mbox{var}\{Y_1+Y_2\} = 
\mbox{var}\left\{ \begin{pmatrix}1&amp;1\end{pmatrix}\begin{pmatrix} Y_1\\Y_2\\ \end{pmatrix}\right\}\]</span></p>
<p><span class="math display">\[ =\begin{pmatrix}1&amp;1\end{pmatrix} \sigma^2 \mathbf{I}\begin{pmatrix} 1\\1\\ \end{pmatrix}=2\sigma^2\]</span></p>
<p>as we expect. We use this result to obtain the standard errors of the LSE (least squares estimate).</p>
</div>
<div id="lse-standard-errors-advanced" class="section level4">
<h4><span class="header-section-number">5.3.0.5</span> LSE standard errors (Advanced)</h4>
<p>Note that <span class="math inline">\(\boldsymbol{\hat{\beta}}\)</span> is a linear combination of <span class="math inline">\(\mathbf{Y}\)</span>: <span class="math inline">\(\mathbf{AY}\)</span> with <span class="math inline">\(\mathbf{A}=\mathbf{(X^\top X)^{-1}X}^\top\)</span>, so we can use the equation above to derive the variance of our estimates:</p>
<p><span class="math display">\[\mbox{var}(\boldsymbol{\hat{\beta}}) = \mbox{var}( \mathbf{(X^\top X)^{-1}X^\top Y} ) =  \]</span></p>
<p><span class="math display">\[\mathbf{(X^\top X)^{-1} X^\top} \mbox{var}(Y) (\mathbf{(X^\top X)^{-1} X^\top})^\top = \]</span></p>
<p><span class="math display">\[\mathbf{(X^\top X)^{-1} X^\top} \sigma^2 \mathbf{I} (\mathbf{(X^\top X)^{-1} X^\top})^\top = \]</span></p>
<p><span class="math display">\[\sigma^2 \mathbf{(X^\top X)^{-1} X^\top}\mathbf{X} \mathbf{(X^\top X)^{-1}} = \]</span></p>
<p><span class="math display">\[\sigma^2\mathbf{(X^\top X)^{-1}}\]</span></p>
<p>The diagonal of the square root of this matrix contains the standard error of our estimates.</p>
</div>
<div id="estimating-sigma2" class="section level4">
<h4><span class="header-section-number">5.3.0.6</span> Estimating <span class="math inline">\(\sigma^2\)</span></h4>
<p>To obtain an actual estimate in practice from the formulas above, we need to estimate <span class="math inline">\(\sigma^2\)</span>. Previously we estimated the standard errors from the sample. However, the sample standard deviation of <span class="math inline">\(Y\)</span> is not <span class="math inline">\(\sigma\)</span> because <span class="math inline">\(Y\)</span> also includes variability introduced by the deterministic part of the model: <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>. The approach we take is to use the residuals.</p>
<p>We form the residuals like this:</p>
<p><span class="math display">\[
\mathbf{r}\equiv\boldsymbol{\hat{\varepsilon}} = \mathbf{Y}-\mathbf{X}\boldsymbol{\hat{\beta}}\]</span></p>
<p>Both <span class="math inline">\(\mathbf{r}\)</span> and <span class="math inline">\(\boldsymbol{\hat{\varepsilon}}\)</span> notations are used to denote residuals.</p>
<p>Then we use these to estimate, in a similar way, to what we do in the univariate case:</p>
<p><span class="math display">\[ s^2 \equiv \hat{\sigma}^2 = \frac{1}{N-p}\mathbf{r}^\top\mathbf{r} = \frac{1}{N-p}\sum_{i=1}^N r_i^2\]</span></p>
<p>Here <span class="math inline">\(N\)</span> is the sample size and <span class="math inline">\(p\)</span> is the number of columns in <span class="math inline">\(\mathbf{X}\)</span> or number of parameters (including the intercept term <span class="math inline">\(\beta_0\)</span>). The reason we divide by <span class="math inline">\(N-p\)</span> is because mathematical theory tells us that this will give us a better (unbiased) estimate.</p>
<p>Let’s try this in R and see if we obtain the same values as we did with the Monte Carlo simulation above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">nrow</span>(father.son)
N &lt;-<span class="st"> </span><span class="dv">50</span>
index &lt;-<span class="st"> </span><span class="kw">sample</span>(n,N)
sampledat &lt;-<span class="st"> </span>father.son[index,]
x &lt;-<span class="st"> </span>sampledat<span class="op">$</span>fheight
y &lt;-<span class="st"> </span>sampledat<span class="op">$</span>sheight
X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>x)

N &lt;-<span class="st"> </span><span class="kw">nrow</span>(X)
p &lt;-<span class="st"> </span><span class="kw">ncol</span>(X)

XtXinv &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">crossprod</span>(X))

resid &lt;-<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>X <span class="op">%*%</span><span class="st"> </span>XtXinv <span class="op">%*%</span><span class="st"> </span><span class="kw">crossprod</span>(X,y)

s &lt;-<span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">sum</span>(resid<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(N<span class="op">-</span>p))
ses &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(XtXinv))<span class="op">*</span>s </code></pre></div>
<p>Let’s compare to what <code>lm</code> provides:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(y<span class="op">~</span>x))<span class="op">$</span>coef[,<span class="dv">2</span>]</code></pre></div>
<pre><code>## (Intercept)           x 
##      8.3900      0.1241</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ses</code></pre></div>
<pre><code>## (Intercept)           x 
##      8.3900      0.1241</code></pre>
<p>They are identical because they are doing the same thing. Also, note that we approximate the Monte Carlo results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">apply</span>(betahat,<span class="dv">2</span>,sd)</code></pre></div>
<pre><code>## (Intercept)           x 
##      8.3818      0.1237</code></pre>
</div>
<div id="linear-combination-of-estimates" class="section level4">
<h4><span class="header-section-number">5.3.0.7</span> Linear combination of estimates</h4>
<p>Frequently, we want to compute the standard deviation of a linear combination of estimates such as <span class="math inline">\(\hat{\beta}_2 - \hat{\beta}_1\)</span>. This is a linear combination of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>:</p>
<p><span class="math display">\[\hat{\beta}_2 - \hat{\beta}_1 = 
\begin{pmatrix}0&amp;-1&amp;1&amp;0&amp;\dots&amp;0\end{pmatrix} \begin{pmatrix}
\hat{\beta}_0\\
\hat{\beta}_1 \\ 
\hat{\beta}_2 \\ 
\vdots\\
\hat{\beta}_p
\end{pmatrix}\]</span></p>
<p>Using the above, we know how to compute the variance covariance matrix of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>.</p>
</div>
<div id="clt-and-t-distribution" class="section level4">
<h4><span class="header-section-number">5.3.0.8</span> CLT and t-distribution</h4>
<p>We have shown how we can obtain standard errors for our estimates. However, as we learned in the first chapter, to perform inference we need to know the distribution of these random variables. The reason we went through the effort to compute the standard errors is because the CLT applies in linear models. If <span class="math inline">\(N\)</span> is large enough, then the LSE will be normally distributed with mean <span class="math inline">\(\boldsymbol{\beta}\)</span> and standard errors as described. For small samples, if the <span class="math inline">\(\varepsilon\)</span> are normally distributed, then the <span class="math inline">\(\hat{\beta}-\beta\)</span> follow a t-distribution. We do not derive this result here, but the results are extremely useful since it is how we construct p-values and confidence intervals in the context of linear models.</p>
</div>
<div id="code-versus-math" class="section level4">
<h4><span class="header-section-number">5.3.0.9</span> Code versus math</h4>
<p>The standard approach to writing linear models either assume the values in <span class="math inline">\(\mathbf{X}\)</span> are fixed or that we are conditioning on them. Thus <span class="math inline">\(\mathbf{X} \boldsymbol{\beta}\)</span> has no variance as the <span class="math inline">\(\mathbf{X}\)</span> is considered fixed. This is why we write <span class="math inline">\(\mbox{var}(Y_i) = \mbox{var}(\varepsilon_i)=\sigma^2\)</span>. This can cause confusion in practice because if you, for example, compute the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x =<span class="st">  </span>father.son<span class="op">$</span>fheight
beta =<span class="st">  </span><span class="kw">c</span>(<span class="dv">34</span>,<span class="fl">0.5</span>)
<span class="kw">var</span>(beta[<span class="dv">1</span>]<span class="op">+</span>beta[<span class="dv">2</span>]<span class="op">*</span>x)</code></pre></div>
<pre><code>## [1] 1.884</code></pre>
<p>it is nowhere near 0. This is an example in which we have to be careful in distinguishing code from math. The function <code>var</code> is simply computing the variance of the list we feed it, while the mathematical definition of variance is considering only quantities that are random variables. In the R code above, <code>x</code> is not fixed at all: we are letting it vary, but when we write <span class="math inline">\(\mbox{var}(Y_i) = \sigma^2\)</span> we are imposing, mathematically, <code>x</code> to be fixed. Similarly, if we use R to compute the variance of <span class="math inline">\(Y\)</span> in our object dropping example, we obtain something very different than <span class="math inline">\(\sigma^2=1\)</span> (the known variance):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="kw">length</span>(tt)
y &lt;-<span class="st"> </span>h0 <span class="op">+</span><span class="st"> </span>v0<span class="op">*</span>tt  <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span><span class="op">*</span>g<span class="op">*</span>tt<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="dv">1</span>)
<span class="kw">var</span>(y)</code></pre></div>
<pre><code>## [1] 329.5</code></pre>
<p>Again, this is because we are not fixing <code>tt</code>.</p>
</div>
</div>
<div id="interactions-and-contrasts" class="section level2">
<h2><span class="header-section-number">5.4</span> Interactions and Contrasts</h2>
<p>As a running example to learn about more complex linear models, we will be using a dataset which compares the different frictional coefficients on the different legs of a spider. Specifically, we will be determining whether more friction comes from a pushing or pulling motion of the leg. The original paper from which the data was provided is:</p>
<p>Jonas O. Wolff &amp; Stanislav N. Gorb, <a href="http://dx.doi.org/10.1038/srep01101">Radial arrangement of Janus-like setae permits friction control in spiders</a>, Scientific Reports, 22 January 2013.</p>
<p>The abstract of the paper says,</p>
<blockquote>
<p>The hunting spider Cupiennius salei (Arachnida, Ctenidae) possesses hairy attachment pads (claw tufts) at its distal legs, consisting of directional branched setae… Friction of claw tufts on smooth glass was measured to reveal the functional effect of seta arrangement within the pad.</p>
</blockquote>
<p><a href="http://www.nature.com/articles/srep01101/figures/1">Figure 1</a> includes some pretty cool electron microscope images of the tufts. We are interested in the comparisons in <a href="http://www.nature.com/articles/srep01101/figures/4">Figure 4</a>, where the pulling and pushing motions are compared for different leg pairs (for a diagram of pushing and pulling see the top of <a href="http://www.nature.com/articles/srep01101/figures/3">Figure 3</a>).</p>
<p>We include the data in our dagdata package and can download it from <a href="https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/spider_wolff_gorb_2013.csv">here</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">spider &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;spider_wolff_gorb_2013.csv&quot;</span>, <span class="dt">skip=</span><span class="dv">1</span>)</code></pre></div>
<div id="initial-visual-inspection-of-the-data" class="section level4">
<h4><span class="header-section-number">5.4.0.1</span> Initial visual inspection of the data</h4>
<p>Each measurement comes from one of our legs while it is either pushing or pulling. So we have two variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(spider<span class="op">$</span>leg,spider<span class="op">$</span>type)</code></pre></div>
<pre><code>##     
##      pull push
##   L1   34   34
##   L2   15   15
##   L3   52   52
##   L4   40   40</code></pre>
<p>We can make a boxplot summarizing the measurements for each of the eight pairs. This is similar to Figure 4 of the original paper:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(spider<span class="op">$</span>friction <span class="op">~</span><span class="st"> </span>spider<span class="op">$</span>type <span class="op">*</span><span class="st"> </span>spider<span class="op">$</span>leg, 
        <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;grey90&quot;</span>,<span class="st">&quot;grey40&quot;</span>), <span class="dt">las=</span><span class="dv">2</span>, 
        <span class="dt">main=</span><span class="st">&quot;Comparison of friction coefficients of different leg pairs&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/spide_data-1.png" alt="Comparison of friction coefficients of spiders' different leg pairs. The friction coefficient is calculated as the ratio of two forces (see paper Methods) so it is unitless." width="672" />
<p class="caption">
(#fig:spide_data)Comparison of friction coefficients of spiders’ different leg pairs. The friction coefficient is calculated as the ratio of two forces (see paper Methods) so it is unitless.
</p>
</div>
<p>What we can immediately see are two trends:</p>
<ul>
<li>The pulling motion has higher friction than the pushing motion.</li>
<li>The leg pairs to the back of the spider (L4 being the last) have higher pulling friction.</li>
</ul>
<p>Another thing to notice is that the groups have different spread around their average, what we call <em>within-group variance</em>. This is somewhat of a problem for the kinds of linear models we will explore below, since we will be assuming that around the population average values, the errors <span class="math inline">\(\varepsilon_i\)</span> are distributed identically, meaning the same variance within each group. The consequence of ignoring the different variances for the different groups is that comparisons between those groups with small variances will be overly “conservative” (because the overall estimate of variance is larger than an estimate for just these groups), and comparisons between those groups with large variances will be overly confident. If the spread is related to the range of friction, such that groups with large friction values also have larger spread, a possibility is to transform the data with a function such as the <code>log</code> or <code>sqrt</code>. This looks like it could be useful here, since three of the four push groups (L1, L2, L3) have the smallest friction values and also the smallest spread.</p>
<p>Some alternative tests for comparing groups without transforming the values first include: t-tests without the equal variance assumption using a “Welch” or “Satterthwaite approximation”, or the Wilcoxon rank sum test mentioned previously. However here, for simplicity of illustration, we will fit a model that assumes equal variance and shows the different kinds of linear model designs using this dataset, setting aside the issue of different within-group variances.</p>
</div>
<div id="a-linear-model-with-one-variable" class="section level4">
<h4><span class="header-section-number">5.4.0.2</span> A linear model with one variable</h4>
<p>To remind ourselves how the simple two-group linear model looks, we will subset the data to include only the L1 leg pair, and run <code>lm</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">spider.sub &lt;-<span class="st"> </span>spider[spider<span class="op">$</span>leg <span class="op">==</span><span class="st"> &quot;L1&quot;</span>,]
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(friction <span class="op">~</span><span class="st"> </span>type, <span class="dt">data=</span>spider.sub)
<span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = friction ~ type, data = spider.sub)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.3315 -0.1074 -0.0494 -0.0015  0.7685 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.9215     0.0383    24.1  &lt; 2e-16 ***
## typepush     -0.5141     0.0541    -9.5  5.7e-14 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.223 on 66 degrees of freedom
## Multiple R-squared:  0.578,  Adjusted R-squared:  0.571 
## F-statistic: 90.2 on 1 and 66 DF,  p-value: 5.7e-14</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(coefs &lt;-<span class="st"> </span><span class="kw">coef</span>(fit))</code></pre></div>
<pre><code>## (Intercept)    typepush 
##      0.9215     -0.5141</code></pre>
<p>These two estimated coefficients are the mean of the pull observations (the first estimated coefficient) and the difference between the means of the two groups (the second coefficient). We can show this with R code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">split</span>(spider.sub<span class="op">$</span>friction, spider.sub<span class="op">$</span>type)
<span class="kw">mean</span>(s[[<span class="st">&quot;pull&quot;</span>]])</code></pre></div>
<pre><code>## [1] 0.9215</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(s[[<span class="st">&quot;push&quot;</span>]]) <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(s[[<span class="st">&quot;pull&quot;</span>]])</code></pre></div>
<pre><code>## [1] -0.5141</code></pre>
<p>We can form the design matrix, which was used inside <code>lm</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>type, <span class="dt">data=</span>spider.sub)
<span class="kw">colnames</span>(X)</code></pre></div>
<pre><code>## [1] &quot;(Intercept)&quot; &quot;typepush&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(X)</code></pre></div>
<pre><code>##   (Intercept) typepush
## 1           1        0
## 2           1        0
## 3           1        0
## 4           1        0
## 5           1        0
## 6           1        0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(X)</code></pre></div>
<pre><code>##    (Intercept) typepush
## 63           1        1
## 64           1        1
## 65           1        1
## 66           1        1
## 67           1        1
## 68           1        1</code></pre>
<p>Now we’ll make a plot of the <span class="math inline">\(\mathbf{X}\)</span> matrix by putting a black block for the 1’s and a white block for the 0’s. This plot will be more interesting for the linear models later on in this script. Along the y-axis is the sample number (the row number of the <code>data</code>) and along the x-axis is the column of the design matrix <span class="math inline">\(\mathbf{X}\)</span>. If you have installed the <em>rafalib</em> library, you can make this plot with the <code>imagemat</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">imagemat</span>(X, <span class="dt">main=</span><span class="st">&quot;Model matrix for linear model with one variable&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/model_matrix_image-1.png" alt="Model matrix for linear model with one variable." width="672" />
<p class="caption">
(#fig:model_matrix_image)Model matrix for linear model with one variable.
</p>
</div>
</div>
<div id="examining-the-estimated-coefficients" class="section level4">
<h4><span class="header-section-number">5.4.0.3</span> Examining the estimated coefficients</h4>
<p>Now we show the coefficient estimates from the linear model in a diagram with arrows (code not shown).</p>
<div class="figure">
<img src="bookdown_files/figure-html/spider_main_coef-1.png" alt="Diagram of the estimated coefficients in the linear model. The green arrow indicates the Intercept term, which goes from zero to the mean of the reference group (here the 'pull' samples). The orange arrow indicates the difference between the push group and the pull group, which is negative in this example. The circles show the individual samples, jittered horizontally to avoid overplotting." width="672" />
<p class="caption">
(#fig:spider_main_coef)Diagram of the estimated coefficients in the linear model. The green arrow indicates the Intercept term, which goes from zero to the mean of the reference group (here the ‘pull’ samples). The orange arrow indicates the difference between the push group and the pull group, which is negative in this example. The circles show the individual samples, jittered horizontally to avoid overplotting.
</p>
</div>
</div>
<div id="a-linear-model-with-two-variables" class="section level4">
<h4><span class="header-section-number">5.4.0.4</span> A linear model with two variables</h4>
<p>Now we’ll continue and examine the full dataset, including the observations from all leg pairs. In order to model both the leg pair differences (L1, L2, L3, L4) and the push vs. pull difference, we need to include both terms in the R formula. Let’s see what kind of design matrix will be formed with two variables in the formula:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>type <span class="op">+</span><span class="st"> </span>leg, <span class="dt">data=</span>spider)
<span class="kw">colnames</span>(X)</code></pre></div>
<pre><code>## [1] &quot;(Intercept)&quot; &quot;typepush&quot;    &quot;legL2&quot;      
## [4] &quot;legL3&quot;       &quot;legL4&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(X)</code></pre></div>
<pre><code>##   (Intercept) typepush legL2 legL3 legL4
## 1           1        0     0     0     0
## 2           1        0     0     0     0
## 3           1        0     0     0     0
## 4           1        0     0     0     0
## 5           1        0     0     0     0
## 6           1        0     0     0     0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">imagemat</span>(X, <span class="dt">main=</span><span class="st">&quot;Model matrix for linear model with two factors&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/model_matrix_image2-1.png" alt="Image of the model matrix for a formula with type + leg" width="672" />
<p class="caption">
(#fig:model_matrix_image2)Image of the model matrix for a formula with type + leg
</p>
</div>
<p>The first column is the intercept, and so it has 1’s for all samples. The second column has 1’s for the push samples, and we can see that there are four groups of them. Finally, the third, fourth and fifth columns have 1’s for the L2, L3 and L4 samples. The L1 samples do not have a column, because <em>L1</em> is the reference level for <code>leg</code>. Similarly, there is no <em>pull</em> column, because <em>pull</em> is the reference level for the <code>type</code> variable.</p>
<p>To estimate coefficients for this model, we use <code>lm</code> with the formula <code>~ type + leg</code>. We’ll save the linear model to <code>fitTL</code> standing for a <em>fit</em> with <em>Type</em> and <em>Leg</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fitTL &lt;-<span class="st"> </span><span class="kw">lm</span>(friction <span class="op">~</span><span class="st"> </span>type <span class="op">+</span><span class="st"> </span>leg, <span class="dt">data=</span>spider)
<span class="kw">summary</span>(fitTL)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = friction ~ type + leg, data = spider)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4639 -0.1344 -0.0053  0.1055  0.6951 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.0539     0.0282   37.43  &lt; 2e-16 ***
## typepush     -0.7790     0.0248  -31.38  &lt; 2e-16 ***
## legL2         0.1719     0.0457    3.76    2e-04 ***
## legL3         0.1605     0.0325    4.94  1.4e-06 ***
## legL4         0.2813     0.0344    8.18  1.0e-14 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.208 on 277 degrees of freedom
## Multiple R-squared:  0.792,  Adjusted R-squared:  0.789 
## F-statistic:  263 on 4 and 277 DF,  p-value: &lt;2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(coefs &lt;-<span class="st"> </span><span class="kw">coef</span>(fitTL))</code></pre></div>
<pre><code>## (Intercept)    typepush       legL2       legL3 
##      1.0539     -0.7790      0.1719      0.1605 
##       legL4 
##      0.2813</code></pre>
<p>R uses the name <code>coefficient</code> to denote the component containing the least squares <strong>estimates</strong>. It is important to remember that the coefficients are parameters that we do not observe, but only estimate.</p>
</div>
<div id="mathematical-representation" class="section level4">
<h4><span class="header-section-number">5.4.0.5</span> Mathematical representation</h4>
<p>The model we are fitting above can be written as</p>
<p><span class="math display">\[
Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \beta_3 x_{i,3} + \beta_4 x_{i,4} + \varepsilon_i, i=1,\dots,N
\]</span></p>
<p>with the <span class="math inline">\(x\)</span> all indicator variables denoting push or pull and which leg. For example, a push on leg 3 will have <span class="math inline">\(x_{i,1}\)</span> and <span class="math inline">\(x_{i,3}\)</span> equal to 1 and the rest would be 0. Throughout this section we will refer to the <span class="math inline">\(\beta\)</span> s with the effects they represent. For example we call <span class="math inline">\(\beta_0\)</span> the intercept, <span class="math inline">\(\beta_1\)</span> the pull effect, <span class="math inline">\(\beta_2\)</span> the L2 effect, etc. We do not observe the coefficients, e.g. <span class="math inline">\(\beta_1\)</span>, directly, but estimate them with, e.g. <span class="math inline">\(\hat{\beta}_4\)</span>.</p>
<p>We can now form the matrix <span class="math inline">\(\mathbf{X}\)</span> depicted above and obtain the least square estimates with:</p>
<p><span class="math display">\[ \hat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y} \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y &lt;-<span class="st"> </span>spider<span class="op">$</span>friction
X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>type <span class="op">+</span><span class="st"> </span>leg, <span class="dt">data=</span>spider)
beta.hat &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>Y
<span class="kw">t</span>(beta.hat)</code></pre></div>
<pre><code>##      (Intercept) typepush  legL2  legL3  legL4
## [1,]       1.054   -0.779 0.1719 0.1605 0.2813</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs</code></pre></div>
<pre><code>## (Intercept)    typepush       legL2       legL3 
##      1.0539     -0.7790      0.1719      0.1605 
##       legL4 
##      0.2813</code></pre>
<p>We can see that these values agree with the output of <code>lm</code>.</p>
</div>
<div id="examining-the-estimated-coefficients-1" class="section level4">
<h4><span class="header-section-number">5.4.0.6</span> Examining the estimated coefficients</h4>
<p>We can make the same plot as before, with arrows for each of the estimated coefficients in the model (code not shown).</p>
<div class="figure">
<img src="bookdown_files/figure-html/spider_interactions-1.png" alt="Diagram of the estimated coefficients in the linear model. As before, the teal-green arrow represents the Intercept, which fits the mean of the reference group (here, the pull samples for leg L1). The purple, pink, and yellow-green arrows represent differences between the three other leg groups and L1. The orange arrow represents the difference between the push and pull samples for all groups." width="672" />
<p class="caption">
(#fig:spider_interactions)Diagram of the estimated coefficients in the linear model. As before, the teal-green arrow represents the Intercept, which fits the mean of the reference group (here, the pull samples for leg L1). The purple, pink, and yellow-green arrows represent differences between the three other leg groups and L1. The orange arrow represents the difference between the push and pull samples for all groups.
</p>
</div>
<p>In this case, the fitted means for each group, derived from the fitted coefficients, do not line up with those we obtain from simply taking the average from each of the eight possible groups. The reason is that our model uses five coefficients, instead of eight. We are <strong>assuming</strong> that the effects are additive. However, as we demonstrate in more detail below, this particular dataset is better described with a model including interactions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">split</span>(spider<span class="op">$</span>friction, spider<span class="op">$</span>group)
<span class="kw">mean</span>(s[[<span class="st">&quot;L1pull&quot;</span>]])</code></pre></div>
<pre><code>## [1] 0.9215</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">1</span>]</code></pre></div>
<pre><code>## (Intercept) 
##       1.054</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(s[[<span class="st">&quot;L1push&quot;</span>]])</code></pre></div>
<pre><code>## [1] 0.4074</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">2</span>]</code></pre></div>
<pre><code>## (Intercept) 
##      0.2749</code></pre>
<p>Here we can demonstrate that the push vs. pull estimated coefficient, <code>coefs[2]</code>, is a weighted average of the difference of the means for each group. Furthermore, the weighting is determined by the sample size of each group. The math works out simply here because the sample size is equal for the push and pull subgroups within each leg pair. If the sample sizes were not equal for push and pull within each leg pair, the weighting is more complicated but uniquely determined by a formula involving the sample size of each subgroup, the total sample size, and the number of coefficients. This can be worked out from <span class="math inline">\((\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">means &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, mean)
##the sample size of push or pull groups for each leg pair
ns &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, length)[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>)]
(w &lt;-<span class="st"> </span>ns<span class="op">/</span><span class="kw">sum</span>(ns))</code></pre></div>
<pre><code>## L1pull L2pull L3pull L4pull 
## 0.2411 0.1064 0.3688 0.2837</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(w <span class="op">*</span><span class="st"> </span>(means[<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">8</span>)] <span class="op">-</span><span class="st"> </span>means[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">5</span>,<span class="dv">7</span>)]))</code></pre></div>
<pre><code>## [1] -0.779</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">2</span>]</code></pre></div>
<pre><code>## typepush 
##   -0.779</code></pre>
</div>
<div id="contrasting-coefficients" class="section level4">
<h4><span class="header-section-number">5.4.0.7</span> Contrasting coefficients</h4>
<p>Sometimes, the comparison we are interested in is represented directly by a single coefficient in the model, such as the push vs. pull difference, which was <code>coefs[2]</code> above. However, sometimes, we want to make a comparison which is not a single coefficient, but a combination of coefficients, which is called a <em>contrast</em>. To introduce the concept of <em>contrasts</em>, first consider the comparisons which we can read off from the linear model summary:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs</code></pre></div>
<pre><code>## (Intercept)    typepush       legL2       legL3 
##      1.0539     -0.7790      0.1719      0.1605 
##       legL4 
##      0.2813</code></pre>
<p>Here we have the intercept estimate, the push vs. pull estimated effect across all leg pairs, and the estimates for the L2 vs. L1 effect, the L3 vs. L1 effect, and the L4 vs. L1 effect. What if we want to compare two groups and one of those groups is not L1? The solution to this question is to use <em>contrasts</em>.</p>
<p>A <em>contrast</em> is a combination of estimated coefficient: <span class="math inline">\(\mathbf{c^\top} \hat{\boldsymbol{\beta}}\)</span>, where <span class="math inline">\(\mathbf{c}\)</span> is a column vector with as many rows as the number of coefficients in the linear model. If <span class="math inline">\(\mathbf{c}\)</span> has a 0 for one or more of its rows, then the corresponding estimated coefficients in <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> are not involved in the contrast.</p>
<p>If we want to compare leg pairs L3 and L2, this is equivalent to contrasting two coefficients from the linear model because, in this contrast, the comparison to the reference level <em>L1</em> cancels out:</p>
<p><span class="math display">\[ (\mbox{L3} - \mbox{L1}) - (\mbox{L2} - \mbox{L1}) = \mbox{L3} - \mbox{L2 }\]</span></p>
<p>An easy way to make these contrasts of two groups is to use the <code>contrast</code> function from the <em>contrast</em> package. We just need to specify which groups we want to compare. We have to pick one of <em>pull</em> or <em>push</em> types, although the answer will not differ, as we will see below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(contrast) <span class="co">#Available from CRAN</span>
L3vsL2 &lt;-<span class="st"> </span><span class="kw">contrast</span>(fitTL,<span class="kw">list</span>(<span class="dt">leg=</span><span class="st">&quot;L3&quot;</span>,<span class="dt">type=</span><span class="st">&quot;pull&quot;</span>),<span class="kw">list</span>(<span class="dt">leg=</span><span class="st">&quot;L2&quot;</span>,<span class="dt">type=</span><span class="st">&quot;pull&quot;</span>))
L3vsL2</code></pre></div>
<pre><code>## lm model parameter contrast
## 
##  Contrast   S.E.    Lower   Upper     t  df Pr(&gt;|t|)
##  -0.01143 0.0432 -0.09647 0.07361 -0.26 277   0.7915</code></pre>
<p>The first column <code>Contrast</code> gives the L3 vs. L2 estimate from the model we fit above.</p>
<p>We can show that the least squares estimates of a linear combination of coefficients is the same linear combination of the estimates. Therefore, the effect size estimate is just the difference between two estimated coefficients. The contrast vector used by <code>contrast</code> is stored as a variable called <code>X</code> within the resulting object (not to be confused with our original <span class="math inline">\(\mathbf{X}\)</span>, the design matrix).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">4</span>] <span class="op">-</span><span class="st"> </span>coefs[<span class="dv">3</span>]</code></pre></div>
<pre><code>##    legL3 
## -0.01143</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(cT &lt;-<span class="st"> </span>L3vsL2<span class="op">$</span>X)</code></pre></div>
<pre><code>##   (Intercept) typepush legL2 legL3 legL4
## 1           0        0    -1     1     0
## attr(,&quot;assign&quot;)
## [1] 0 1 2 2 2
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$type
## [1] &quot;contr.treatment&quot;
## 
## attr(,&quot;contrasts&quot;)$leg
## [1] &quot;contr.treatment&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cT <span class="op">%*%</span><span class="st"> </span>coefs</code></pre></div>
<pre><code>##       [,1]
## 1 -0.01143</code></pre>
<p>What about the standard error and t-statistic? As before, the t-statistic is the estimate divided by the standard error. The standard error of the contrast estimate is formed by multiplying the contrast vector <span class="math inline">\(\mathbf{c}\)</span> on either side of the estimated covariance matrix, <span class="math inline">\(\hat{\Sigma}\)</span>, our estimate for <span class="math inline">\(\mathrm{var}(\hat{\boldsymbol{\beta}})\)</span>:</p>
<p><span class="math display">\[ \sqrt{\mathbf{c^\top} \hat{\boldsymbol{\Sigma}} \mathbf{c}} \]</span></p>
<p>where we saw the covariance of the coefficients earlier:</p>
<p><span class="math display">\[ 
\boldsymbol{\Sigma} = \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}
\]</span></p>
<p>We estimate <span class="math inline">\(\sigma^2\)</span> with the sample estimate <span class="math inline">\(\hat{\sigma}^2\)</span> described above and obtain:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sigma.hat &lt;-<span class="st"> </span><span class="kw">sum</span>(fitTL<span class="op">$</span>residuals<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>(<span class="kw">nrow</span>(X) <span class="op">-</span><span class="st"> </span><span class="kw">ncol</span>(X)) <span class="op">*</span><span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>X)
<span class="kw">signif</span>(Sigma.hat, <span class="dv">2</span>)</code></pre></div>
<pre><code>##             (Intercept) typepush    legL2    legL3
## (Intercept)     0.00079 -3.1e-04 -0.00064 -0.00064
## typepush       -0.00031  6.2e-04  0.00000  0.00000
## legL2          -0.00064 -6.4e-20  0.00210  0.00064
## legL3          -0.00064 -6.4e-20  0.00064  0.00110
## legL4          -0.00064 -1.2e-19  0.00064  0.00064
##                legL4
## (Intercept) -0.00064
## typepush     0.00000
## legL2        0.00064
## legL3        0.00064
## legL4        0.00120</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(cT <span class="op">%*%</span><span class="st"> </span>Sigma.hat <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(cT))</code></pre></div>
<pre><code>##        1
## 1 0.0432</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">L3vsL2<span class="op">$</span>SE</code></pre></div>
<pre><code>## [1] 0.0432</code></pre>
<p>We would have obtained the same result for a contrast of L3 and L2 had we picked <code>type=&quot;push&quot;</code>. The reason it does not change the contrast is because it leads to addition of the <code>typepush</code> effect on both sides of the difference, which cancels out:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">L3vsL2.equiv &lt;-<span class="st"> </span><span class="kw">contrast</span>(fitTL,<span class="kw">list</span>(<span class="dt">leg=</span><span class="st">&quot;L3&quot;</span>,<span class="dt">type=</span><span class="st">&quot;push&quot;</span>),<span class="kw">list</span>(<span class="dt">leg=</span><span class="st">&quot;L2&quot;</span>,<span class="dt">type=</span><span class="st">&quot;push&quot;</span>))
L3vsL2.equiv<span class="op">$</span>X</code></pre></div>
<pre><code>##   (Intercept) typepush legL2 legL3 legL4
## 1           0        0    -1     1     0
## attr(,&quot;assign&quot;)
## [1] 0 1 2 2 2
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$type
## [1] &quot;contr.treatment&quot;
## 
## attr(,&quot;contrasts&quot;)$leg
## [1] &quot;contr.treatment&quot;</code></pre>
</div>
</div>
<div id="linear-model-with-interactions" class="section level2">
<h2><span class="header-section-number">5.5</span> Linear Model with Interactions</h2>
<p>In the previous linear model, we assumed that the push vs. pull effect was the same for all of the leg pairs (the same orange arrow). You can easily see that this does not capture the trends in the data that well. That is, the tips of the arrows did not line up perfectly with the group averages. For the L1 leg pair, the push vs. pull estimated coefficient was too large, and for the L3 leg pair, the push vs. pull coefficient was somewhat too small.</p>
<p><em>Interaction terms</em> will help us overcome this problem by introducing additional coefficients to compensate for differences in the push vs. pull effect across the 4 groups. As we already have a push vs. pull term in the model, we only need to add three more terms to have the freedom to find leg-pair-specific push vs. pull differences. As we will see, interaction terms are added to the design matrix by multiplying the columns of the design matrix representing existing terms.</p>
<p>We can rebuild our linear model with an interaction between <code>type</code> and <code>leg</code>, by including an extra term in the formula <code>type:leg</code>. The <code>:</code> symbol adds an interaction between the two variables surrounding it. An equivalent way to specify this model is <code>~ type*leg</code>, which will expand to the formula <code>~ type + leg + type:leg</code>, with main effects for <code>type</code>, <code>leg</code> and an interaction term <code>type:leg</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span>type <span class="op">+</span><span class="st"> </span>leg <span class="op">+</span><span class="st"> </span>type<span class="op">:</span>leg, <span class="dt">data=</span>spider)
<span class="kw">colnames</span>(X)</code></pre></div>
<pre><code>## [1] &quot;(Intercept)&quot;    &quot;typepush&quot;       &quot;legL2&quot;         
## [4] &quot;legL3&quot;          &quot;legL4&quot;          &quot;typepush:legL2&quot;
## [7] &quot;typepush:legL3&quot; &quot;typepush:legL4&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(X)</code></pre></div>
<pre><code>##   (Intercept) typepush legL2 legL3 legL4
## 1           1        0     0     0     0
## 2           1        0     0     0     0
## 3           1        0     0     0     0
## 4           1        0     0     0     0
## 5           1        0     0     0     0
## 6           1        0     0     0     0
##   typepush:legL2 typepush:legL3 typepush:legL4
## 1              0              0              0
## 2              0              0              0
## 3              0              0              0
## 4              0              0              0
## 5              0              0              0
## 6              0              0              0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">imagemat</span>(X, <span class="dt">main=</span><span class="st">&quot;Model matrix for linear model with interactions&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/model_matrix_with_interaction_image-1.png" alt="Image of model matrix with interactions." width="672" />
<p class="caption">
(#fig:model_matrix_with_interaction_image)Image of model matrix with interactions.
</p>
</div>
<p>Columns 6-8 (<code>typepush:legL2</code>, <code>typepush:legL3</code>, and <code>typepush:legL4</code>) are the product of the 2nd column (<code>typepush</code>) and columns 3-5 (the three <code>leg</code> columns). Looking at the last column, for example, the <code>typepush:legL4</code> column is adding an extra coefficient <span class="math inline">\(\beta_{\textrm{push,L4}}\)</span> to those samples which are both push samples and leg pair L4 samples. This accounts for a possible difference when the mean of samples in the L4-push group are not at the location which would be predicted by adding the estimated intercept, the estimated push coefficient <code>typepush</code>, and the estimated L4 coefficient <code>legL4</code>.</p>
<p>We can run the linear model using the same code as before:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fitX &lt;-<span class="st"> </span><span class="kw">lm</span>(friction <span class="op">~</span><span class="st"> </span>type <span class="op">+</span><span class="st"> </span>leg <span class="op">+</span><span class="st"> </span>type<span class="op">:</span>leg, <span class="dt">data=</span>spider)
<span class="kw">summary</span>(fitX)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = friction ~ type + leg + type:leg, data = spider)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4638 -0.1074 -0.0111  0.0785  0.7685 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)      0.9215     0.0327   28.21  &lt; 2e-16
## typepush        -0.5141     0.0462  -11.13  &lt; 2e-16
## legL2            0.2239     0.0590    3.79  0.00018
## legL3            0.3524     0.0420    8.39  2.6e-15
## legL4            0.4793     0.0444   10.79  &lt; 2e-16
## typepush:legL2  -0.1039     0.0835   -1.24  0.21441
## typepush:legL3  -0.3838     0.0594   -6.46  4.7e-10
## typepush:legL4  -0.3959     0.0628   -6.30  1.2e-09
##                   
## (Intercept)    ***
## typepush       ***
## legL2          ***
## legL3          ***
## legL4          ***
## typepush:legL2    
## typepush:legL3 ***
## typepush:legL4 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.19 on 274 degrees of freedom
## Multiple R-squared:  0.828,  Adjusted R-squared:  0.824 
## F-statistic:  188 on 7 and 274 DF,  p-value: &lt;2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs &lt;-<span class="st"> </span><span class="kw">coef</span>(fitX)</code></pre></div>
<div id="examining-the-estimated-coefficients-2" class="section level4">
<h4><span class="header-section-number">5.5.0.1</span> Examining the estimated coefficients</h4>
<p>Here is where the plot with arrows really helps us interpret the coefficients. The estimated interaction coefficients (the yellow, brown and silver arrows) allow leg-pair-specific differences in the push vs. pull difference. The orange arrow now represents the estimated push vs. pull difference only for the reference leg pair, which is L1. If an estimated interaction coefficient is large, this means that the push vs. pull difference for that leg pair is very different than the push vs. pull difference in the reference leg pair.</p>
<p>Now, as we have eight terms in the model and eight parameters, you can check that the tips of the arrowheads are exactly equal to the group means (code not shown).</p>
<div class="figure">
<img src="bookdown_files/figure-html/spider_interactions2-1.png" alt="Diagram of the estimated coefficients in the linear model. In the design with interaction terms, the orange arrow now indicates the push vs. pull difference only for the reference group (L1), while three new arrows (yellow, brown and grey) indicate the additional push vs. pull differences in the non-reference groups (L2, L3 and L4) with respect to the reference group." width="672" />
<p class="caption">
(#fig:spider_interactions2)Diagram of the estimated coefficients in the linear model. In the design with interaction terms, the orange arrow now indicates the push vs. pull difference only for the reference group (L1), while three new arrows (yellow, brown and grey) indicate the additional push vs. pull differences in the non-reference groups (L2, L3 and L4) with respect to the reference group.
</p>
</div>
</div>
<div id="contrasts" class="section level4">
<h4><span class="header-section-number">5.5.0.2</span> Contrasts</h4>
<p>Again we will show how to combine estimated coefficients from the model using contrasts. For some simple cases, we can use the contrast package. Suppose we want to know the push vs. pull effect for the L2 leg pair samples. We can see from the arrow plot that this is the orange arrow plus the yellow arrow. We can also specify this comparison with the <code>contrast</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(contrast) ##Available from CRAN
L2push.vs.pull &lt;-<span class="st"> </span><span class="kw">contrast</span>(fitX,
                   <span class="kw">list</span>(<span class="dt">leg=</span><span class="st">&quot;L2&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;push&quot;</span>), 
                   <span class="kw">list</span>(<span class="dt">leg=</span><span class="st">&quot;L2&quot;</span>, <span class="dt">type =</span> <span class="st">&quot;pull&quot;</span>))
L2push.vs.pull</code></pre></div>
<pre><code>## lm model parameter contrast
## 
##  Contrast    S.E.   Lower   Upper     t  df Pr(&gt;|t|)
##    -0.618 0.06954 -0.7549 -0.4811 -8.89 274        0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>coefs[<span class="dv">6</span>] ##we know this is also orange + yellow arrow</code></pre></div>
<pre><code>## typepush 
##   -0.618</code></pre>
</div>
<div id="differences-of-differences" class="section level4">
<h4><span class="header-section-number">5.5.0.3</span> Differences of differences</h4>
<p>The question of whether the push vs. pull difference is <em>different</em> in L2 compared to L1, is answered by a single term in the model: the <code>typepush:legL2</code> estimated coefficient corresponding to the yellow arrow in the plot. A p-value for whether this coefficient is actually equal to zero can be read off from the table printed with <code>summary(fitX)</code> above. Similarly, we can read off the p-values for the differences of differences for L3 vs. L1 and for L4 vs. L1.</p>
<p>Suppose we want to know if the push vs. pull difference is <em>different</em> in L3 compared to L2. By examining the arrows in the diagram above, we can see that the push vs. pull effect for a leg pair other than L1 is the <code>typepush</code> arrow plus the interaction term for that group.</p>
<p>If we work out the math for comparing across two non-reference leg pairs, this is:</p>
<p><span class="math display">\[
(\mbox{typepush} + \mbox{typepush:legL3}) - (\mbox{typepush} + \mbox{typepush:legL2})
\]</span></p>
<p>…which simplifies to:</p>
<p><span class="math display">\[
= \mbox{typepush:legL3} - \mbox{typepush:legL2}
\]</span></p>
<p>We can’t make this contrast using the <code>contrast</code> function shown before, but we can make this comparison using the <code>glht</code> (for “general linear hypothesis test”) function from the <em>multcomp</em> package. We need to form a 1-row matrix which has a -1 for the <code>typepush:legL2</code> coefficient and a +1 for the <code>typepush:legL3</code> coefficient. We provide this matrix to the <code>linfct</code> (linear function) argument, and obtain a summary table for this contrast of estimated interaction coefficients.</p>
<p>Note that there are other ways to perform contrasts using base R, and this is just our preferred way.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(multcomp) ##Available from CRAN
C &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>), <span class="dv">1</span>)
L3vsL2interaction &lt;-<span class="st"> </span><span class="kw">glht</span>(fitX, <span class="dt">linfct=</span>C)
<span class="kw">summary</span>(L3vsL2interaction)</code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = friction ~ type + leg + type:leg, data = spider)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(&gt;|t|)    
## 1 == 0  -0.2799     0.0789   -3.55  0.00046 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">7</span>] <span class="op">-</span><span class="st"> </span>coefs[<span class="dv">6</span>] ##we know this is also brown - yellow</code></pre></div>
<pre><code>## typepush:legL3 
##        -0.2799</code></pre>
</div>
</div>
<div id="analysis-of-variance" class="section level2">
<h2><span class="header-section-number">5.6</span> Analysis of Variance</h2>
<p>Suppose that we want to know if the push vs. pull difference is different across leg pairs in general. We do not want to compare any two leg pairs in particular, but rather we want to know if the three interaction terms which represent differences in the push vs. pull difference across leg pairs are larger than we would expect them to be if the push vs. pull difference was in fact equal across all leg pairs.</p>
<p>Such a question can be answered by an <em>analysis of variance</em>, which is often abbreviated as ANOVA. ANOVA compares the reduction in the sum of squares of the residuals for models of different complexity. The model with eight coefficients is more complex than the model with five coefficients where we assumed the push vs. pull difference was equal across leg pairs. The least complex model would only use a single coefficient, an intercept. Under certain assumptions we can also perform inference that determines the probability of improvements as large as what we observed. Let’s first print the result of an ANOVA in R and then examine the results in detail:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(fitX)</code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: friction
##            Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## type        1   42.8    42.8  1179.7 &lt; 2e-16 ***
## leg         3    2.9     1.0    26.9 3.0e-15 ***
## type:leg    3    2.1     0.7    19.3 2.3e-11 ***
## Residuals 274    9.9     0.0                    
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The first line tells us that adding a variable <code>type</code> (push or pull) to the design is very useful (reduces the sum of squared residuals) compared to a model with only an intercept. We can see that it is useful, because this single coefficient reduces the sum of squares by 42.783. The original sum of squares of the model with just an intercept is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu0 &lt;-<span class="st"> </span><span class="kw">mean</span>(spider<span class="op">$</span>friction)
(initial.ss &lt;-<span class="st"> </span><span class="kw">sum</span>((spider<span class="op">$</span>friction <span class="op">-</span><span class="st"> </span>mu0)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 57.74</code></pre>
<p>Note that this initial sum of squares is just a scaled version of the sample variance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">N &lt;-<span class="st"> </span><span class="kw">nrow</span>(spider)
(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="kw">var</span>(spider<span class="op">$</span>friction)</code></pre></div>
<pre><code>## [1] 57.74</code></pre>
<p>Let’s see exactly how we get this 42.783. We need to calculate the sum of squared residuals for the model with only the type information. We can do this by calculating the residuals, squaring these, summing these within groups and then summing across the groups.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">split</span>(spider<span class="op">$</span>friction, spider<span class="op">$</span>type)
after.type.ss &lt;-<span class="st"> </span><span class="kw">sum</span>( <span class="kw">sapply</span>(s, <span class="cf">function</span>(x) {
  residual &lt;-<span class="st"> </span>x <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(x) 
  <span class="kw">sum</span>(residual<span class="op">^</span><span class="dv">2</span>)
  }) )</code></pre></div>
<p>The reduction in sum of squared residuals from introducing the <code>type</code> coefficient is therefore:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(type.ss &lt;-<span class="st"> </span>initial.ss <span class="op">-</span><span class="st"> </span>after.type.ss)</code></pre></div>
<pre><code>## [1] 42.78</code></pre>
<p>Through <a href="http://en.wikipedia.org/wiki/Partition_of_sums_of_squares#Proof">simple arithmetic</a>, this reduction can be shown to be equivalent to the sum of squared differences between the fitted values for the models with formula <code>~type</code> and <code>~1</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(<span class="kw">sapply</span>(s, length) <span class="op">*</span><span class="st"> </span>(<span class="kw">sapply</span>(s, mean) <span class="op">-</span><span class="st"> </span>mu0)<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 42.78</code></pre>
<p>Keep in mind that the order of terms in the formula, and therefore rows in the ANOVA table, is important: each row considers the reduction in the sum of squared residuals after adding coefficients <em>compared to the model in the previous row</em>.</p>
<p>The other columns in the ANOVA table show the “degrees of freedom” with each row. As the <code>type</code> variable introduced only one term in the model, the <code>Df</code> column has a 1. Because the <code>leg</code> variable introduced three terms in the model (<code>legL2</code>, <code>legL3</code> and <code>legL4</code>), the <code>Df</code> column has a 3.</p>
<p>Finally, there is a column which lists the <em>F value</em>. The F value is the <em>mean of squares</em> for the inclusion of the terms of interest (the sum of squares divided by the degrees of freedom) divided by the mean squared residuals (from the bottom row):</p>
<p><span class="math display">\[ r_i = Y_i - \hat{Y}_i \]</span></p>
<p><span class="math display">\[ \mbox{Mean Sq Residuals} = \frac{1}{N - p} \sum_{i=1}^N r_i^2 \]</span></p>
<p>where <span class="math inline">\(p\)</span> is the number of coefficients in the model (here eight, including the intercept term).</p>
<p>Under the null hypothesis (the true value of the additional coefficient(s) is 0), we have a theoretical result for what the distribution of the F value will be for each row. The assumptions needed for this approximation to hold are similar to those of the t-distribution approximation we described in earlier chapters. We either need a large sample size so that CLT applies or we need the population data to follow a normal approximation.</p>
<p>As an example of how one interprets these p-values, let’s take the last row <code>type:leg</code> which specifies the three interaction coefficients. Under the null hypothesis that the true value for these three additional terms is actually 0, e.g. <span class="math inline">\(\beta_{\textrm{push,L2}} = 0, \beta_{\textrm{push,L3}} = 0, \beta_{\textrm{push,L4}} = 0\)</span>, then we can calculate the chance of seeing such a large F-value for this row of the ANOVA table. Remember that we are only concerned with large values here, because we have a ratio of sum of squares, the F-value can only be positive. The p-value in the last column for the <code>type:leg</code> row can be interpreted as: under the null hypothesis that there are no differences in the push vs. pull difference across leg pair, this is the probability of an estimated interaction coefficient explaining so much of the observed variance. If this p-value is small, we would consider rejecting the null hypothesis that the push vs. pull difference is the same across leg pairs.</p>
<p>The <a href="http://en.wikipedia.org/wiki/F-distribution">F distribution</a> has two parameters: one for the degrees of freedom of the numerator (the terms of interest) and one for the denominator (the residuals). In the case of the interaction coefficients row, this is 3, the number of interaction coefficients divided by 274, the number of samples minus the total number of coefficients.</p>
<div id="a-different-specification-of-the-same-model" class="section level4">
<h4><span class="header-section-number">5.6.0.1</span> A different specification of the same model</h4>
<p>Now we show an alternate specification of the same model, wherein we assume that each combination of type and leg has its own mean value (and so that the push vs. pull effect is not the same for each leg pair). This specification is in some ways simpler, as we will see, but it does not allow us to build the ANOVA table as above, because it does not split interaction coefficients out in the same way.</p>
<p>We start by constructing a factor variable with a level for each unique combination of <code>type</code> and <code>leg</code>. We include a <code>0 +</code> in the formula because we do not want to include an intercept in the model matrix.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">##earlier, we defined the &#39;group&#39; column:
spider<span class="op">$</span>group &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">paste0</span>(spider<span class="op">$</span>leg, spider<span class="op">$</span>type))
X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>group, <span class="dt">data=</span>spider)
<span class="kw">colnames</span>(X)</code></pre></div>
<pre><code>## [1] &quot;groupL1pull&quot; &quot;groupL1push&quot; &quot;groupL2pull&quot;
## [4] &quot;groupL2push&quot; &quot;groupL3pull&quot; &quot;groupL3push&quot;
## [7] &quot;groupL4pull&quot; &quot;groupL4push&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(X)</code></pre></div>
<pre><code>##   groupL1pull groupL1push groupL2pull groupL2push
## 1           1           0           0           0
## 2           1           0           0           0
## 3           1           0           0           0
## 4           1           0           0           0
## 5           1           0           0           0
## 6           1           0           0           0
##   groupL3pull groupL3push groupL4pull groupL4push
## 1           0           0           0           0
## 2           0           0           0           0
## 3           0           0           0           0
## 4           0           0           0           0
## 5           0           0           0           0
## 6           0           0           0           0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">imagemat</span>(X, <span class="dt">main=</span><span class="st">&quot;Model matrix for linear model with group variable&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/matrix_model_image_group_variable-1.png" alt="Image of model matrix for linear model with group variable. This model, also with eight terms, gives a unique fitted value for each combination of type and leg." width="672" />
<p class="caption">
(#fig:matrix_model_image_group_variable)Image of model matrix for linear model with group variable. This model, also with eight terms, gives a unique fitted value for each combination of type and leg.
</p>
</div>
<p>We can run the linear model with the familiar call:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fitG &lt;-<span class="st"> </span><span class="kw">lm</span>(friction <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>group, <span class="dt">data=</span>spider)
<span class="kw">summary</span>(fitG)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = friction ~ 0 + group, data = spider)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4638 -0.1074 -0.0111  0.0785  0.7685 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## groupL1pull   0.9215     0.0327    28.2   &lt;2e-16 ***
## groupL1push   0.4074     0.0327    12.5   &lt;2e-16 ***
## groupL2pull   1.1453     0.0492    23.3   &lt;2e-16 ***
## groupL2push   0.5273     0.0492    10.7   &lt;2e-16 ***
## groupL3pull   1.2738     0.0264    48.2   &lt;2e-16 ***
## groupL3push   0.3760     0.0264    14.2   &lt;2e-16 ***
## groupL4pull   1.4007     0.0301    46.5   &lt;2e-16 ***
## groupL4push   0.4908     0.0301    16.3   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.19 on 274 degrees of freedom
## Multiple R-squared:  0.96,   Adjusted R-squared:  0.959 
## F-statistic:  821 on 8 and 274 DF,  p-value: &lt;2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs &lt;-<span class="st"> </span><span class="kw">coef</span>(fitG)</code></pre></div>
</div>
<div id="examining-the-estimated-coefficients-3" class="section level4">
<h4><span class="header-section-number">5.6.0.2</span> Examining the estimated coefficients</h4>
<p>Now we have eight arrows, one for each group. The arrow tips align directly with the mean of each group:</p>
<div class="figure">
<img src="bookdown_files/figure-html/estimated_group_variables-1.png" alt="Diagram of the estimated coefficients in the linear model, with each term representing the mean of a combination of type and leg." width="672" />
<p class="caption">
(#fig:estimated_group_variables)Diagram of the estimated coefficients in the linear model, with each term representing the mean of a combination of type and leg.
</p>
</div>
</div>
<div id="simple-contrasts-using-the-contrast-package" class="section level4">
<h4><span class="header-section-number">5.6.0.3</span> Simple contrasts using the contrast package</h4>
<p>While we cannot perform an ANOVA with this formulation, we can easily contrast the estimated coefficients for individual groups using the <code>contrast</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">groupL2push.vs.pull &lt;-<span class="st"> </span><span class="kw">contrast</span>(fitG,
                                <span class="kw">list</span>(<span class="dt">group =</span> <span class="st">&quot;L2push&quot;</span>), 
                                <span class="kw">list</span>(<span class="dt">group =</span> <span class="st">&quot;L2pull&quot;</span>))
groupL2push.vs.pull</code></pre></div>
<pre><code>## lm model parameter contrast
## 
##   Contrast    S.E.   Lower   Upper     t  df Pr(&gt;|t|)
## 1   -0.618 0.06954 -0.7549 -0.4811 -8.89 274        0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">coefs[<span class="dv">4</span>] <span class="op">-</span><span class="st"> </span>coefs[<span class="dv">3</span>]</code></pre></div>
<pre><code>## groupL2push 
##      -0.618</code></pre>
</div>
<div id="differences-of-differences-when-there-is-no-intercept" class="section level4">
<h4><span class="header-section-number">5.6.0.4</span> Differences of differences when there is no intercept</h4>
<p>We can also make pair-wise comparisons of the estimated push vs. pull difference across leg pair. For example, if we want to compare the push vs. pull difference in leg pair L3 vs. leg pair L2:</p>
<p><span class="math display">\[ (\mbox{L3push} - \mbox{L3pull}) - (\mbox{L2push} - \mbox{L2pull}) \]</span></p>
<p><span class="math display">\[ = \mbox{L3 push} + \mbox{L2pull} - \mbox{L3pull} - \mbox{L2push} \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">C &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>), <span class="dv">1</span>)
groupL3vsL2interaction &lt;-<span class="st"> </span><span class="kw">glht</span>(fitG, <span class="dt">linfct=</span>C)
<span class="kw">summary</span>(groupL3vsL2interaction)</code></pre></div>
<pre><code>## 
##   Simultaneous Tests for General Linear Hypotheses
## 
## Fit: lm(formula = friction ~ 0 + group, data = spider)
## 
## Linear Hypotheses:
##        Estimate Std. Error t value Pr(&gt;|t|)    
## 1 == 0  -0.2799     0.0789   -3.55  0.00046 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## (Adjusted p values reported -- single-step method)</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(coefs)</code></pre></div>
<pre><code>## [1] &quot;groupL1pull&quot; &quot;groupL1push&quot; &quot;groupL2pull&quot;
## [4] &quot;groupL2push&quot; &quot;groupL3pull&quot; &quot;groupL3push&quot;
## [7] &quot;groupL4pull&quot; &quot;groupL4push&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(coefs[<span class="dv">6</span>] <span class="op">-</span><span class="st"> </span>coefs[<span class="dv">5</span>]) <span class="op">-</span><span class="st"> </span>(coefs[<span class="dv">4</span>] <span class="op">-</span><span class="st"> </span>coefs[<span class="dv">3</span>])</code></pre></div>
<pre><code>## groupL3push 
##     -0.2799</code></pre>
</div>
</div>
<div id="collinearity" class="section level2">
<h2><span class="header-section-number">5.7</span> Collinearity</h2>
<p>If an experiment is designed incorrectly we may not be able to estimate the parameters of interest. Similarly, when analyzing data we may incorrectly decide to use a model that can’t be fit. If we are using linear models then we can detect these problems mathematically by looking for collinearity in the design matrix.</p>
<div id="system-of-equations-example" class="section level4">
<h4><span class="header-section-number">5.7.0.1</span> System of equations example</h4>
<p>The following system of equations:</p>
<p><span class="math display">\[
\begin{align*}
a+c &amp;=1\\
b-c &amp;=1\\
a+b &amp;=2
\end{align*}
\]</span></p>
<p>has more than one solution since there are an infinite number of triplets that satisfy <span class="math inline">\(a=1-c, b=1+c\)</span>. Two examples are <span class="math inline">\(a=1,b=1,c=0\)</span> and <span class="math inline">\(a=0,b=2,c=1\)</span>.</p>
</div>
<div id="matrix-algebra-approach" class="section level4">
<h4><span class="header-section-number">5.7.0.2</span> Matrix algebra approach</h4>
<p>The system of equations above can be written like this:</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
1&amp;0&amp;1\\
0&amp;1&amp;-1\\
1&amp;1&amp;0\\
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}
=
\begin{pmatrix}
1\\
1\\
2
\end{pmatrix}
\]</span></p>
<p>Note that the third column is a linear combination of the first two:</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
1\\
0\\
1
\end{pmatrix}
+
-1 \begin{pmatrix}
0\\
1\\
1
\end{pmatrix}
=
\begin{pmatrix}
1\\
-1\\
0
\end{pmatrix}
\]</span></p>
<p>We say that the third column is collinear with the first 2. This implies that the system of equations can be written like this:</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
1&amp;0&amp;1\\
0&amp;1&amp;-1\\
1&amp;1&amp;0
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c
\end{pmatrix}
=
a
\begin{pmatrix}
1\\
0\\
1
\end{pmatrix}
+
b \begin{pmatrix}
0\\
1\\
1
\end{pmatrix}
+
c
\begin{pmatrix}
1-0\\
0-1\\
1-1
\end{pmatrix}
\]</span></p>
<p><span class="math display">\[
=(a+c)
\begin{pmatrix}
1\\
0\\
1\\
\end{pmatrix}
+
(b-c)
\begin{pmatrix}
0\\
1\\
1\\
\end{pmatrix}
\]</span></p>
<p>The third column does not add a constraint and what we really have are three equations and two unknowns: <span class="math inline">\(a+c\)</span> and <span class="math inline">\(b-c\)</span>. Once we have values for those two quantities, there are an infinity number of triplets that can be used.</p>
</div>
<div id="collinearity-and-least-squares" class="section level4">
<h4><span class="header-section-number">5.7.0.3</span> Collinearity and least squares</h4>
<p>Consider a design matrix <span class="math inline">\(\mathbf{X}\)</span> with two collinear columns. Here we create an extreme example in which one column is the opposite of another:</p>
<p><span class="math display">\[
\mathbf{X} = \begin{pmatrix}
\mathbf{1}&amp;\mathbf{X}_1&amp;\mathbf{X}_2&amp;\mathbf{X}_3\\
\end{pmatrix}
\mbox{ with, say, }
\mathbf{X}_3 = - \mathbf{X}_2
\]</span></p>
<p>This means that we can rewrite the residuals like this:</p>
<p><span class="math display">\[
\mathbf{Y}- \left\{ \mathbf{1}\beta_0 + \mathbf{X}_1\beta_1 + \mathbf{X}_2\beta_2 + \mathbf{X}_3\beta_3\right\}\\ 
= \mathbf{Y}- \left\{ \mathbf{1}\beta_0 + \mathbf{X}_1\beta_1 + \mathbf{X}_2\beta_2 - \mathbf{X}_2\beta_3\right\}\\
= \mathbf{Y}- \left\{\mathbf{1}\beta_0 + \mathbf{X}_1 \beta_1 + \mathbf{X}_2(\beta_2  - \beta_3)\right\}
\]</span></p>
<p>and if <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(\hat{\beta}_2\)</span>, <span class="math inline">\(\hat{\beta}_3\)</span> is a least squares solution, then, for example, <span class="math inline">\(\hat{\beta}_1\)</span>, <span class="math inline">\(\hat{\beta}_2+1\)</span>, <span class="math inline">\(\hat{\beta}_3+1\)</span> is also a solution.</p>
</div>
<div id="confounding-as-an-example" class="section level4">
<h4><span class="header-section-number">5.7.0.4</span> Confounding as an example</h4>
<p>Now we will demonstrate how collinearity helps us determine problems with our design using one of the most common errors made in current experimental design: confounding. To illustrate, let’s use an imagined experiment in which we are interested in the effect of four treatments A, B, C and D. We assign two mice to each treatment. After starting the experiment by giving A and B to female mice, we realize there might be a sex effect. We decide to give C and D to males with hopes of estimating this effect. But can we estimate the sex effect? The described design implies the following design matrix:</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
Sex &amp; A &amp; B &amp; C &amp; D\\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\
\end{pmatrix}
\]</span></p>
<p>Here we can see that sex and treatment are confounded. Specifically, the sex column can be written as a linear combination of the C and D matrices.</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
Sex \\
0\\
0 \\
0 \\
0 \\
1\\
1\\
1 \\
1 \\
\end{pmatrix}
=
\begin{pmatrix}
C \\
0\\
0\\
0\\
0\\
1\\
1\\
0\\
0\\
\end{pmatrix}
+
\begin{pmatrix}
D \\
0\\
0\\
0\\
0\\
0\\
0\\
1\\
1\\
\end{pmatrix}
\]</span></p>
<p>This implies that a unique least squares estimate is not achievable.</p>
</div>
</div>
<div id="rank" class="section level2">
<h2><span class="header-section-number">5.8</span> Rank</h2>
<p>The <em>rank</em> of a matrix columns is the number of columns that are independent of all the others. If the rank is smaller than the number of columns, then the LSE are not unique. In R, we can obtain the rank of matrix with the function <code>qr</code>, which we will describe in more detail in a following section.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sex &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)
A &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)
B &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)
C &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>)
D &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>)
X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>Sex<span class="op">+</span>A<span class="op">+</span>B<span class="op">+</span>C<span class="op">+</span>D<span class="op">-</span><span class="dv">1</span>)
<span class="kw">cat</span>(<span class="st">&quot;ncol=&quot;</span>,<span class="kw">ncol</span>(X),<span class="st">&quot;rank=&quot;</span>, <span class="kw">qr</span>(X)<span class="op">$</span>rank,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>## ncol= 5 rank= 4</code></pre>
<p>Here we will not be able to estimate the effect of sex.</p>
</div>
<div id="removing-confounding" class="section level2">
<h2><span class="header-section-number">5.9</span> Removing Confounding</h2>
<p>This particular experiment could have been designed better. Using the same number of male and female mice, we can easily design an experiment that allows us to compute the sex effect as well as all the treatment effects. Specifically, when we balance sex and treatments, the confounding is removed as demonstrated by the fact that the rank is now the same as the number of columns:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Sex &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>)
A &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)
B &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>)
C &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">0</span>)
D &lt;-<span class="st">   </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>)
X &lt;-<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>Sex<span class="op">+</span>A<span class="op">+</span>B<span class="op">+</span>C<span class="op">+</span>D<span class="op">-</span><span class="dv">1</span>)
<span class="kw">cat</span>(<span class="st">&quot;ncol=&quot;</span>,<span class="kw">ncol</span>(X),<span class="st">&quot;rank=&quot;</span>, <span class="kw">qr</span>(X)<span class="op">$</span>rank,<span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>## ncol= 5 rank= 5</code></pre>
</div>
<div id="the-qr-factorization-advanced" class="section level2">
<h2><span class="header-section-number">5.10</span> The QR Factorization (Advanced)</h2>
<p>We have seen that in order to calculate the LSE, we need to invert a matrix. In previous sections we used the function <code>solve</code>. However, solve is not a stable solution. When coding LSE computation, we use the QR decomposition.</p>
<div id="inverting-mathbfxtop-x" class="section level4">
<h4><span class="header-section-number">5.10.0.1</span> Inverting <span class="math inline">\(\mathbf{X^\top X}\)</span></h4>
<p>Remember that to minimize the RSS:</p>
<p><span class="math display">\[
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})^\top
(\mathbf{Y}-\mathbf{X}\boldsymbol{\beta})
\]</span></p>
<p>We need to solve:</p>
<p><span class="math display">\[
\mathbf{X}^\top \mathbf{X} \boldsymbol{\hat{\beta}} = \mathbf{X}^\top \mathbf{Y}   
\]</span></p>
<p>The solution is:</p>
<p><span class="math display">\[
\boldsymbol{\hat{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1} \mathbf{X}^\top \mathbf{Y}   
\]</span></p>
<p>Thus, we need to compute <span class="math inline">\((\mathbf{X}^\top \mathbf{X})^{-1}\)</span>.</p>
</div>
<div id="solve-is-numerically-unstable" class="section level4">
<h4><span class="header-section-number">5.10.0.2</span> <code>solve</code> is numerically unstable</h4>
<p>To demonstrate what we mean by <em>numerically unstable</em>, we construct an extreme case:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">50</span>;M &lt;-<span class="st"> </span><span class="dv">500</span>
x &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,M,<span class="dt">len=</span>n)
X &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>,x,x<span class="op">^</span><span class="dv">2</span>,x<span class="op">^</span><span class="dv">3</span>)
<span class="kw">colnames</span>(X) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Intercept&quot;</span>,<span class="st">&quot;x&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;x3&quot;</span>)
beta &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>),<span class="dv">4</span>,<span class="dv">1</span>)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
y &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="dv">1</span>)</code></pre></div>
<p>The standard R function for inverse gives an error:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">solve</span>(<span class="kw">crossprod</span>(X)) <span class="op">%*%</span><span class="st"> </span><span class="kw">crossprod</span>(X,y)</code></pre></div>
<p>To see why this happens, look at <span class="math inline">\((\mathbf{X}^\top \mathbf{X})\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">4</span>)
<span class="kw">log10</span>(<span class="kw">crossprod</span>(X))</code></pre></div>
<pre><code>##           Intercept      x     x2     x3
## Intercept     1.699  4.098  6.625  9.203
## x             4.098  6.625  9.203 11.810
## x2            6.625  9.203 11.810 14.434
## x3            9.203 11.810 14.434 17.070</code></pre>
<p>Note the difference of several orders of magnitude. On a digital computer, we have a limited range of numbers. This makes some numbers seem like 0, when we also have to consider very large numbers. This in turn leads to divisions that are practically divisions by 0 errors.</p>
</div>
<div id="the-factorization" class="section level4">
<h4><span class="header-section-number">5.10.0.3</span> The factorization</h4>
<p>The QR factorization is based on a mathematical result that tells us that we can decompose any full rank <span class="math inline">\(N\times p\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> as:</p>
<p><span class="math display">\[
\mathbf{X = QR}
\]</span></p>
<p>with:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q}\)</span> a <span class="math inline">\(N \times p\)</span> matrix with <span class="math inline">\(\mathbf{Q^\top Q=I}\)</span></li>
<li><span class="math inline">\(\mathbf{R}\)</span> a <span class="math inline">\(p \times p\)</span> upper triangular matrix.</li>
</ul>
<p>Upper triangular matrices are very convenient for solving system of equations.</p>
</div>
<div id="example-of-upper-triangular-matrix" class="section level4">
<h4><span class="header-section-number">5.10.0.4</span> Example of upper triangular matrix</h4>
<p>In the example below, the matrix on the left is upper triangular: it only has 0s below the diagonal. This facilitates solving the system of equations greatly:</p>
<p><span class="math display">\[
\,
\begin{pmatrix}
1&amp;2&amp;-1\\
0&amp;1&amp;2\\
0&amp;0&amp;1\\
\end{pmatrix}
\begin{pmatrix}
a\\
b\\
c\\
\end{pmatrix}
=
\begin{pmatrix}
6\\
4\\
1\\
\end{pmatrix}
\]</span></p>
<p>We immediately know that <span class="math inline">\(c=1\)</span>, which implies that <span class="math inline">\(b+2=4\)</span>. This in turn implies <span class="math inline">\(b=2\)</span> and thus <span class="math inline">\(a+4-1=6\)</span> so <span class="math inline">\(a = 3\)</span>. Writing an algorithm to do this is straight-forward for any upper triangular matrix.</p>
</div>
<div id="finding-lse-with-qr" class="section level4">
<h4><span class="header-section-number">5.10.0.5</span> Finding LSE with QR</h4>
<p>If we rewrite the equations of the LSE using <span class="math inline">\(\mathbf{QR}\)</span> instead of <span class="math inline">\(\mathbf{X}\)</span> we have:</p>
<p><span class="math display">\[\mathbf{X}^\top \mathbf{X} \boldsymbol{\beta} = \mathbf{X}^\top \mathbf{Y}\]</span></p>
<p><span class="math display">\[(\mathbf{Q}\mathbf{R})^\top (\mathbf{Q}\mathbf{R}) \boldsymbol{\beta} = (\mathbf{Q}\mathbf{R})^\top \mathbf{Y}\]</span></p>
<p><span class="math display">\[\mathbf{R}^\top (\mathbf{Q}^\top \mathbf{Q}) \mathbf{R} \boldsymbol{\beta} = \mathbf{R}^\top \mathbf{Q}^\top \mathbf{Y}\]</span></p>
<p><span class="math display">\[\mathbf{R}^\top \mathbf{R} \boldsymbol{\beta} = \mathbf{R}^\top \mathbf{Q}^\top \mathbf{Y}\]</span></p>
<p><span class="math display">\[(\mathbf{R}^\top)^{-1} \mathbf{R}^\top \mathbf{R} \boldsymbol{\beta} = (\mathbf{R}^\top)^{-1} \mathbf{R}^\top \mathbf{Q}^\top \mathbf{Y}\]</span></p>
<p><span class="math display">\[\mathbf{R} \boldsymbol{\beta} = \mathbf{Q}^\top \mathbf{Y}\]</span></p>
<p><span class="math inline">\(\mathbf{R}\)</span> being upper triangular makes solving this more stable. Also, because <span class="math inline">\(\mathbf{Q}^\top\mathbf{Q}=\mathbf{I}\)</span> , we know that the columns of <span class="math inline">\(\mathbf{Q}\)</span> are in the same scale which stabilizes the right side.</p>
<p>Now we are ready to find LSE using the QR decomposition. To solve:</p>
<p><span class="math display">\[\mathbf{R} \boldsymbol{\beta} = \mathbf{Q}^\top \mathbf{Y}\]</span></p>
<p>We use <code>backsolve</code> which takes advantage of the upper triangular nature of <span class="math inline">\(\mathbf{R}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">QR &lt;-<span class="st"> </span><span class="kw">qr</span>(X)
Q &lt;-<span class="st"> </span><span class="kw">qr.Q</span>( QR )
R &lt;-<span class="st"> </span><span class="kw">qr.R</span>( QR )
(betahat &lt;-<span class="st"> </span><span class="kw">backsolve</span>(R, <span class="kw">crossprod</span>(Q,y) ) )</code></pre></div>
<pre><code>##        [,1]
## [1,] 0.9038
## [2,] 1.0066
## [3,] 1.0000
## [4,] 1.0000</code></pre>
<p>In practice, we do not need to do any of this due to the built-in <code>solve.qr</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">QR &lt;-<span class="st"> </span><span class="kw">qr</span>(X)
(betahat &lt;-<span class="st"> </span><span class="kw">solve.qr</span>(QR, y))</code></pre></div>
<pre><code>##             [,1]
## Intercept 0.9038
## x         1.0066
## x2        1.0000
## x3        1.0000</code></pre>
</div>
<div id="fitted-values" class="section level4">
<h4><span class="header-section-number">5.10.0.6</span> Fitted values</h4>
<p>This factorization also simplifies the calculation for fitted values:</p>
<p><span class="math display">\[\mathbf{X}\boldsymbol{\hat{\beta}} = 
(\mathbf{QR})\mathbf{R}^{-1}\mathbf{Q}^\top \mathbf{y}= \mathbf{Q}\mathbf{Q}^\top\mathbf{y} \]</span></p>
<p>In R, we simply do the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(x,y)
fitted &lt;-<span class="st"> </span><span class="kw">tcrossprod</span>(Q)<span class="op">%*%</span>y
<span class="kw">lines</span>(x,fitted,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/unnamed-chunk-236-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="standard-errors-1" class="section level4">
<h4><span class="header-section-number">5.10.0.7</span> Standard errors</h4>
<p>To obtain the standard errors of the LSE, we note that:</p>
<p><span class="math display">\[(\mathbf{X^\top X})^{-1} = (\mathbf{R^\top Q^\top QR})^{-1} = (\mathbf{R^\top R})^{-1}\]</span></p>
<p>The function <code>chol2inv</code> is specifically designed to find this inverse. So all we do is the following:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">df &lt;-<span class="st"> </span><span class="kw">length</span>(y) <span class="op">-</span><span class="st"> </span>QR<span class="op">$</span>rank
sigma2 &lt;-<span class="st"> </span><span class="kw">sum</span>((y<span class="op">-</span>fitted)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span>df
varbeta &lt;-<span class="st"> </span>sigma2<span class="op">*</span><span class="kw">chol2inv</span>(<span class="kw">qr.R</span>(QR))
SE &lt;-<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">diag</span>(varbeta))
<span class="kw">cbind</span>(betahat,SE)</code></pre></div>
<pre><code>##                         SE
## Intercept 0.9038 4.508e-01
## x         1.0066 7.858e-03
## x2        1.0000 3.662e-05
## x3        1.0000 4.802e-08</code></pre>
<p>This gives us identical results to the <code>lm</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">lm</span>(y<span class="op">~</span><span class="dv">0</span><span class="op">+</span>X))<span class="op">$</span>coef</code></pre></div>
<pre><code>##            Estimate Std. Error   t value   Pr(&gt;|t|)
## XIntercept   0.9038  4.508e-01 2.005e+00  5.089e-02
## Xx           1.0066  7.858e-03 1.281e+02  2.171e-60
## Xx2          1.0000  3.662e-05 2.731e+04 1.745e-167
## Xx3          1.0000  4.802e-08 2.082e+07 4.559e-300</code></pre>
</div>
</div>
<div id="going-further" class="section level2">
<h2><span class="header-section-number">5.11</span> Going Further</h2>
<p>Linear models can be extended in many directions. Here are some examples of extensions, which you might come across in analyzing data in the life sciences:</p>
<div id="robust-linear-models" class="section level4">
<h4><span class="header-section-number">5.11.0.1</span> Robust linear models</h4>
<p>In calculating the solution and its estimated error in the standard linear model, we minimize the squared errors. This involves a sum of squares from all the data points, which means that a few <em>outlier</em> data points can have a large influence on the solution. In addition, the errors are assumed to have constant variance (called <em>homoskedasticity</em>), which might not always hold true (when this is not true, it is called <em>heteroskedasticity</em>). Therefore, methods have been developed to generate more <em>robust</em> solutions, which behave well in the presence of outliers, or when the distributional assumptions are not met. A number of these are mentioned on the <a href="http://cran.r-project.org/web/views/Robust.html">robust statistics</a> page on the CRAN website. For more background, there is also a <a href="http://en.wikipedia.org/wiki/Robust_regression">Wikipedia article</a> with references.</p>
</div>
<div id="generalized-linear-models" class="section level4">
<h4><span class="header-section-number">5.11.0.2</span> Generalized linear models</h4>
<p>In the standard linear model, we did not make any assumptions about the distribution of <span class="math inline">\(\mathbf{Y}\)</span>, though in some cases we can gain better estimates if we know that <span class="math inline">\(\mathbf{Y}\)</span> is, for example, restricted to non-negative integers <span class="math inline">\(0,1,2,\dots\)</span>, or restricted to the interval <span class="math inline">\([0,1]\)</span>. A framework for analyzing such cases is referred to as <em>generalized linear models</em>, commonly abbreviated as GLMs. The two key components of the GLM are the <em>link function</em> and a probability distribution. The link function <span class="math inline">\(g\)</span> connects our familiar matrix product <span class="math inline">\(\mathbf{X} \boldsymbol{\beta}\)</span> to the <span class="math inline">\(\mathbf{Y}\)</span> values through:</p>
<p><span class="math display">\[ \textrm{E}(\mathbf{Y}) = g^{-1}( \mathbf{X} \boldsymbol{\beta} ) \]</span></p>
<p>R includes the function <code>glm</code> which fits GLMs and uses a familiar form as <code>lm</code>. Additional arguments include <code>family</code>, which can be used to specify the distributional assumption for <span class="math inline">\(\mathbf{Y}\)</span>. Some examples of the use of GLMs are shown at the <a href="http://www.statmethods.net/advstats/glm.html">Quick R</a> website. There are a number of references for GLMs on the <a href="http://en.wikipedia.org/wiki/Generalized_linear_model">Wikipedia page</a>.</p>
</div>
<div id="mixed-effects-linear-models" class="section level4">
<h4><span class="header-section-number">5.11.0.3</span> Mixed effects linear models</h4>
<p>In the standard linear model, we assumed that the matrix <span class="math inline">\(\mathbf{X}\)</span> was <em>fixed</em> and not random. For example, we measured the frictional coefficients for each leg pair, and in the push and pull direction. The fact that an observation had a <span class="math inline">\(1\)</span> for a given column in <span class="math inline">\(\mathbf{X}\)</span> was not random, but dictated by the experimental design. However, in the father and son heights example, we did not fix the values of the fathers’ heights, but observed these (and likely these were measured with some error). A framework for studying the effect of the randomness for various columns in <span class="math inline">\(X\)</span> is referred to as <em>mixed effects</em> models, which implies that some effects are <em>fixed</em> and some effects are <em>random</em>. One of the most popular packages in R for fitting linear mixed effects models is <a href="http://lme4.r-forge.r-project.org/">lme4</a> which has an accompanying paper on <a href="http://arxiv.org/abs/1406.5823">Fitting Linear Mixed-Effects Models using lme4</a>. There is also a <a href="http://en.wikipedia.org/wiki/Mixed_model">Wikipedia page</a> with more references.</p>
</div>
<div id="bayesian-linear-models" class="section level4">
<h4><span class="header-section-number">5.11.0.4</span> Bayesian linear models</h4>
<p>The approach presented here assumed <span class="math inline">\(\boldsymbol{\beta}\)</span> was a fixed (non-random) parameter. We presented methodology that estimates this parameter, along with standard errors that quantify uncertainty, in the estimation process. This is referred to as the <em>frequentist</em> approach. An alternative approach is to assume that <span class="math inline">\(\boldsymbol{\beta}\)</span> is random and its distribution quantifies our prior beliefs about what <span class="math inline">\(\boldsymbol{\beta}\)</span> should be. Once we have observed data, then we update our prior beliefs by computing the conditional distribution, referred to as the <em>posterior</em> distribution, of <span class="math inline">\(\boldsymbol{\beta}\)</span> given the data. This is referred to as the <em>Bayesian</em> approach. For example, once we have computed the posterior distribution of <span class="math inline">\(\boldsymbol{\beta}\)</span> we can report the most likely outcome of an interval that occurs with high probability (credible interval). In addition, many models can be connected together in what is referred to as a <em>hierarchical model</em>. Note that we provide a brief introduction to Bayesian statistics and hierarchical models in a later chapter. A good reference for Bayesian hierarchical models is <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a>, and some software for computing Bayesian linear models can be found on the <a href="http://cran.r-project.org/web/views/Bayesian.html">Bayes</a> page on CRAN. Some well known software for computing Bayesian models are <a href="http://mc-stan.org/">stan</a> and <a href="http://www.mrc-bsu.cam.ac.uk/software/bugs/">BUGS</a>.</p>
</div>
<div id="penalized-linear-models" class="section level4">
<h4><span class="header-section-number">5.11.0.5</span> Penalized linear models</h4>
<p>Note that if we include enough parameters in a model we can achieve a residual sum of squares of 0. Penalized linear models introduce a penalty term to the least square equation we minimize. These penalities are typically of the form, <span class="math inline">\(\lambda \sum_{j=1}^p \|\beta_j\|^k\)</span> and they penalize for large absolute values of <span class="math inline">\(\beta\)</span> as well as large numbers of parameters. The motivation for this extra term is to avoid over-fitting. To use these models, we need to pick <span class="math inline">\(\lambda\)</span> which determines how much we penalize. When <span class="math inline">\(k=2\)</span>, this is referred to as <em>ridge</em> regression, Tikhonov regularization, or L2 regularization. When <span class="math inline">\(k=1\)</span>, this is referred to as <em>LASSO</em> or L1 regularization. A good reference for these penalized linear models is the <a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/">Elements of Statistical Learning</a> textbook, which is available as a free pdf. Some R packages which implement penalized linear models are the <a href="https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/lm.ridge.html">lm.ridge</a> function in the MASS package, the <a href="http://cran.r-project.org/web/packages/lars/index.html">lars</a> package, and the <a href="http://cran.r-project.org/web/packages/glmnet/index.html">glmnet</a> package.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="matrix-algebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-for-high-dimensional-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/yihui/bookdown-chinese/edit/master/05_linear.Rmd",
"text": "编辑"
},
"download": ["bookdown.pdf", "bookdown.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
