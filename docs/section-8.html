<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>生物信息R数据分析</title>
  <meta name="description" content="生物信息R数据分析">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="生物信息R数据分析" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="images/cover.jpg" />
  <meta property="og:description" content="生物信息R数据分析" />
  <meta name="github-repo" content="xie186/HarvardDataScienceForLifeScience_cn" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="生物信息R数据分析" />
  
  <meta name="twitter:description" content="生物信息R数据分析" />
  <meta name="twitter:image" content="images/cover.jpg" />

<meta name="author" content="作者：Rafael A. Irizarry; Mike I. Love 翻译：张三 李四 麻子">


<meta name="date" content="2018-04-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="section-7.html">
<link rel="next" href="basic-machine-learning.html">
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">生物信息R数据分析</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover picture</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a><ul>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#introduction"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="0.1" data-path="acknowledgments.html"><a href="acknowledgments.html#section-0.1"><i class="fa fa-check"></i><b>0.1</b> 简介</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#who-will-find-this-book-useful"><i class="fa fa-check"></i>Who Will Find This Book Useful?</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#what-does-this-book-cover"><i class="fa fa-check"></i>What Does This Book Cover?</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#e8bf99e69cace4b9a6e58c85e590abe4bb80e4b988e58685e5aeb9efbc9f"><i class="fa fa-check"></i>这本书包含什么内容？</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html#how-is-this-book-different"><i class="fa fa-check"></i>How Is This Book Different?</a></li>
<li class="chapter" data-level="0.2" data-path="acknowledgments.html"><a href="acknowledgments.html#section-0.2"><i class="fa fa-check"></i><b>0.2</b> 这本书有什么不同？</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#installing-r"><i class="fa fa-check"></i><b>1.1</b> Installing R</a></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#installing-rstudio"><i class="fa fa-check"></i><b>1.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#learn-r-basics"><i class="fa fa-check"></i><b>1.3</b> Learn R Basics</a></li>
<li class="chapter" data-level="1.4" data-path="getting-started.html"><a href="getting-started.html#installing-packages"><i class="fa fa-check"></i><b>1.4</b> Installing Packages</a></li>
<li class="chapter" data-level="1.5" data-path="getting-started.html"><a href="getting-started.html#importing-data-into-r"><i class="fa fa-check"></i><b>1.5</b> Importing Data into R</a><ul>
<li class="chapter" data-level="1.5.1" data-path="getting-started.html"><a href="getting-started.html#getting-started-exercises"><i class="fa fa-check"></i><b>1.5.1</b> Getting Started Exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="getting-started.html"><a href="getting-started.html#brief-introduction-to-dplyr"><i class="fa fa-check"></i><b>1.6</b> Brief Introduction to <code>dplyr</code></a><ul>
<li class="chapter" data-level="1.6.1" data-path="getting-started.html"><a href="getting-started.html#dplyr-exercises"><i class="fa fa-check"></i><b>1.6.1</b> <code>dplyr</code> exercises</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="getting-started.html"><a href="getting-started.html#mathematical-notation"><i class="fa fa-check"></i><b>1.7</b> Mathematical Notation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>2</b> Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="inference.html"><a href="inference.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="inference.html"><a href="inference.html#random-variables"><i class="fa fa-check"></i><b>2.2</b> Random Variables</a></li>
<li class="chapter" data-level="2.3" data-path="inference.html"><a href="inference.html#the-null-hypothesis"><i class="fa fa-check"></i><b>2.3</b> The Null Hypothesis</a></li>
<li class="chapter" data-level="2.4" data-path="inference.html"><a href="inference.html#distributions"><i class="fa fa-check"></i><b>2.4</b> Distributions</a></li>
<li class="chapter" data-level="2.5" data-path="inference.html"><a href="inference.html#probability-distribution"><i class="fa fa-check"></i><b>2.5</b> Probability Distribution</a></li>
<li class="chapter" data-level="2.6" data-path="inference.html"><a href="inference.html#normal-distribution"><i class="fa fa-check"></i><b>2.6</b> Normal Distribution</a></li>
<li class="chapter" data-level="2.7" data-path="inference.html"><a href="inference.html#populations-samples-and-estimates"><i class="fa fa-check"></i><b>2.7</b> Populations, Samples and Estimates</a><ul>
<li class="chapter" data-level="2.7.1" data-path="inference.html"><a href="inference.html#population-samples-and-estimates-exercises"><i class="fa fa-check"></i><b>2.7.1</b> Population, Samples, and Estimates Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="inference.html"><a href="inference.html#central-limit-theorem-and-t-distribution"><i class="fa fa-check"></i><b>2.8</b> Central Limit Theorem and t-distribution</a></li>
<li class="chapter" data-level="2.9" data-path="inference.html"><a href="inference.html#central-limit-theorem-in-practice"><i class="fa fa-check"></i><b>2.9</b> Central Limit Theorem in Practice</a></li>
<li class="chapter" data-level="2.10" data-path="inference.html"><a href="inference.html#t-tests-in-practice"><i class="fa fa-check"></i><b>2.10</b> t-tests in Practice</a></li>
<li class="chapter" data-level="2.11" data-path="inference.html"><a href="inference.html#the-t-distribution-in-practice"><i class="fa fa-check"></i><b>2.11</b> The t-distribution in Practice</a></li>
<li class="chapter" data-level="2.12" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>2.12</b> Confidence Intervals</a></li>
<li class="chapter" data-level="2.13" data-path="inference.html"><a href="inference.html#power-calculations"><i class="fa fa-check"></i><b>2.13</b> Power Calculations</a></li>
<li class="chapter" data-level="2.14" data-path="inference.html"><a href="inference.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>2.14</b> Monte Carlo Simulation</a></li>
<li class="chapter" data-level="2.15" data-path="inference.html"><a href="inference.html#parametric-simulations-for-the-observations"><i class="fa fa-check"></i><b>2.15</b> Parametric Simulations for the Observations</a></li>
<li class="chapter" data-level="2.16" data-path="inference.html"><a href="inference.html#permutation-tests"><i class="fa fa-check"></i><b>2.16</b> Permutation Tests</a></li>
<li class="chapter" data-level="2.17" data-path="inference.html"><a href="inference.html#association-tests"><i class="fa fa-check"></i><b>2.17</b> Association Tests</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>3</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#quantile-quantile-plots"><i class="fa fa-check"></i><b>3.1</b> Quantile Quantile Plots</a></li>
<li class="chapter" data-level="3.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#boxplots"><i class="fa fa-check"></i><b>3.2</b> Boxplots</a></li>
<li class="chapter" data-level="3.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#scatterplots-and-correlation"><i class="fa fa-check"></i><b>3.3</b> Scatterplots and Correlation</a></li>
<li class="chapter" data-level="3.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>3.4</b> Stratification</a></li>
<li class="chapter" data-level="3.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#bivariate-normal-distribution"><i class="fa fa-check"></i><b>3.5</b> Bivariate Normal Distribution</a></li>
<li class="chapter" data-level="3.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#plots-to-avoid"><i class="fa fa-check"></i><b>3.6</b> Plots to Avoid</a></li>
<li class="chapter" data-level="3.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#misunderstanding-correlation-advanced"><i class="fa fa-check"></i><b>3.7</b> Misunderstanding Correlation (Advanced)</a></li>
<li class="chapter" data-level="3.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#robust-summaries"><i class="fa fa-check"></i><b>3.8</b> Robust Summaries</a></li>
<li class="chapter" data-level="3.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>3.9</b> Wilcoxon Rank Sum Test</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="matrix-algebra.html"><a href="matrix-algebra.html"><i class="fa fa-check"></i><b>4</b> Matrix Algebra</a><ul>
<li class="chapter" data-level="4.1" data-path="matrix-algebra.html"><a href="matrix-algebra.html#motivating-examples"><i class="fa fa-check"></i><b>4.1</b> Motivating Examples</a></li>
<li class="chapter" data-level="4.2" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrix-notation"><i class="fa fa-check"></i><b>4.2</b> Matrix Notation</a></li>
<li class="chapter" data-level="4.3" data-path="matrix-algebra.html"><a href="matrix-algebra.html#solving-systems-of-equations"><i class="fa fa-check"></i><b>4.3</b> Solving Systems of Equations</a></li>
<li class="chapter" data-level="4.4" data-path="matrix-algebra.html"><a href="matrix-algebra.html#vectors-matrices-and-scalars"><i class="fa fa-check"></i><b>4.4</b> Vectors, Matrices, and Scalars</a></li>
<li class="chapter" data-level="4.5" data-path="matrix-algebra.html"><a href="matrix-algebra.html#matrix-operations"><i class="fa fa-check"></i><b>4.5</b> Matrix Operations</a></li>
<li class="chapter" data-level="4.6" data-path="matrix-algebra.html"><a href="matrix-algebra.html#examples"><i class="fa fa-check"></i><b>4.6</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="linear-models-1.html"><a href="linear-models-1.html"><i class="fa fa-check"></i><b>5</b> Linear Models</a><ul>
<li class="chapter" data-level="5.1" data-path="linear-models-1.html"><a href="linear-models-1.html#the-design-matrix"><i class="fa fa-check"></i><b>5.1</b> The Design Matrix</a></li>
<li class="chapter" data-level="5.2" data-path="linear-models-1.html"><a href="linear-models-1.html#the-mathematics-behind-lm"><i class="fa fa-check"></i><b>5.2</b> The Mathematics Behind lm()</a></li>
<li class="chapter" data-level="5.3" data-path="linear-models-1.html"><a href="linear-models-1.html#standard-errors"><i class="fa fa-check"></i><b>5.3</b> Standard Errors</a></li>
<li class="chapter" data-level="5.4" data-path="linear-models-1.html"><a href="linear-models-1.html#interactions-and-contrasts"><i class="fa fa-check"></i><b>5.4</b> Interactions and Contrasts</a></li>
<li class="chapter" data-level="5.5" data-path="linear-models-1.html"><a href="linear-models-1.html#linear-model-with-interactions"><i class="fa fa-check"></i><b>5.5</b> Linear Model with Interactions</a></li>
<li class="chapter" data-level="5.6" data-path="linear-models-1.html"><a href="linear-models-1.html#analysis-of-variance"><i class="fa fa-check"></i><b>5.6</b> Analysis of Variance</a></li>
<li class="chapter" data-level="5.7" data-path="linear-models-1.html"><a href="linear-models-1.html#collinearity"><i class="fa fa-check"></i><b>5.7</b> Collinearity</a></li>
<li class="chapter" data-level="5.8" data-path="linear-models-1.html"><a href="linear-models-1.html#rank"><i class="fa fa-check"></i><b>5.8</b> Rank</a></li>
<li class="chapter" data-level="5.9" data-path="linear-models-1.html"><a href="linear-models-1.html#removing-confounding"><i class="fa fa-check"></i><b>5.9</b> Removing Confounding</a></li>
<li class="chapter" data-level="5.10" data-path="linear-models-1.html"><a href="linear-models-1.html#the-qr-factorization-advanced"><i class="fa fa-check"></i><b>5.10</b> The QR Factorization (Advanced)</a></li>
<li class="chapter" data-level="5.11" data-path="linear-models-1.html"><a href="linear-models-1.html#going-further"><i class="fa fa-check"></i><b>5.11</b> Going Further</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html"><i class="fa fa-check"></i><b>6</b> Inference for High Dimensional Data</a><ul>
<li class="chapter" data-level="6.1" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#inference-in-practice"><i class="fa fa-check"></i><b>6.2</b> Inference in Practice</a></li>
<li class="chapter" data-level="6.3" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#procedures"><i class="fa fa-check"></i><b>6.3</b> Procedures</a></li>
<li class="chapter" data-level="6.4" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#error-rates"><i class="fa fa-check"></i><b>6.4</b> Error Rates</a></li>
<li class="chapter" data-level="6.5" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#the-bonferroni-correction"><i class="fa fa-check"></i><b>6.5</b> The Bonferroni Correction</a></li>
<li class="chapter" data-level="6.6" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#false-discovery-rate"><i class="fa fa-check"></i><b>6.6</b> False Discovery Rate</a></li>
<li class="chapter" data-level="6.7" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#direct-approach-to-fdr-and-q-values-advanced"><i class="fa fa-check"></i><b>6.7</b> Direct Approach to FDR and q-values (Advanced)</a></li>
<li class="chapter" data-level="6.8" data-path="inference-for-high-dimensional-data.html"><a href="inference-for-high-dimensional-data.html#basic-exploratory-data-analysis"><i class="fa fa-check"></i><b>6.8</b> Basic Exploratory Data Analysis</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.html"><a href="section-7.html"><i class="fa fa-check"></i><b>7</b> 统计模型</a><ul>
<li class="chapter" data-level="7.1" data-path="section-7.html"><a href="section-7.html#-the-binomial-distribution"><i class="fa fa-check"></i><b>7.1</b> 二项分布 (The Binomial Distribution)</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.html"><a href="section-7.html#-the-poisson-distribution"><i class="fa fa-check"></i><b>7.2</b> 泊松分布 (The Poisson Distribution)</a></li>
<li class="chapter" data-level="7.3" data-path="section-7.html"><a href="section-7.html#section-7.3"><i class="fa fa-check"></i><b>7.3</b> 最大似然估计</a></li>
<li class="chapter" data-level="7.4" data-path="section-7.html"><a href="section-7.html#section-7.4"><i class="fa fa-check"></i><b>7.4</b> 连续变量的分布</a></li>
<li class="chapter" data-level="7.5" data-path="section-7.html"><a href="section-7.html#section-7.5"><i class="fa fa-check"></i><b>7.5</b> 贝叶斯统计</a></li>
<li class="chapter" data-level="7.6" data-path="section-7.html"><a href="section-7.html#hierarchical-models"><i class="fa fa-check"></i><b>7.6</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.html"><a href="section-8.html"><i class="fa fa-check"></i><b>8</b> 距离和维度降低   </a><ul>
<li class="chapter" data-level="8.1" data-path="section-8.html"><a href="section-8.html#-1"><i class="fa fa-check"></i><b>8.1</b> 简介   </a></li>
<li class="chapter" data-level="8.2" data-path="section-8.html"><a href="section-8.html#euclidean-distance"><i class="fa fa-check"></i><b>8.2</b> Euclidean Distance</a></li>
<li class="chapter" data-level="8.3" data-path="section-8.html"><a href="section-8.html#section-8.3"><i class="fa fa-check"></i><b>8.3</b> 高维数据的距离   </a></li>
<li class="chapter" data-level="8.4" data-path="section-8.html"><a href="section-8.html#distance-exercises"><i class="fa fa-check"></i><b>8.4</b> Distance exercises</a></li>
<li class="chapter" data-level="8.5" data-path="section-8.html"><a href="section-8.html#dimension-reduction-motivation"><i class="fa fa-check"></i><b>8.5</b> Dimension Reduction Motivation</a></li>
<li class="chapter" data-level="8.6" data-path="section-8.html"><a href="section-8.html#singular-value-decomposition"><i class="fa fa-check"></i><b>8.6</b> Singular Value Decomposition</a></li>
<li class="chapter" data-level="8.7" data-path="section-8.html"><a href="section-8.html#projections"><i class="fa fa-check"></i><b>8.7</b> Projections</a></li>
<li class="chapter" data-level="8.8" data-path="section-8.html"><a href="section-8.html#rotations-1"><i class="fa fa-check"></i><b>8.8</b> Rotations</a></li>
<li class="chapter" data-level="8.9" data-path="section-8.html"><a href="section-8.html#multi-dimensional-scaling-plots"><i class="fa fa-check"></i><b>8.9</b> Multi-Dimensional Scaling Plots</a></li>
<li class="chapter" data-level="8.10" data-path="section-8.html"><a href="section-8.html#mds-exercises"><i class="fa fa-check"></i><b>8.10</b> MDS exercises</a></li>
<li class="chapter" data-level="8.11" data-path="section-8.html"><a href="section-8.html#principal-component-analysis"><i class="fa fa-check"></i><b>8.11</b> Principal Component Analysis</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html"><i class="fa fa-check"></i><b>9</b> Basic Machine Learning</a><ul>
<li class="chapter" data-level="9.1" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#clustering"><i class="fa fa-check"></i><b>9.1</b> Clustering</a></li>
<li class="chapter" data-level="9.2" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#conditional-probabilities-and-expectations"><i class="fa fa-check"></i><b>9.2</b> Conditional Probabilities and Expectations</a></li>
<li class="chapter" data-level="9.3" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#smoothing"><i class="fa fa-check"></i><b>9.3</b> Smoothing</a></li>
<li class="chapter" data-level="9.4" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#bin-smoothing"><i class="fa fa-check"></i><b>9.4</b> Bin Smoothing</a></li>
<li class="chapter" data-level="9.5" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#loess"><i class="fa fa-check"></i><b>9.5</b> Loess</a></li>
<li class="chapter" data-level="9.6" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#class-prediction"><i class="fa fa-check"></i><b>9.6</b> Class Prediction</a></li>
<li class="chapter" data-level="9.7" data-path="basic-machine-learning.html"><a href="basic-machine-learning.html#cross-validation"><i class="fa fa-check"></i><b>9.7</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="batch-effects.html"><a href="batch-effects.html"><i class="fa fa-check"></i><b>10</b> Batch Effects</a><ul>
<li class="chapter" data-level="10.1" data-path="batch-effects.html"><a href="batch-effects.html#confounding"><i class="fa fa-check"></i><b>10.1</b> Confounding</a></li>
<li class="chapter" data-level="10.2" data-path="batch-effects.html"><a href="batch-effects.html#confounding-high-throughput-example"><i class="fa fa-check"></i><b>10.2</b> Confounding: High-Throughput Example</a></li>
<li class="chapter" data-level="10.3" data-path="batch-effects.html"><a href="batch-effects.html#discovering-batch-effects-with-eda"><i class="fa fa-check"></i><b>10.3</b> Discovering Batch Effects with EDA</a></li>
<li class="chapter" data-level="10.4" data-path="batch-effects.html"><a href="batch-effects.html#gene-expression-data"><i class="fa fa-check"></i><b>10.4</b> Gene Expression Data</a></li>
<li class="chapter" data-level="10.5" data-path="batch-effects.html"><a href="batch-effects.html#motivation-for-statistical-approaches"><i class="fa fa-check"></i><b>10.5</b> Motivation for Statistical Approaches</a></li>
<li class="chapter" data-level="10.6" data-path="batch-effects.html"><a href="batch-effects.html#adjusting-for-batch-effects-with-linear-models"><i class="fa fa-check"></i><b>10.6</b> Adjusting for Batch Effects with Linear Models</a></li>
<li class="chapter" data-level="10.7" data-path="batch-effects.html"><a href="batch-effects.html#factor-analysis"><i class="fa fa-check"></i><b>10.7</b> Factor Analysis</a></li>
<li class="chapter" data-level="10.8" data-path="batch-effects.html"><a href="batch-effects.html#modeling-batch-effects-with-factor-analysis"><i class="fa fa-check"></i><b>10.8</b> Modeling Batch Effects with Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>参考文献</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">本书由 bookdown 强力驱动</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">生物信息R数据分析</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-8" class="section level1">
<h1><span class="header-section-number">第 8 章</span> 距离和维度降低   </h1>
<div id="-1" class="section level2">
<h2><span class="header-section-number">8.1</span> 简介   </h2>
<p>  距离的概念是相当直观和简单的。例如，当我们将不同的动物划分成不同的类别时，我们通过一定的标准定义了一个距离从而使得我们可以说哪些动物互相之间是比较接近的。</p>
<div class="figure">
<img src="images/handmade/animals.png" alt="Clustering of animals." />
<p class="caption">Clustering of animals.</p>
</div>
<p>  在我们分析的高维度的数据中大多数的分析总是直接或者间接的与距离有关系。很多的聚类和机器学习依赖于利用特征值或者预测变量来定义距离。例如在基因组学和其他高通量领域中，<code>热图</code>被广泛用于数据的展示中。在这个过程中，很明显的我们需要计算距离。</p>
<div class="figure">
<img src="images/handmade/Heatmap.png" alt="Example of heatmap. Image Source: Heatmap, Gaeddal, 01.28.2007 Wikimedia Commons" />
<p class="caption">Example of heatmap. Image Source: Heatmap, Gaeddal, 01.28.2007 Wikimedia Commons</p>
</div>
<p>  在这些图中，所用的数据通常储存在一个矩阵中。首先，通过距离行和列被分别聚类；然后，这些数据在图中用不同的颜色来代表（边注：红和绿是在热图中被广泛使用的一种主体颜色，但是许多色盲的人无法辨别这两种颜色）。这里我们将通过学习基本的数学和计算技巧来理解和产生热图。下面我们将从距离的数学定义开始学习。</p>
</div>
<div id="euclidean-distance" class="section level2">
<h2><span class="header-section-number">8.2</span> Euclidean Distance</h2>
<p>As a review, let’s define the distance between two points, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, on a Cartesian plane.</p>
<p><img src="bookdown_files/figure-html/unnamed-chunk-393-1.png" width="672" /></p>
<p>  <span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>之间的欧氏距离可以很直观的表示为：</p>
<p><span class="math display">\[\sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}\]</span></p>
</div>
<div id="section-8.3" class="section level2">
<h2><span class="header-section-number">8.3</span> 高维数据的距离   </h2>
<p>  这里我们用的数据是22,215个基因在189个样本中的表达量。数据对应的R对象可以通过下面的R代码来下载：</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#library(devtools)</span>
<span class="co">#install_github(&quot;genomicsclass/tissuesGeneExpression&quot;)</span></code></pre></div>
<p>  这个数据代表了八个组织的基因表达量，每个组织又有来自不同个体的生物学重复。</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tissuesGeneExpression)
<span class="kw">data</span>(tissuesGeneExpression)
<span class="kw">dim</span>(e) ##e contains the expression data</code></pre></div>
<pre><code>## [1] 22215   189</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(tissue) ##tissue[i] tells us what tissue is represented by e[,i]</code></pre></div>
<pre><code>## tissue
##  cerebellum       colon endometrium hippocampus 
##          38          34          15          31 
##      kidney       liver    placenta 
##          39          26           6</code></pre>
<p>  在这数据中，我们对不同样本之间的距离比较感兴趣。我们可能也会对不同样本中有__相似表达模式__的基因比较感兴趣。</p>
<p>   为了定义距离，我们需要知道这个数据中点的定义是什么，因为数学上定义的距离是通过计算点和点之间的距离得到的。然而在高维数据中，这些点并不是在直角平面中的点。他们是在更高维度中的点。例如，<span class="math inline">\(i\)</span>样本是一个有着22,215维度的点，<span class="math inline">\((Y_{1,i},\dots,Y_{22215,i})^\top\)</span>。而基因<span class="math inline">\(g\)</span>是一个有着189个维度的点<span class="math inline">\((Y_{g,1},\dots,Y_{g,189})^\top\)</span>。</p>
<p>   在我们定义了点之后，我们可以像计算二维平面中距离一样来计算欧式距离。例如，<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>样本之间的距离可以表示为：</p>
<p><span class="math display">\[
\mbox{dist}(i,j) = \sqrt{ \sum_{g=1}^{22215} (Y_{g,i}-Y_{g,j })^2 }
\]</span></p>
<p>  同时，两个基因<span class="math inline">\(h\)</span>和<span class="math inline">\(g\)</span>可以表示为：</p>
<p><span class="math display">\[
\mbox{dist}(h,g) = \sqrt{ \sum_{i=1}^{189} (Y_{h,i}-Y_{g,i})^2 }
\]</span></p>
<p>  我们需要指出的是，在实践中，通常在计算基因和基因之间的距离之前，通常需要对每个基因的数据进行标准化，例如Z分数(Z-score)转化。这是因为不同基因之间整体的差异往往不是有生物学的因素造成的，往往这些差异是有技术因素造成的。如果想了解更多的细节，可以参考<a href="http://master.bioconductor.org/help/course-materials/2002/Summer02Course/Distance/distance.pdf">这个报告</a>。</p>
<div id="distance-with-matrix-algebra" class="section level4">
<h4><span class="header-section-number">8.3.0.1</span> Distance with matrix algebra</h4>
<p>The distance between samples <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> can be written as</p>
<p><span class="math display">\[ \mbox{dist}(i,j) = (\mathbf{Y}_i - \mathbf{Y}_j)^\top(\mathbf{Y}_i - \mathbf{Y}_j)\]</span></p>
<p>with <span class="math inline">\(\mathbf{Y}_i\)</span> and <span class="math inline">\(\mathbf{Y}_j\)</span> columns <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>. This result can be very convenient in practice as computations can be made much faster using matrix multiplication.</p>
</div>
<div id="examples-1" class="section level4">
<h4><span class="header-section-number">8.3.0.2</span> Examples</h4>
<p>We can now use the formulas above to compute distance. Let’s compute distance between samples 1 and 2, both kidneys, and then to sample 87, a colon.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>e[,<span class="dv">1</span>]
y &lt;-<span class="st"> </span>e[,<span class="dv">2</span>]
z &lt;-<span class="st"> </span>e[,<span class="dv">87</span>]
<span class="kw">sqrt</span>(<span class="kw">sum</span>((x<span class="op">-</span>y)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 85.85</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">sum</span>((x<span class="op">-</span>z)<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## [1] 122.9</code></pre>
<p>As expected, the kidneys are closer to each other. A faster way to compute this is using matrix algebra:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>( <span class="kw">crossprod</span>(x<span class="op">-</span>y) )</code></pre></div>
<pre><code>##       [,1]
## [1,] 85.85</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>( <span class="kw">crossprod</span>(x<span class="op">-</span>z) )</code></pre></div>
<pre><code>##       [,1]
## [1,] 122.9</code></pre>
<p>Now to compute all the distances at once, we have the function <code>dist</code>. Because it computes the distance between each row, and here we are interested in the distance between samples, we transpose the matrix</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(e))
<span class="kw">class</span>(d)</code></pre></div>
<pre><code>## [1] &quot;dist&quot;</code></pre>
<p>Note that this produces an object of class <code>dist</code> and, to access the entries using row and column indices, we need to coerce it into a matrix:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.matrix</span>(d)[<span class="dv">1</span>,<span class="dv">2</span>]</code></pre></div>
<pre><code>## [1] 85.85</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.matrix</span>(d)[<span class="dv">1</span>,<span class="dv">87</span>]</code></pre></div>
<pre><code>## [1] 122.9</code></pre>
<p>It is important to remember that if we run <code>dist</code> on <code>e</code>, it will compute all pairwise distances between genes. This will try to create a <span class="math inline">\(22215 \times 22215\)</span> matrix that may crash your R sessions.</p>
</div>
</div>
<div id="distance-exercises" class="section level2">
<h2><span class="header-section-number">8.4</span> Distance exercises</h2>
<p><strong>Exercises</strong></p>
<p>If you have not done so already, install the data package <code>tissueGeneExpression</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(devtools)
<span class="kw">install_github</span>(<span class="st">&quot;genomicsclass/tissuesGeneExpression&quot;</span>)</code></pre></div>
<p>The data represents RNA expression levels for eight tissues, each with several <em>biological replictes</em>. We call samples that we consider to be from the same population, such as liver tissue from different individuals, <em>biological replicates</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tissuesGeneExpression)
<span class="kw">data</span>(tissuesGeneExpression)
<span class="kw">head</span>(e)
<span class="kw">head</span>(tissue)</code></pre></div>
<ol style="list-style-type: decimal">
<li><p>How many biological replicates for hippocampus?</p></li>
<li><p>What is the distance between samples 3 and 45?</p></li>
<li><p>What is the distance between gene <code>210486_at</code> and <code>200805_at</code></p></li>
<li><p>If I run the command (don’t run it!):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">dist</span>(e) )</code></pre></div>
<p>how many cells (number of rows times number of columns) will this matrix have?</p></li>
<li><p>Compute the distance between all pair of samples:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d =<span class="st"> </span><span class="kw">dist</span>( <span class="kw">t</span>(e) )</code></pre></div>
<p>Read the help file for <code>dist</code>.</p>
<p>How many distances are stored in <code>d</code>? Hint: What is the length of d?</p></li>
<li>Why is the answer to exercise 5 not <code>ncol(e)^2</code>?
<ul>
<li><ol style="list-style-type: upper-alpha">
<li>R made a mistake there.</li>
</ol></li>
<li><ol start="2" style="list-style-type: upper-alpha">
<li>Distances of 0 are left out.</li>
</ol></li>
<li><ol start="3" style="list-style-type: upper-alpha">
<li>Because we take advantage of symmetry: only lower triangular matrix is stored thus only <code>ncol(e)*(ncol(e)-1)/2</code> values.</li>
</ol></li>
<li><ol start="4" style="list-style-type: upper-alpha">
<li>Because it is equal<code>nrow(e)^2</code></li>
</ol></li>
</ul></li>
</ol>
</div>
<div id="dimension-reduction-motivation" class="section level2">
<h2><span class="header-section-number">8.5</span> Dimension Reduction Motivation</h2>
<p>Visualizing data is one of the most, if not the most, important step in the analysis of high-throughput data. The right visualization method may reveal problems with the experimental data that can render the results from a standard analysis, although typically appropriate, completely useless.</p>
<p>We have shown methods for visualizing global properties of the columns or rows, but plots that reveal relationships between columns or between rows are more complicated due to the high dimensionality of data. For example, to compare each of the 189 samples to each other, we would have to create, for example, 17,766 MA-plots. Creating one single scatterplot of the data is impossible since points are very high dimensional.</p>
<p>We will describe powerful techniques for exploratory data analysis based on <em>dimension reduction</em>. The general idea is to reduce the dataset to have fewer dimensions, yet approximately preserve important properties, such as the distance between samples. If we are able to reduce down to, say, two dimensions, we can then easily make plots. The technique behind it all, the singular value decomposition (SVD), is also useful in other contexts. Before introducing the rather complicated mathematics behind the SVD, we will motivate the ideas behind it with a simple example.</p>
<div id="example-reducing-two-dimensions-to-one" class="section level4">
<h4><span class="header-section-number">8.5.0.1</span> Example: Reducing two dimensions to one</h4>
<p>We consider an example with twin heights. Here we simulate 100 two dimensional points that represent the number of standard deviations each individual is from the mean height. Each point is a pair of twins:</p>
<div class="figure">
<img src="bookdown_files/figure-html/simulate_twin_heights-1.png" alt="Simulated twin pair heights." width="672" />
<p class="caption">
(#fig:simulate_twin_heights)Simulated twin pair heights.
</p>
</div>
<p>To help with the illustration, think of this as high-throughput gene expression data with the twin pairs representing the <span class="math inline">\(N\)</span> samples and the two heights representing gene expression from two genes.</p>
<p>We are interested in the distance between any two samples. We can compute this using <code>dist</code>. For example, here is the distance between the two orange points in the figure above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d=<span class="kw">dist</span>(<span class="kw">t</span>(y))
<span class="kw">as.matrix</span>(d)[<span class="dv">1</span>,<span class="dv">2</span>]</code></pre></div>
<pre><code>## [1] 1.141</code></pre>
<p>What if making two dimensional plots was too complex and we were only able to make 1 dimensional plots. Can we, for example, reduce the data to a one dimensional matrix that preserves distances between points?</p>
<p>If we look back at the plot, and visualize a line between any pair of points, the length of this line is the distance between the two points. These lines tend to go along the direction of the diagonal. We have seen before that we can “rotate” the plot so that the diagonal is in the x-axis by making a MA-plot instead:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">z1 =<span class="st"> </span>(y[<span class="dv">1</span>,]<span class="op">+</span>y[<span class="dv">2</span>,])<span class="op">/</span><span class="dv">2</span> <span class="co">#the sum </span>
z2 =<span class="st"> </span>(y[<span class="dv">1</span>,]<span class="op">-</span>y[<span class="dv">2</span>,])   <span class="co">#the difference</span>

z =<span class="st"> </span><span class="kw">rbind</span>( z1, z2) <span class="co">#matrix now same dimensions as y</span>

thelim &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>)
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)

<span class="kw">plot</span>(y[<span class="dv">1</span>,],y[<span class="dv">2</span>,],<span class="dt">xlab=</span><span class="st">&quot;Twin 1 (standardized height)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Twin 2 (standardized height)&quot;</span>,
     <span class="dt">xlim=</span>thelim,<span class="dt">ylim=</span>thelim)
<span class="kw">points</span>(y[<span class="dv">1</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],y[<span class="dv">2</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">16</span>)

<span class="kw">plot</span>(z[<span class="dv">1</span>,],z[<span class="dv">2</span>,],<span class="dt">xlim=</span>thelim,<span class="dt">ylim=</span>thelim,<span class="dt">xlab=</span><span class="st">&quot;Average height&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Difference in height&quot;</span>)
<span class="kw">points</span>(z[<span class="dv">1</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],z[<span class="dv">2</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">16</span>)</code></pre></div>
<div class="figure"><span id="fig:rotation"></span>
<img src="bookdown_files/figure-html/rotation-1.png" alt="Twin height scatterplot (left) and MA-plot (right)." width="1008" />
<p class="caption">
图 8.1: Twin height scatterplot (left) and MA-plot (right).
</p>
</div>
<p>Later, we will start using linear algebra to represent transformation of the data such as this. Here we can see that to get <code>z</code> we multiplied <code>y</code> by the matrix:</p>
<p><span class="math display">\[
A = \,
\begin{pmatrix}
1/2&amp;1/2\\
1&amp;-1\\
\end{pmatrix}
\implies
z = A y
\]</span></p>
<p>Remember that we can transform back by simply multiplying by <span class="math inline">\(A^{-1}\)</span> as follows:</p>
<p><span class="math display">\[
A^{-1} = \,
\begin{pmatrix}
1&amp;1/2\\
1&amp;-1/2\\
\end{pmatrix}
\implies
y = A^{-1} z
\]</span></p>
</div>
<div id="rotations" class="section level4">
<h4><span class="header-section-number">8.5.0.2</span> Rotations</h4>
<p>In the plot above, the distance between the two orange points remains roughly the same, relative to the distance between other points. This is true for all pairs of points. A simple re-scaling of the transformation we performed above will actually make the distances exactly the same. What we will do is multiply by a scalar so that the standard deviations of each point is preserved. If you think of the columns of <code>y</code> as independent random variables with standard deviation <span class="math inline">\(\sigma\)</span>, then note that the standard deviations of <span class="math inline">\(M\)</span> and <span class="math inline">\(A\)</span> are:</p>
<p><span class="math display">\[
\mbox{sd}[ Z_1 ] = \mbox{sd}[ (Y_1 + Y_2) / 2 ] = \frac{1}{\sqrt{2}} \sigma \mbox{ and } \mbox{sd}[ Z_2] = \mbox{sd}[ Y_1 - Y_2  ] = {\sqrt{2}} \sigma 
\]</span></p>
<p>This implies that if we change the transformation above to:</p>
<p><span class="math display">\[
A = \frac{1}{\sqrt{2}}
\begin{pmatrix}
1&amp;1\\
1&amp;-1\\
\end{pmatrix}
\]</span></p>
<p>then the SD of the columns of <span class="math inline">\(Y\)</span> are the same as the variance of the columns <span class="math inline">\(Z\)</span>. Also, notice that <span class="math inline">\(A^{-1}A=I\)</span>. We call matrices with these properties <em>orthogonal</em> and it guarantees the SD-preserving properties described above. The distances are now exactly preserved:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">A &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span>)<span class="op">*</span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>),<span class="dv">2</span>,<span class="dv">2</span>)
z &lt;-<span class="st"> </span>A<span class="op">%*%</span>y
d &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(y))
d2 &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(z))
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(<span class="kw">as.numeric</span>(d),<span class="kw">as.numeric</span>(d2)) <span class="co">#as.numeric turns distances into long vector</span>
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">col=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/rotation_preserves_dist-1.png" alt="Distance computed from original data and after rotation is the same." width="672" />
<p class="caption">
(#fig:rotation_preserves_dist)Distance computed from original data and after rotation is the same.
</p>
</div>
<p>We call this particular transformation a <em>rotation</em> of <code>y</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)

thelim &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>)
<span class="kw">plot</span>(y[<span class="dv">1</span>,],y[<span class="dv">2</span>,],<span class="dt">xlab=</span><span class="st">&quot;Twin 1 (standardized height)&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Twin 2 (standardized height)&quot;</span>,
     <span class="dt">xlim=</span>thelim,<span class="dt">ylim=</span>thelim)
<span class="kw">points</span>(y[<span class="dv">1</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],y[<span class="dv">2</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">16</span>)

<span class="kw">plot</span>(z[<span class="dv">1</span>,],z[<span class="dv">2</span>,],<span class="dt">xlim=</span>thelim,<span class="dt">ylim=</span>thelim,<span class="dt">xlab=</span><span class="st">&quot;Average height&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Difference in height&quot;</span>)
<span class="kw">points</span>(z[<span class="dv">1</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],z[<span class="dv">2</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>],<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">16</span>)</code></pre></div>
<div class="figure"><span id="fig:rotation2"></span>
<img src="bookdown_files/figure-html/rotation2-1.png" alt="Twin height scatterplot (left) and after rotation (right)." width="1008" />
<p class="caption">
图 8.2: Twin height scatterplot (left) and after rotation (right).
</p>
</div>
<p>The reason we applied this transformation in the first place was because we noticed that to compute the distances between points, we followed a direction along the diagonal in the original plot, which after the rotation falls on the horizontal, or the first dimension of <code>z</code>. So this rotation actually achieves what we originally wanted: we can preserve the distances between points with just one dimension. Let’s remove the second dimension of <code>z</code> and recompute distances:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d3 =<span class="st"> </span><span class="kw">dist</span>(z[<span class="dv">1</span>,]) ##distance computed using just first dimension
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(<span class="kw">as.numeric</span>(d),<span class="kw">as.numeric</span>(d3)) 
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/approx_dist-1.png" alt="Distance computed with just one dimension after rotation versus actual distance." width="672" />
<p class="caption">
(#fig:approx_dist)Distance computed with just one dimension after rotation versus actual distance.
</p>
</div>
<p>The distance computed with just the one dimension provides a very good approximation to the actual distance and a very useful dimension reduction: from 2 dimensions to 1. This first dimension of the transformed data is actually the first <em>principal component</em>. This idea motivates the use of principal component analysis (PCA) and the singular value decomposition (SVD) to achieve dimension reduction more generally.</p>
</div>
<div id="important-note-on-a-difference-to-other-explanations" class="section level4">
<h4><span class="header-section-number">8.5.0.3</span> Important note on a difference to other explanations</h4>
<p>If you search the web for descriptions of PCA, you will notice a difference in notation to how we describe it here. This mainly stems from the fact that it is more common to have rows represent units. Hence, in the example shown here, <span class="math inline">\(Y\)</span> would be transposed to be an <span class="math inline">\(N \times 2\)</span> matrix. In statistics this is also the most common way to represent the data: individuals in the rows. However, for practical reasons, in genomics it is more common to represent units in the columns. For example, genes are rows and samples are columns. For this reason, in this book we explain PCA and all the math that goes with it in a slightly different way than it is usually done. As a result, many of the explanations you find for PCA start out with the sample covariance matrix usually denoted with <span class="math inline">\(\mathbf{X}^\top\mathbf{X}\)</span> and having cells representing covariance between two units. Yet for this to be the case, we need the rows of <span class="math inline">\(\mathbf{X}\)</span> to represents units. So in our notation above, you would have to compute, after scaling, <span class="math inline">\(\mathbf{Y}\mathbf{Y}^\top\)</span> instead.</p>
<p>Basically, if you want our explanations to match others you have to transpose the matrices we show here.</p>
</div>
</div>
<div id="singular-value-decomposition" class="section level2">
<h2><span class="header-section-number">8.6</span> Singular Value Decomposition</h2>
<p>In the previous section, we motivated dimension reduction and showed a transformation that permitted us to approximate the distance between two dimensional points with just one dimension. The singular value decomposition (SVD) is a generalization of the algorithm we used in the motivational section. As in the example, the SVD provides a transformation of the original data. This transformation has some very useful properties.</p>
<p>The main result SVD provides is that we can write an <span class="math inline">\(m \times n\)</span>, matrix <span class="math inline">\(\mathbf{Y}\)</span> as</p>
<p><span class="math display">\[\mathbf{U}^\top\mathbf{Y} = \mathbf{DV}^\top\]</span></p>
<p>With:</p>
<ul>
<li><span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(m \times p\)</span> orthogonal matrix</li>
<li><span class="math inline">\(\mathbf{V}\)</span> is an <span class="math inline">\(p \times p\)</span> orthogonal matrix</li>
<li><span class="math inline">\(\mathbf{D}\)</span> is an <span class="math inline">\(n \times p\)</span> diagonal matrix</li>
</ul>
<p>with <span class="math inline">\(p=\mbox{min}(m,n)\)</span>. <span class="math inline">\(\mathbf{U}^\top\)</span> provides a rotation of our data <span class="math inline">\(\mathbf{Y}\)</span> that turns out to be very useful because the variability (sum of squares to be precise) of the columns of <span class="math inline">\(\mathbf{U}^\top \mathbf{Y}=\mathbf{VD}\)</span> are decreasing. Because <span class="math inline">\(\mathbf{U}\)</span> is orthogonal, we can write the SVD like this:</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{UDV}^\top\]</span></p>
<p>In fact, this formula is much more commonly used. We can also write the transformation like this:</p>
<p><span class="math display">\[\mathbf{YV} = \mathbf{UD}\]</span></p>
<p>This transformation of <span class="math inline">\(Y\)</span> also results in a matrix with column of decreasing sum of squares.</p>
<p>Applying the SVD to the motivating example we have:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">library</span>(MASS)
n &lt;-<span class="st"> </span><span class="dv">100</span>
y &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">mvrnorm</span>(n,<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="fl">0.95</span>,<span class="fl">0.95</span>,<span class="dv">1</span>),<span class="dv">2</span>,<span class="dv">2</span>)))
s &lt;-<span class="st"> </span><span class="kw">svd</span>(y)</code></pre></div>
<p>We can immediately see that applying the SVD results in a transformation very similar to the one we used in the motivating example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>s<span class="op">$</span>u , <span class="dv">3</span>)</code></pre></div>
<pre><code>##        [,1]   [,2]
## [1,] -0.982 -1.017
## [2,] -1.017  0.982</code></pre>
<p>The plot we showed after the rotation was showing what we call the <em>principal components</em>: the second plotted against the first. To obtain the principal components from the SVD, we simply need the columns of the rotation <span class="math inline">\(\mathbf{U}^\top\mathbf{Y}\)</span> :</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">PC1 =<span class="st"> </span>s<span class="op">$</span>d[<span class="dv">1</span>]<span class="op">*</span>s<span class="op">$</span>v[,<span class="dv">1</span>]
PC2 =<span class="st"> </span>s<span class="op">$</span>d[<span class="dv">2</span>]<span class="op">*</span>s<span class="op">$</span>v[,<span class="dv">2</span>]
<span class="kw">plot</span>(PC1,PC2,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>),<span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>))</code></pre></div>
<div class="figure"><span id="fig:PCAplot"></span>
<img src="bookdown_files/figure-html/PCAplot-1.png" alt="Second PC plotted against first PC for the twins height data." width="672" />
<p class="caption">
图 8.3: Second PC plotted against first PC for the twins height data.
</p>
</div>
<div id="how-is-this-useful" class="section level4">
<h4><span class="header-section-number">8.6.0.1</span> How is this useful?</h4>
<p>It is not immediately obvious how incredibly useful the SVD can be, so let’s consider some examples. In this example, we will greatly reduce the dimension of <span class="math inline">\(V\)</span> and still be able to reconstruct <span class="math inline">\(Y\)</span>.</p>
<p>Let’s compute the SVD on the gene expression table we have been working with. We will take a subset of 100 genes so that computations are faster.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tissuesGeneExpression)
<span class="kw">data</span>(tissuesGeneExpression)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
ind &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">nrow</span>(e),<span class="dv">500</span>) 
Y &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">apply</span>(e[ind,],<span class="dv">1</span>,scale)) <span class="co">#standardize data for illustration</span></code></pre></div>
<p>The <code>svd</code> command returns the three matrices (only the diagonal entries are returned for <span class="math inline">\(D\)</span>)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">svd</span>(Y)
U &lt;-<span class="st"> </span>s<span class="op">$</span>u
V &lt;-<span class="st"> </span>s<span class="op">$</span>v
D &lt;-<span class="st"> </span><span class="kw">diag</span>(s<span class="op">$</span>d) ##turn it into a matrix</code></pre></div>
<p>First note that we can in fact reconstruct y:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Yhat &lt;-<span class="st"> </span>U <span class="op">%*%</span><span class="st"> </span>D <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V)
resid &lt;-<span class="st"> </span>Y <span class="op">-</span><span class="st"> </span>Yhat
<span class="kw">max</span>(<span class="kw">abs</span>(resid))</code></pre></div>
<pre><code>## [1] 2.563e-14</code></pre>
<p>If we look at the sum of squares of <span class="math inline">\(\mathbf{UD}\)</span>, we see that the last few are quite close to 0 (perhaps we have some replicated columns).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(s<span class="op">$</span>d)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/D_entries-1.png" alt="Entries of the diagonal of D for gene expression data." width="672" />
<p class="caption">
(#fig:D_entries)Entries of the diagonal of D for gene expression data.
</p>
</div>
<p>This implies that the last columns of <code>V</code> have a very small effect on the reconstruction of <code>Y</code>. To see this, consider the extreme example in which the last entry of <span class="math inline">\(V\)</span> is 0. In this case the last column of <span class="math inline">\(V\)</span> is not needed at all. Because of the way the SVD is created, the columns of <span class="math inline">\(V\)</span> have less and less influence on the reconstruction of <span class="math inline">\(Y\)</span>. You commonly see this described as “explaining less variance”. This implies that for a large matrix, by the time you get to the last columns, it is possible that there is not much left to “explain” As an example, we will look at what happens if we remove the four last columns:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="kw">ncol</span>(U)<span class="op">-</span><span class="dv">4</span>
Yhat &lt;-<span class="st"> </span>U[,<span class="dv">1</span><span class="op">:</span>k] <span class="op">%*%</span><span class="st"> </span>D[<span class="dv">1</span><span class="op">:</span>k,<span class="dv">1</span><span class="op">:</span>k] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V[,<span class="dv">1</span><span class="op">:</span>k])
resid &lt;-<span class="st"> </span>Y <span class="op">-</span><span class="st"> </span>Yhat 
<span class="kw">max</span>(<span class="kw">abs</span>(resid))</code></pre></div>
<pre><code>## [1] 1.577e-14</code></pre>
<p>The largest residual is practically 0, meaning that we <code>Yhat</code> is practically the same as <code>Y</code>, yet we need 4 fewer dimensions to transmit the information.</p>
<p>By looking at <span class="math inline">\(d\)</span>, we can see that, in this particular dataset, we can obtain a good approximation keeping only 94 columns. The following plots are useful for seeing how much of the variability is explained by each column:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">100</span>,<span class="dt">ylab=</span><span class="st">&quot;Percent variability explained&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/percent_var_explained-1.png" alt="Percent variance explained by each principal component of gene expression data." width="672" />
<p class="caption">
(#fig:percent_var_explained)Percent variance explained by each principal component of gene expression data.
</p>
</div>
<p>We can also make a cumulative plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">cumsum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>)<span class="op">*</span><span class="dv">100</span>,<span class="dt">ylab=</span><span class="st">&quot;Percent variability explained&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">100</span>),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/cum_variance_explained-1.png" alt="Cumulative variance explained by principal components of gene expression data." width="672" />
<p class="caption">
(#fig:cum_variance_explained)Cumulative variance explained by principal components of gene expression data.
</p>
</div>
<p>Although we start with 189 dimensions, we can approximate <span class="math inline">\(Y\)</span> with just 95:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">k &lt;-<span class="st"> </span><span class="dv">95</span> ##out a possible 189
Yhat &lt;-<span class="st"> </span>U[,<span class="dv">1</span><span class="op">:</span>k] <span class="op">%*%</span><span class="st"> </span>D[<span class="dv">1</span><span class="op">:</span>k,<span class="dv">1</span><span class="op">:</span>k] <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V[,<span class="dv">1</span><span class="op">:</span>k])
resid &lt;-<span class="st"> </span>Y <span class="op">-</span><span class="st"> </span>Yhat
<span class="kw">boxplot</span>(resid,<span class="dt">ylim=</span><span class="kw">quantile</span>(Y,<span class="kw">c</span>(<span class="fl">0.01</span>,<span class="fl">0.99</span>)),<span class="dt">range=</span><span class="dv">0</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/reconstruction_with_less_dimensions-1.png" alt="Residuals from comparing a reconstructed gene expression table using 95 PCs to the original data with 189 dimensions." width="672" />
<p class="caption">
(#fig:reconstruction_with_less_dimensions)Residuals from comparing a reconstructed gene expression table using 95 PCs to the original data with 189 dimensions.
</p>
</div>
<p>Therefore, by using only half as many dimensions, we retain most of the variability in our data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">var</span>(<span class="kw">as.vector</span>(resid))<span class="op">/</span><span class="kw">var</span>(<span class="kw">as.vector</span>(Y))</code></pre></div>
<pre><code>## [1] 0.04077</code></pre>
<p>We say that we explain 96% of the variability.</p>
<p>Note that we can compute this proportion from <span class="math inline">\(D\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span><span class="op">-</span><span class="kw">sum</span>(s<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span>k]<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.04077</code></pre>
<p>The entries of <span class="math inline">\(D\)</span> therefore tell us how much each PC contributes in term of variability explained.</p>
</div>
<div id="highly-correlated-data" class="section level4">
<h4><span class="header-section-number">8.6.0.2</span> Highly correlated data</h4>
<p>To help understand how the SVD works, we construct a dataset with two highly correlated columns.</p>
<p>For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="dv">100</span>
n &lt;-<span class="st"> </span><span class="dv">2</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(m)
e &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n<span class="op">*</span>m,<span class="dv">0</span>,<span class="fl">0.01</span>)
Y &lt;-<span class="st"> </span><span class="kw">cbind</span>(x,x)<span class="op">+</span>e
<span class="kw">cor</span>(Y)</code></pre></div>
<pre><code>##        x      x
## x 1.0000 0.9999
## x 0.9999 1.0000</code></pre>
<p>In this case, the second column adds very little “information” since all the entries of <code>Y[,1]-Y[,2]</code> are close to 0. Reporting <code>rowMeans(Y)</code> is even more efficient since <code>Y[,1]-rowMeans(Y)</code> and <code>Y[,2]-rowMeans(Y)</code> are even closer to 0. <code>rowMeans(Y)</code> turns out to be the information represented in the first column on <span class="math inline">\(U\)</span>. The SVD helps us notice that we explain almost all the variability with just this first column:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">svd</span>(Y)<span class="op">$</span>d
d[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(d<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.9999</code></pre>
<p>In cases with many correlated columns, we can achieve great dimension reduction:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="dv">100</span>
n &lt;-<span class="st"> </span><span class="dv">25</span>
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(m)
e &lt;-<span class="st"> </span><span class="kw">rnorm</span>(n<span class="op">*</span>m,<span class="dv">0</span>,<span class="fl">0.01</span>)
Y &lt;-<span class="st"> </span><span class="kw">replicate</span>(n,x)<span class="op">+</span>e
d &lt;-<span class="st"> </span><span class="kw">svd</span>(Y)<span class="op">$</span>d
d[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(d<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.9999</code></pre>
</div>
</div>
<div id="projections" class="section level2">
<h2><span class="header-section-number">8.7</span> Projections</h2>
<p>Now that we have described the concept of dimension reduction and some of the applications of SVD and principal component analysis, we focus on more details related to the mathematics behind these. We start with <em>projections</em>. A projection is a linear algebra concept that helps us understand many of the mathematical operations we perform on high-dimensional data. For more details, you can review projects in a linear algebra book. Here we provide a quick review and then provide some data analysis related examples.</p>
<p>As a review, remember that projections minimize the distance between points and subspace.</p>
<div class="figure">
<img src="http://upload.wikimedia.org/wikipedia/commons/8/84/Linalg_projection_3.png" alt="Illustration of projection." />
<p class="caption">Illustration of projection.</p>
</div>
<p>We illustrate projections using a figure, in which the arrow on top is pointing to a point in space. In this particular cartoon, the space is two dimensional, but we should be thinking abstractly. The space is represented by the Cartesian plan and the line on which the little person stands is a subspace of points. The projection to this subspace is the place that is closest to the original point. Geometry tells us that we can find this closest point by dropping a perpendicular line (dotted line) from the point to the space. The little person is standing on the projection. The amount this person had to walk from the origin to the new projected point is referred to as <em>the coordinate</em>.</p>
<p>For the explanation of projections, we will use the standard matrix algebra notation for points: <span class="math inline">\(\vec{y} \in \mathbb{R}^N\)</span> is a point in <span class="math inline">\(N\)</span>-dimensional space and <span class="math inline">\(L \subset \mathbb{R}^N\)</span> is smaller subspace.</p>
<div id="simple-example-with-n2" class="section level4">
<h4><span class="header-section-number">8.7.0.1</span> Simple example with N=2</h4>
<p>If we let <span class="math inline">\(Y = \begin{pmatrix} 2 \\ 3\end{pmatrix}\)</span>. We can plot it like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span> (<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>),<span class="dt">xlab=</span><span class="st">&quot;Dimension 1&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Dimension 2&quot;</span>,<span class="dt">type=</span><span class="st">&quot;n&quot;</span>)
<span class="kw">arrows</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">text</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="st">&quot; Y&quot;</span>,<span class="dt">pos=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:point"></span>
<img src="bookdown_files/figure-html/point-1.png" alt="Geometric representation of Y." width="672" />
<p class="caption">
图 8.4: Geometric representation of Y.
</p>
</div>
<p>We can immediately define a coordinate system by projecting this vector to the space defined by: <span class="math inline">\(\begin{pmatrix} 1\\ 0\end{pmatrix}\)</span> (the x-axis) and <span class="math inline">\(\begin{pmatrix} 0\\ 1\end{pmatrix}\)</span> (the y-axis). The projections of <span class="math inline">\(Y\)</span> to the subspace defined by these points are 2 and 3 respectively:</p>
<p><span class="math display">\[
\begin{align*}
Y &amp;= \begin{pmatrix} 2 \\ 3\end{pmatrix} \\
&amp;=2  \begin{pmatrix} 1\\ 0\end{pmatrix} + 3 \begin{pmatrix} 0\\ 1\end{pmatrix} 
\end{align*}\]</span></p>
<p>We say that <span class="math inline">\(2\)</span> and <span class="math inline">\(3\)</span> are the <em>coordinates</em> and that <span class="math inline">\(\begin{pmatrix} 1\\ 0\end{pmatrix} \mbox{and} \begin{pmatrix} 0\\1 \end{pmatrix}\)</span> are the bases.</p>
<p>Now let’s define a new subspace. The red line in the plot below is subset <span class="math inline">\(L\)</span> defined by points satisfying <span class="math inline">\(c \vec{v}\)</span> with <span class="math inline">\(\vec{v}=\begin{pmatrix} 2&amp; 1\end{pmatrix}^\top\)</span>. The projection of <span class="math inline">\(\vec{y}\)</span> onto <span class="math inline">\(L\)</span> is the closest point on <span class="math inline">\(L\)</span> to <span class="math inline">\(\vec{y}\)</span>. So we need to find the <span class="math inline">\(c\)</span> that minimizes the distance between <span class="math inline">\(\vec{y}\)</span> and <span class="math inline">\(c\vec{v}=(2c,c)\)</span>. In linear algebra, we learn that the difference between these points is orthogonal to the space so:</p>
<p><span class="math display">\[
(\vec{y}-\hat{c}\vec{v}) \cdot \vec{v} = 0
\]</span></p>
<p>this implies that:</p>
<p><span class="math display">\[
\vec{y}\cdot\vec{v} - \hat{c}\vec{v}\cdot\vec{v} =  0
\]</span></p>
<p>and:</p>
<p><span class="math display">\[\hat{c} = \frac{\vec{y}\cdot\vec{v}}
{\vec{v}\cdot\vec{v}}\]</span></p>
<p>Here the dot <span class="math inline">\(\cdot\)</span> represents the dot product: <span class="math inline">\(\,\, \vec{x} \cdot \vec{y} = x_1 y_1+\dots x_n y_n\)</span>.</p>
<p>The following R code confirms this equation works:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>),<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">4</span>),<span class="dt">xlab=</span><span class="st">&quot;Dimension 1&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Dimension 2&quot;</span>,<span class="dt">type=</span><span class="st">&quot;n&quot;</span>)
<span class="kw">arrows</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="fl">0.5</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>) <span class="co">#if x=2c and y=c then slope is 0.5 (y=0.5x)</span>
<span class="kw">text</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="st">&quot; Y&quot;</span>,<span class="dt">pos=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="dv">3</span>)
y=<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)
x=<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>)
cc =<span class="st"> </span><span class="kw">crossprod</span>(x,y)<span class="op">/</span><span class="kw">crossprod</span>(x)
<span class="kw">segments</span>(x[<span class="dv">1</span>]<span class="op">*</span>cc,x[<span class="dv">2</span>]<span class="op">*</span>cc,y[<span class="dv">1</span>],y[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">text</span>(x[<span class="dv">1</span>]<span class="op">*</span>cc,x[<span class="dv">2</span>]<span class="op">*</span>cc,<span class="kw">expression</span>(<span class="kw">hat</span>(Y)),<span class="dt">pos=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:projection"></span>
<img src="bookdown_files/figure-html/projection-1.png" alt="Projection of Y onto new subspace." width="672" />
<p class="caption">
图 8.5: Projection of Y onto new subspace.
</p>
</div>
<p>Note that if <span class="math inline">\(\vec{v}\)</span> was such that <span class="math inline">\(\vec{v}\cdot \vec{v}=1\)</span>, then <span class="math inline">\(\hat{c}\)</span> is simply <span class="math inline">\(\vec{y} \cdot \vec{v}\)</span> and the space <span class="math inline">\(L\)</span> does not change. This simplification is one reason we like orthogonal matrices.</p>
</div>
<div id="example-the-sample-mean-is-a-projection" class="section level4">
<h4><span class="header-section-number">8.7.0.2</span> Example: The sample mean is a projection</h4>
<p>Let <span class="math inline">\(\vec{y} \in \mathbb{R}^N\)</span> and <span class="math inline">\(L \subset \mathbb{R}^N\)</span> is the space spanned by:</p>
<p><span class="math display">\[\vec{v}=\begin{pmatrix} 1\\ \vdots \\  1\end{pmatrix};
L = \{ c \vec{v}; c \in \mathbb{R}\}\]</span></p>
<p>In this space, all components of the vectors are the same number, so we can think of this space as representing the constants: in the projection each dimension will be the same value. So what <span class="math inline">\(c\)</span> minimizes the distance between <span class="math inline">\(c\vec{v}\)</span> and <span class="math inline">\(\vec{y}\)</span> ?</p>
<p>When talking about problems like this, we sometimes use two dimensional figures such as the one above. We simply abstract and think of <span class="math inline">\(\vec{y}\)</span> as a point in <span class="math inline">\(N-dimensions\)</span> and <span class="math inline">\(L\)</span> as a subspace defined by a smaller number of values, in this case just one: <span class="math inline">\(c\)</span>.</p>
<p>Getting back to our question, we know that the projection is:</p>
<p><span class="math display">\[\hat{c} = \frac{\vec{y}\cdot\vec{v}}
{\vec{v}\cdot\vec{v}}\]</span></p>
<p>which in this case is the average:</p>
<p><span class="math display">\[
\hat{c} = \frac{\vec{y}\cdot\vec{v}}
{\vec{v}\cdot\vec{v}} = \frac{\sum_{i=1}^N Y_i}{\sum_{i=1}^N 1} = \bar{Y}
\]</span></p>
<p>Here, it also would have been just as easy to use calculus:</p>
<p><span class="math display">\[\frac{\partial}{\partial c}\sum_{i=1}^N (Y_i - c)^2 = 0 \implies 
 - 2 \sum_{i=1}^N (Y_i - \hat{c}) = 0 \implies\]</span></p>
<p><span class="math display">\[ N c = \sum_{i=1}^N Y_i \implies \hat{c}=\bar{Y}\]</span></p>
</div>
<div id="example-regression-is-also-a-projection" class="section level4">
<h4><span class="header-section-number">8.7.0.3</span> Example: Regression is also a projection</h4>
<p>Let us give a slightly more complicated example. Simple linear regression can also be explained with projections. Our data <span class="math inline">\(\mathbf{Y}\)</span> (we are no longer going to use the <span class="math inline">\(\vec{y}\)</span> notation) is again an <em>N</em>-dimensional vector and our model predicts <span class="math inline">\(Y_i\)</span> with a line <span class="math inline">\(\beta_0 + \beta_1 X_i\)</span>. We want to find the <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimize the distance between <span class="math inline">\(Y\)</span> and the space defined by:</p>
<p><span class="math display">\[ L = \{ \beta_0 \vec{v}_0 + \beta_1 \vec{v}_1 ; \vec{\beta}=(\beta_0,\beta_1) \in \mathbb{R}^2 \}\]</span></p>
<p>with:</p>
<p><span class="math display">\[
\vec{v}_0=
\begin{pmatrix}
1\\
1\\
\vdots \\
1\\
\end{pmatrix} 
\mbox{ and }
\vec{v}_1=
\begin{pmatrix}
X_{1}\\
X_{2}\\
\vdots \\
X_{N}\\
\end{pmatrix} 
\]</span></p>
<p>Our <span class="math inline">\(N\times 2\)</span> matrix <span class="math inline">\(\mathbf{X}\)</span> is <span class="math inline">\([ \vec{v}_0 \,\, \vec{v}_1]\)</span> and any point in <span class="math inline">\(L\)</span> can be written as <span class="math inline">\(X\vec{\beta}\)</span>.</p>
<p>The equation for the multidimensional version of orthogonal projection is:</p>
<p><span class="math display">\[X^\top (\vec{y}-X\vec{\beta}) = 0\]</span></p>
<p>which we have seen before and gives us:</p>
<p><span class="math display">\[X^\top X \hat{\beta}=  X^\top \vec{y} \]</span></p>
<p><span class="math display">\[\hat{\beta}= (X^\top X)^{-1}X^\top \vec{y}\]</span></p>
<p>And the projection to <span class="math inline">\(L\)</span> is therefore:</p>
<p><span class="math display">\[X (X^\top X)^{-1}X^\top \vec{y}\]</span></p>
</div>
</div>
<div id="rotations-1" class="section level2">
<h2><span class="header-section-number">8.8</span> Rotations</h2>
<p>One of the most useful applications of projections relates to coordinate rotations. In data analysis, simple rotations can result in easier to visualize and interpret data. We will describe the mathematics behind rotations and give some data analysis examples.</p>
<p>In our previous section, we used the following example:</p>
<p><span class="math display">\[
Y = \begin{pmatrix} 2 \\ 
3 
\end{pmatrix} 
= 
2  
\begin{pmatrix} 1\\
0 
\end{pmatrix} + 
3 
\begin{pmatrix} 0\\ 
1 
\end{pmatrix} 
\]</span></p>
<p>and noted that <span class="math inline">\(2\)</span> and <span class="math inline">\(3\)</span> are the <em>coordinates</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>()
<span class="kw">plot</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">4</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">4</span>),<span class="dt">xlab=</span><span class="st">&quot;Dimension 1&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Dimension 2&quot;</span>,
     <span class="dt">type=</span><span class="st">&quot;n&quot;</span>,<span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="dt">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="dt">bty=</span><span class="st">&quot;n&quot;</span>)
<span class="kw">text</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>),<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)),<span class="kw">as.character</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>))),<span class="dt">pos=</span><span class="dv">2</span>)
<span class="kw">text</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>),<span class="kw">as.character</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>))),<span class="dt">pos=</span><span class="dv">1</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">h=</span><span class="dv">0</span>)
<span class="kw">arrows</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">segments</span>(<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">segments</span>(<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">text</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="st">&quot; Y&quot;</span>,<span class="dt">pos=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-421"></span>
<img src="bookdown_files/figure-html/unnamed-chunk-421-1.png" alt="Plot of (2,3) as coordinates along Dimension 1 (1,0) and Dimension 2 (0,1)." width="672" />
<p class="caption">
图 8.6: Plot of (2,3) as coordinates along Dimension 1 (1,0) and Dimension 2 (0,1).
</p>
</div>
<p>However, mathematically we can represent the point <span class="math inline">\((2,3)\)</span> with other linear combinations:</p>
<p><span class="math display">\[
\begin{align*}
Y &amp;= \begin{pmatrix} 2 \\ 3\end{pmatrix} \\
&amp;= 2.5 \begin{pmatrix} 1\\ 1\end{pmatrix} + -1 \begin{pmatrix} \phantom{-}0.5\\ -0.5\end{pmatrix} 
\end{align*}\]</span></p>
<p>The new coordinates are:</p>
<p><span class="math display">\[Z = \begin{pmatrix} 2.5 \\ -1 \end{pmatrix}\]</span></p>
<p>Graphically, we can see that the coordinates are the projections to the spaces defined by the new basis:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">mypar</span>()
<span class="kw">plot</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">4</span>),<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">4</span>),<span class="dt">xlab=</span><span class="st">&quot;Dimension 1&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Dimension 2&quot;</span>,
     <span class="dt">type=</span><span class="st">&quot;n&quot;</span>,<span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>,<span class="dt">yaxt=</span><span class="st">&quot;n&quot;</span>,<span class="dt">bty=</span><span class="st">&quot;n&quot;</span>)
<span class="kw">text</span>(<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>),<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)),<span class="kw">as.character</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>))),<span class="dt">pos=</span><span class="dv">2</span>)
<span class="kw">text</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>)),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>),<span class="kw">as.character</span>(<span class="kw">c</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>),<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>))),<span class="dt">pos=</span><span class="dv">1</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">h=</span><span class="dv">0</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">arrows</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
y=<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)
x1=<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)##new basis
x2=<span class="kw">c</span>(<span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.5</span>)##new basis
c1 =<span class="st"> </span><span class="kw">crossprod</span>(x1,y)<span class="op">/</span><span class="kw">crossprod</span>(x1)
c2 =<span class="st"> </span><span class="kw">crossprod</span>(x2,y)<span class="op">/</span><span class="kw">crossprod</span>(x2)
<span class="kw">segments</span>(x1[<span class="dv">1</span>]<span class="op">*</span>c1,x1[<span class="dv">2</span>]<span class="op">*</span>c1,y[<span class="dv">1</span>],y[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">segments</span>(x2[<span class="dv">1</span>]<span class="op">*</span>c2,x2[<span class="dv">2</span>]<span class="op">*</span>c2,y[<span class="dv">1</span>],y[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">text</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="st">&quot; Y&quot;</span>,<span class="dt">pos=</span><span class="dv">4</span>,<span class="dt">cex=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:unnamed-chunk-422"></span>
<img src="bookdown_files/figure-html/unnamed-chunk-422-1.png" alt="Plot of (2,3) as a vector in a rotatated space, relative to the original dimensions." width="672" />
<p class="caption">
图 8.7: Plot of (2,3) as a vector in a rotatated space, relative to the original dimensions.
</p>
</div>
<p>We can go back and forth between these two representations of <span class="math inline">\((2,3)\)</span> using matrix multiplication.</p>
<p><span class="math display">\[
Y =   AZ\\
\]</span></p>
<p><span class="math display">\[
 A^{-1} Y =  Z\\
\]</span></p>
<p><span class="math display">\[
A= \begin{pmatrix} 1&amp; \phantom{-}0.5\\ 1 &amp; -0.5\end{pmatrix} \implies
A^{-1}= \begin{pmatrix} 0.5&amp; 0.5 \\ 1 &amp;-1\end{pmatrix}
\]</span></p>
<p><span class="math inline">\(Z\)</span> and <span class="math inline">\(Y\)</span> carry the same information, but in a different <em>coordinate system</em>.</p>
<div id="example-twin-heights" class="section level4">
<h4><span class="header-section-number">8.8.0.1</span> Example: Twin heights</h4>
<p>Here are 100 two dimensional points <span class="math inline">\(Y\)</span></p>
<div class="figure"><span id="fig:twin-heights"></span>
<img src="bookdown_files/figure-html/twin-heights-1.png" alt="Twin 2 heights versus twin 1 heights." width="672" />
<p class="caption">
图 8.8: Twin 2 heights versus twin 1 heights.
</p>
</div>
<p>Here are the rotations: <span class="math inline">\(Z = A^{-1} Y\)</span></p>
<div class="figure"><span id="fig:twin-heights-rotated"></span>
<img src="bookdown_files/figure-html/twin-heights-rotated-1.png" alt="Rotation of twin 2 heights versus twin 1 heights." width="672" />
<p class="caption">
图 8.9: Rotation of twin 2 heights versus twin 1 heights.
</p>
</div>
<p>What we have done here is rotate the data so that the first coordinate of <span class="math inline">\(Z\)</span> is the average height, while the second is the difference between twin heights.</p>
<p>We have used the singular value decomposition to find principal components. It is sometimes useful to think of the SVD as a rotation, for example <span class="math inline">\(\mathbf{U}^\top \mathbf{Y}\)</span>, that gives us a new coordinate system <span class="math inline">\(\mathbf{DV}^\top\)</span> in which the dimensions are ordered by how much variance they explain.</p>
</div>
</div>
<div id="multi-dimensional-scaling-plots" class="section level2">
<h2><span class="header-section-number">8.9</span> Multi-Dimensional Scaling Plots</h2>
<p>We will motivate multi-dimensional scaling (MDS) plots with a gene expression example. To simplify the illustration we will only consider three tissues:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
<span class="kw">library</span>(tissuesGeneExpression)
<span class="kw">data</span>(tissuesGeneExpression)
colind &lt;-<span class="st"> </span>tissue<span class="op">%in%</span><span class="kw">c</span>(<span class="st">&quot;kidney&quot;</span>,<span class="st">&quot;colon&quot;</span>,<span class="st">&quot;liver&quot;</span>)
mat &lt;-<span class="st"> </span>e[,colind]
group &lt;-<span class="st"> </span><span class="kw">factor</span>(tissue[colind])
<span class="kw">dim</span>(mat)</code></pre></div>
<pre><code>## [1] 22215    99</code></pre>
<p>As an exploratory step, we wish to know if gene expression profiles stored in the columns of <code>mat</code> show more similarity between tissues than across tissues. Unfortunately, as mentioned above, we can’t plot multi-dimensional points. In general, we prefer two-dimensional plots, but making plots for every pair of genes or every pair of samples is not practical. MDS plots become a powerful tool in this situation.</p>
<div id="the-math-behind-mds" class="section level4">
<h4><span class="header-section-number">8.9.0.1</span> The math behind MDS</h4>
<p>Now that we know about SVD and matrix algebra, understanding MDS is relatively straightforward. For illustrative purposes let’s consider the SVD decomposition:</p>
<p><span class="math display">\[\mathbf{Y} = \mathbf{UDV}^\top\]</span></p>
<p>and assume that the sum of squares of the first two columns <span class="math inline">\(\mathbf{U^\top Y=DV^\top}\)</span> is much larger than the sum of squares of all other columns. This can be written as: <span class="math inline">\(d_1+ d_2 \gg d_3 + \dots + d_n\)</span> with <span class="math inline">\(d_i\)</span> the i-th entry of the <span class="math inline">\(\mathbf{D}\)</span> matrix. When this happens, we then have:</p>
<p><span class="math display">\[\mathbf{Y}\approx [\mathbf{U}_1 \mathbf{U}_2] 
  \begin{pmatrix}
    d_{1}&amp;0\\
    0&amp;d_{2}\\
  \end{pmatrix}
  [\mathbf{V}_1 \mathbf{V}_2]^\top  
\]</span></p>
<p>This implies that column <span class="math inline">\(i\)</span> is approximately:</p>
<p><span class="math display">\[
\mathbf{Y}_i \approx
[\mathbf{U}_1 \mathbf{U}_2] 
  \begin{pmatrix}
    d_{1}&amp;0\\
    0&amp;d_{2}\\
  \end{pmatrix}
  \begin{pmatrix}
    v_{i,1}\\
    v_{i,2}\\
     \end{pmatrix}
    =
    [\mathbf{U}_1 \mathbf{U}_2] 
  \begin{pmatrix}
    d_{1} v_{i,1}\\
    d_{2} v_{i,2}
 \end{pmatrix}
\]</span></p>
<p>If we define the following two dimensional vector…</p>
<p><span class="math display">\[\mathbf{Z}_i=\begin{pmatrix}
    d_{1} v_{i,1}\\
    d_{2} v_{i,2}
 \end{pmatrix}
 \]</span></p>
<p>… then</p>
<p><span class="math display">\[
\begin{align*}
(\mathbf{Y}_i - \mathbf{Y}_j)^\top(\mathbf{Y}_i - \mathbf{Y}_j) &amp;\approx \left\{ [\mathbf{U}_1 \mathbf{U}_2] (\mathbf{Z}_i-\mathbf{Z}_j) \right\}^\top \left\{[\mathbf{U}_1 \mathbf{U}_2]  (\mathbf{Z}_i-\mathbf{Z}_j)\right\}\\
&amp;= (\mathbf{Z}_i-\mathbf{Z}_j)^\top [\mathbf{U}_1 \mathbf{U}_2]^\top [\mathbf{U}_1 \mathbf{U}_2] (\mathbf{Z}_i-\mathbf{Z}_j) \\
&amp;=(\mathbf{Z}_i-\mathbf{Z}_j)^\top(\mathbf{Z}_i-\mathbf{Z}_j)\\
&amp;=(Z_{i,1}-Z_{j,1})^2 + (Z_{i,2}-Z_{j,2})^2
\end{align*}
\]</span></p>
<p>This derivation tells us that the distance between samples <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is approximated by the distance between two dimensional points.</p>
<p><span class="math display">\[ (\mathbf{Y}_i - \mathbf{Y}_j)^\top(\mathbf{Y}_i - \mathbf{Y}_j) \approx
 (Z_{i,1}-Z_{j,1})^2 + (Z_{i,2}-Z_{j,2})^2
\]</span></p>
<p>Because <span class="math inline">\(Z\)</span> is a two dimensional vector, we can visualize the distances between each sample by plotting <span class="math inline">\(\mathbf{Z}_1\)</span> versus <span class="math inline">\(\mathbf{Z}_2\)</span> and visually inspect the distance between points. Here is this plot for our example dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">svd</span>(mat<span class="op">-</span><span class="kw">rowMeans</span>(mat))
PC1 &lt;-<span class="st"> </span>s<span class="op">$</span>d[<span class="dv">1</span>]<span class="op">*</span>s<span class="op">$</span>v[,<span class="dv">1</span>]
PC2 &lt;-<span class="st"> </span>s<span class="op">$</span>d[<span class="dv">2</span>]<span class="op">*</span>s<span class="op">$</span>v[,<span class="dv">2</span>]
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(PC1,PC2,<span class="dt">pch=</span><span class="dv">21</span>,<span class="dt">bg=</span><span class="kw">as.numeric</span>(group))
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,<span class="kw">levels</span>(group),<span class="dt">col=</span><span class="kw">seq</span>(<span class="dt">along=</span><span class="kw">levels</span>(group)),<span class="dt">pch=</span><span class="dv">15</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</code></pre></div>
<div class="figure"><span id="fig:MDS"></span>
<img src="bookdown_files/figure-html/MDS-1.png" alt="Multi-dimensional scaling (MDS) plot for tissue gene expression data." width="672" />
<p class="caption">
图 8.10: Multi-dimensional scaling (MDS) plot for tissue gene expression data.
</p>
</div>
<p>Note that the points separate by tissue type as expected. Now the accuracy of the approximation above depends on the proportion of variance explained by the first two principal components. As we showed above, we can quickly see this by plotting the variance explained plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="kw">sum</span>(s<span class="op">$</span>d<span class="op">^</span><span class="dv">2</span>))</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/variance_explained-1.png" alt="Variance explained for each principal component." width="672" />
<p class="caption">
(#fig:variance_explained)Variance explained for each principal component.
</p>
</div>
<p>Although the first two PCs explain over 50% of the variability, there is plenty of information that this plot does not show. However, it is an incredibly useful plot for obtaining, via visualization, a general idea of the distance between points. Also, notice that we can plot other dimensions as well to search for patterns. Here are the 3rd and 4th PCs:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">PC3 &lt;-<span class="st"> </span>s<span class="op">$</span>d[<span class="dv">3</span>]<span class="op">*</span>s<span class="op">$</span>v[,<span class="dv">3</span>]
PC4 &lt;-<span class="st"> </span>s<span class="op">$</span>d[<span class="dv">4</span>]<span class="op">*</span>s<span class="op">$</span>v[,<span class="dv">4</span>]
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(PC3,PC4,<span class="dt">pch=</span><span class="dv">21</span>,<span class="dt">bg=</span><span class="kw">as.numeric</span>(group))
<span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,<span class="kw">levels</span>(group),<span class="dt">col=</span><span class="kw">seq</span>(<span class="dt">along=</span><span class="kw">levels</span>(group)),<span class="dt">pch=</span><span class="dv">15</span>,<span class="dt">cex=</span><span class="fl">1.5</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/PC_3_and_4-1.png" alt="Third and fourth principal components." width="672" />
<p class="caption">
(#fig:PC_3_and_4)Third and fourth principal components.
</p>
</div>
<p>Note that the 4th PC shows a strong separation within the kidney samples. Later we will learn about batch effects, which might explain this finding.</p>
</div>
<div id="cmdscale" class="section level4">
<h4><span class="header-section-number">8.9.0.2</span> <code>cmdscale</code></h4>
<p>Although we used the <code>svd</code> functions above, there is a special function that is specifically made for MDS plots. It takes a distance object as an argument and then uses principal component analysis to provide the best approximation to this distance that can be obtained with <span class="math inline">\(k\)</span> dimensions. This function is more efficient because one does not have to perform the full SVD, which can be time consuming. By default it returns two dimensions, but we can change that through the parameter <code>k</code> which defaults to 2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(mat))
mds &lt;-<span class="st"> </span><span class="kw">cmdscale</span>(d)
<span class="kw">mypar</span>()
<span class="kw">plot</span>(mds[,<span class="dv">1</span>],mds[,<span class="dv">2</span>],<span class="dt">bg=</span><span class="kw">as.numeric</span>(group),<span class="dt">pch=</span><span class="dv">21</span>,
     <span class="dt">xlab=</span><span class="st">&quot;First dimension&quot;</span>,<span class="dt">ylab=</span><span class="st">&quot;Second dimension&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>,<span class="kw">levels</span>(group),<span class="dt">col=</span><span class="kw">seq</span>(<span class="dt">along=</span><span class="kw">levels</span>(group)),<span class="dt">pch=</span><span class="dv">15</span>)</code></pre></div>
<div class="figure"><span id="fig:mds2"></span>
<img src="bookdown_files/figure-html/mds2-1.png" alt="MDS computed with cmdscale function." width="672" />
<p class="caption">
图 8.11: MDS computed with cmdscale function.
</p>
</div>
<p>These two approaches are equivalent up to an arbitrary sign change.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>){
  <span class="kw">plot</span>(mds[,i],s<span class="op">$</span>d[i]<span class="op">*</span>s<span class="op">$</span>v[,i],<span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,i))
  b =<span class="st"> </span><span class="kw">ifelse</span>( <span class="kw">cor</span>(mds[,i],s<span class="op">$</span>v[,i]) <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)
  <span class="kw">abline</span>(<span class="dv">0</span>,b) ##b is 1 or -1 depending on the arbitrary sign &quot;flip&quot;
}</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/mds_same_as_svd-1.png" alt="Comparison of MDS first two PCs to SVD first two PCs." width="1008" />
<p class="caption">
(#fig:mds_same_as_svd)Comparison of MDS first two PCs to SVD first two PCs.
</p>
</div>
</div>
<div id="why-the-arbitrary-sign" class="section level4">
<h4><span class="header-section-number">8.9.0.3</span> Why the arbitrary sign?</h4>
<p>The SVD is not unique because we can multiply any column of <span class="math inline">\(\mathbf{V}\)</span> by -1 as long as we multiply the sample column of <span class="math inline">\(\mathbf{U}\)</span> by -1. We can see this immediately by noting that:</p>
<p><span class="math display">\[
\mathbf{-1UD(-1)V}^\top = \mathbf{UDV}^\top
\]</span></p>
</div>
<div id="why-we-substract-the-mean" class="section level4">
<h4><span class="header-section-number">8.9.0.4</span> Why we substract the mean</h4>
<p>In all calculations above we subtract the row means before we compute the singular value decomposition. If what we are trying to do is approximate the distance between columns, the distance between <span class="math inline">\(\mathbf{Y}_i\)</span> and <span class="math inline">\(\mathbf{Y}_j\)</span> is the same as the distance between <span class="math inline">\(\mathbf{Y}_i- \mathbf{\mu}\)</span> and <span class="math inline">\(\mathbf{Y}_j - \mathbf{\mu}\)</span> since the <span class="math inline">\(\mu\)</span> cancels out when computing said distance:</p>
<p><span class="math display">\[
\left\{ ( \mathbf{Y}_i- \mathbf{\mu} ) - ( \mathbf{Y}_j - \mathbf{\mu} ) \right\}^\top \left\{ (\mathbf{Y}_i- \mathbf{\mu}) - (\mathbf{Y}_j - \mathbf{\mu} ) \right\} = \left\{  \mathbf{Y}_i-  \mathbf{Y}_j  \right\}^\top \left\{ \mathbf{Y}_i - \mathbf{Y}_j  \right\}
\]</span></p>
<p>Because removing the row averages reduces the total variation, it can only make the SVD approximation better.</p>
</div>
</div>
<div id="mds-exercises" class="section level2">
<h2><span class="header-section-number">8.10</span> MDS exercises</h2>
<p><strong>Exercises</strong></p>
<ol style="list-style-type: decimal">
<li><p>Using the <code>z</code> we computed in exercise 4 of the previous exercises:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tissuesGeneExpression)
<span class="kw">data</span>(tissuesGeneExpression)
y =<span class="st"> </span>e <span class="op">-</span><span class="st"> </span><span class="kw">rowMeans</span>(e)
s =<span class="st"> </span><span class="kw">svd</span>(y)
z =<span class="st"> </span>s<span class="op">$</span>d <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(s<span class="op">$</span>v)</code></pre></div>
<p>we can make an mds plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rafalib)
ftissue =<span class="st"> </span><span class="kw">factor</span>(tissue)
<span class="kw">mypar2</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(z[<span class="dv">1</span>,],z[<span class="dv">2</span>,],<span class="dt">col=</span><span class="kw">as.numeric</span>(ftissue))
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>,<span class="kw">levels</span>(ftissue),<span class="dt">col=</span><span class="kw">seq_along</span>(ftissue),<span class="dt">pch=</span><span class="dv">1</span>)</code></pre></div>
<p>Now run the function <code>cmdscale</code> on the original data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d =<span class="st"> </span><span class="kw">dist</span>(<span class="kw">t</span>(e))
mds =<span class="st"> </span><span class="kw">cmdscale</span>(d)</code></pre></div>
<p>What is the absolute value of the correlation between the first dimension of <code>z</code> and the first dimension in mds?</p></li>
<li><p>What is the absolute value of the correlation between the second dimension of <code>z</code> and the second dimension in mds?</p></li>
<li><p>Load the following dataset:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GSE5859Subset)
<span class="kw">data</span>(GSE5859Subset)</code></pre></div>
<p>Compute the svd and compute <code>z</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s =<span class="st"> </span><span class="kw">svd</span>(geneExpression<span class="op">-</span><span class="kw">rowMeans</span>(geneExpression))
z =<span class="st"> </span>s<span class="op">$</span>d <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(s<span class="op">$</span>v)</code></pre></div>
<p>Which dimension of <code>z</code> most correlates with the outcome <code>sampleInfo$group</code> ?</p></li>
<li><p>What is this max correlation?</p></li>
<li><p>Which dimension of <code>z</code> has the second highest correlation with the outcome <code>sampleInfo$group</code>?</p></li>
<li><p>Note these measurements were made during two months:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sampleInfo<span class="op">$</span>date</code></pre></div>
<p>We can extract the month this way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">month =<span class="st"> </span><span class="kw">format</span>( sampleInfo<span class="op">$</span>date, <span class="st">&quot;%m&quot;</span>)
month =<span class="st"> </span><span class="kw">factor</span>( month)</code></pre></div>
<p>Which dimension of <code>z</code> has the second highest correlation with the outcome <code>month</code></p></li>
<li><p>What is this correlation?</p></li>
<li><p>(Advanced) The same dimension is correlated with both the group and the date. The following are also correlated:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">table</span>(sampleInfo<span class="op">$</span>g,month)</code></pre></div>
<p>So is this first dimension related directly to group or is it related only through the month? Note that the correlation with month is higher. This is related to <em>batch effects</em> which we will learn about later.</p>
<p>In exercise 3 we saw that one of the dimensions was highly correlated to the <code>sampleInfo$group</code>. Now take the 5th column of <span class="math display">\[\mathbf{U}\]</span> and stratify by the gene chromosome. Remove <code>chrUn</code> and make a boxplot of the values of <span class="math display">\[\mathbf{U}_5\]</span> stratified by chromosome.</p>
<p>Which chromosome looks different from the rest? Copy and paste the name as it appears in <code>geneAnnotation</code>.</p></li>
</ol>
<p>Given the answer to the last exercise, any guesses as to what <code>sampleInfo$group</code> represents?</p>
</div>
<div id="principal-component-analysis" class="section level2">
<h2><span class="header-section-number">8.11</span> Principal Component Analysis</h2>
<p>We have already mentioned principal component analysis (PCA) above and noted its relation to the SVD. Here we provide further mathematical details.</p>
<div id="example-twin-heights-1" class="section level4">
<h4><span class="header-section-number">8.11.0.1</span> Example: Twin heights</h4>
<p>We started the motivation for dimension reduction with a simulated example and showed a rotation that is very much related to PCA.</p>
<div class="figure">
<img src="bookdown_files/figure-html/simulate_twin_heights_again-1.png" alt="Twin heights scatter plot." width="672" />
<p class="caption">
(#fig:simulate_twin_heights_again)Twin heights scatter plot.
</p>
</div>
<p>Here we explain specifically what are the principal components (PCs).</p>
<p>Let <span class="math inline">\(\mathbf{Y}\)</span> be <span class="math inline">\(2 \times N\)</span> matrix representing our data. The analogy is that we measure expression from 2 genes and each column is a sample. Suppose we are given the task of finding a <span class="math inline">\(2 \times 1\)</span> vector <span class="math inline">\(\mathbf{u}_1\)</span> such that <span class="math inline">\(\mathbf{u}_1^\top \mathbf{v}_1 = 1\)</span> and it maximizes <span class="math inline">\((\mathbf{u}_1^\top\mathbf{Y})^\top (\mathbf{u}_1^\top\mathbf{Y})\)</span>. This can be viewed as a projection of each sample or column of <span class="math inline">\(\mathbf{Y}\)</span> into the subspace spanned by <span class="math inline">\(\mathbf{u}_1\)</span>. So we are looking for a transformation in which the coordinates show high variability.</p>
<p>Let’s try <span class="math inline">\(\mathbf{u}=(1,0)^\top\)</span>. This projection simply gives us the height of twin 1 shown in orange below. The sum of squares is shown in the title.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(<span class="kw">t</span>(Y), <span class="dt">xlim=</span>thelim, <span class="dt">ylim=</span>thelim,
     <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&quot;Sum of squares :&quot;</span>,<span class="kw">round</span>(<span class="kw">crossprod</span>(Y[<span class="dv">1</span>,]),<span class="dv">1</span>)))
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>)
<span class="kw">apply</span>(Y,<span class="dv">2</span>,<span class="cf">function</span>(y) <span class="kw">segments</span>(y[<span class="dv">1</span>],<span class="dv">0</span>,y[<span class="dv">1</span>],y[<span class="dv">2</span>],<span class="dt">lty=</span><span class="dv">2</span>))</code></pre></div>
<pre><code>## NULL</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">points</span>(Y[<span class="dv">1</span>,],<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(Y)),<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">16</span>,<span class="dt">cex=</span><span class="fl">0.75</span>)</code></pre></div>
<p><img src="bookdown_files/figure-html/projection_not_PC1-1.png" width="624" /></p>
<p>Can we find a direction with higher variability? How about:</p>
<p><span class="math inline">\(\mathbf{u} =\begin{pmatrix}1\\-1\end{pmatrix}\)</span> ? This does not satisfy <span class="math inline">\(\mathbf{u}^\top\mathbf{u}= 1\)</span> so let’s instead try <span class="math inline">\(\mathbf{u} =\begin{pmatrix}1/\sqrt{2}\\-1/\sqrt{2}\end{pmatrix}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="op">-</span><span class="dv">1</span>)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span>),<span class="dt">ncol=</span><span class="dv">1</span>)
w=<span class="kw">t</span>(u)<span class="op">%*%</span>Y
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">1</span>)
<span class="kw">plot</span>(<span class="kw">t</span>(Y),
     <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&quot;Sum of squares:&quot;</span>,<span class="kw">round</span>(<span class="kw">tcrossprod</span>(w),<span class="dv">1</span>)),<span class="dt">xlim=</span>thelim,<span class="dt">ylim=</span>thelim)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="op">-</span><span class="dv">1</span>,<span class="dt">col=</span><span class="dv">2</span>)
Z =<span class="st"> </span>u<span class="op">%*%</span>w
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq</span>(<span class="dt">along=</span>w))
  <span class="kw">segments</span>(Z[<span class="dv">1</span>,i],Z[<span class="dv">2</span>,i],Y[<span class="dv">1</span>,i],Y[<span class="dv">2</span>,i],<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">points</span>(<span class="kw">t</span>(Z), <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.5</span>)</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/projection_not_PC1_either-1.png" alt="Data projected onto space spanned by (1 0)." width="624" />
<p class="caption">
(#fig:projection_not_PC1_either)Data projected onto space spanned by (1 0).
</p>
</div>
<p>This relates to the difference between twins, which we know is small. The sum of squares confirms this.</p>
<p>Finally, let’s try:</p>
<p><span class="math inline">\(\mathbf{u} =\begin{pmatrix}1/\sqrt{2}\\1/\sqrt{2}\end{pmatrix}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">u &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="kw">sqrt</span>(<span class="dv">2</span>),<span class="dt">ncol=</span><span class="dv">1</span>)
w=<span class="kw">t</span>(u)<span class="op">%*%</span>Y
<span class="kw">mypar</span>()
<span class="kw">plot</span>(<span class="kw">t</span>(Y), <span class="dt">main=</span><span class="kw">paste</span>(<span class="st">&quot;Sum of squares:&quot;</span>,<span class="kw">round</span>(<span class="kw">tcrossprod</span>(w),<span class="dv">1</span>)),
     <span class="dt">xlim=</span>thelim, <span class="dt">ylim=</span>thelim)
<span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dt">col=</span><span class="dv">2</span>)
<span class="kw">points</span>(u<span class="op">%*%</span>w, <span class="dt">col=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="dv">1</span>)
Z =<span class="st"> </span>u<span class="op">%*%</span>w
<span class="cf">for</span>(i <span class="cf">in</span> <span class="kw">seq</span>(<span class="dt">along=</span>w))
  <span class="kw">segments</span>(Z[<span class="dv">1</span>,i], Z[<span class="dv">2</span>,i], Y[<span class="dv">1</span>,i], Y[<span class="dv">2</span>,i], <span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">points</span>(<span class="kw">t</span>(Z),<span class="dt">col=</span><span class="dv">2</span>,<span class="dt">pch=</span><span class="dv">16</span>,<span class="dt">cex=</span><span class="fl">0.5</span>)</code></pre></div>
<div class="figure"><span id="fig:PC1"></span>
<img src="bookdown_files/figure-html/PC1-1.png" alt="Data projected onto space spanned by first PC." width="624" />
<p class="caption">
图 8.12: Data projected onto space spanned by first PC.
</p>
</div>
<p>This is a re-scaled average height, which has higher sum of squares. There is a mathematical procedure for determining which <span class="math inline">\(\mathbf{v}\)</span> maximizes the sum of squares and the SVD provides it for us.</p>
</div>
<div id="the-principal-components" class="section level4">
<h4><span class="header-section-number">8.11.0.2</span> The principal components</h4>
<p>The orthogonal vector that maximizes the sum of squares:</p>
<p><span class="math display">\[(\mathbf{u}_1^\top\mathbf{Y})^\top(\mathbf{u}_1^\top\mathbf{Y})\]</span></p>
<p><span class="math inline">\(\mathbf{u}_1^\top\mathbf{Y}\)</span> is referred to as the first PC. The <em>weights</em> <span class="math inline">\(\mathbf{u}\)</span> used to obtain this PC are referred to as the <em>loadings</em>. Using the language of rotations, it is also referred to as the <em>direction</em> of the first PC, which are the new coordinates.</p>
<p>To obtain the second PC, we repeat the exercise above, but for the residuals:</p>
<p><span class="math display">\[\mathbf{r} = \mathbf{Y} - \mathbf{u}_1^\top \mathbf{Yv}_1 \]</span></p>
<p>The second PC is the vector with the following properties:</p>
<p><span class="math display">\[ \mathbf{v}_2^\top \mathbf{v}_2=1\]</span></p>
<p><span class="math display">\[ \mathbf{v}_2^\top \mathbf{v}_1=0\]</span></p>
<p>and maximizes <span class="math inline">\((\mathbf{rv}_2)^\top \mathbf{rv}_2\)</span>.</p>
<p>When <span class="math inline">\(Y\)</span> is <span class="math inline">\(N \times m\)</span> we repeat to find 3rd, 4th, …, m-th PCs.</p>
</div>
<div id="prcomp" class="section level4">
<h4><span class="header-section-number">8.11.0.3</span> <code>prcomp</code></h4>
<p>We have shown how to obtain PCs using the SVD. However, R has a function specifically designed to find the principal components. In this case, the data is centered by default. The following function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pc &lt;-<span class="st"> </span><span class="kw">prcomp</span>( <span class="kw">t</span>(Y) )</code></pre></div>
<p>produces the same results as the SVD up to arbitrary sign flips:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">svd</span>( Y <span class="op">-</span><span class="st"> </span><span class="kw">rowMeans</span>(Y) )
<span class="kw">mypar</span>(<span class="dv">1</span>,<span class="dv">2</span>)
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(Y) ){
  <span class="kw">plot</span>(pc<span class="op">$</span>x[,i], s<span class="op">$</span>d[i]<span class="op">*</span>s<span class="op">$</span>v[,i])
}</code></pre></div>
<div class="figure">
<img src="bookdown_files/figure-html/pca_svd-1.png" alt="Plot showing SVD and prcomp give same results." width="1008" />
<p class="caption">
(#fig:pca_svd)Plot showing SVD and prcomp give same results.
</p>
</div>
<p>The loadings can be found this way:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pc<span class="op">$</span>rotation</code></pre></div>
<pre><code>##         PC1     PC2
## [1,] 0.7072  0.7070
## [2,] 0.7070 -0.7072</code></pre>
<p>which are equivalent (up to a sign flip) to:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s<span class="op">$</span>u</code></pre></div>
<pre><code>##         [,1]    [,2]
## [1,] -0.7072 -0.7070
## [2,] -0.7070  0.7072</code></pre>
<p>The equivalent of the variance explained is included in the:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pc<span class="op">$</span>sdev</code></pre></div>
<pre><code>## [1] 1.2543 0.2142</code></pre>
<p>component.</p>
<p>We take the transpose of <code>Y</code> because <code>prcomp</code> assumes the previously discussed ordering: units/samples in row and features in columns.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="basic-machine-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/xie186/DataAnalysisForLifeScience_cn/edit/master/08-highdim.Rmd",
"text": "编辑"
},
"download": ["bookdown.pdf", "bookdown.epub"],
"toc": {
"collapse": "none"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
